# -*- coding: utf-8 -*-
"""RCNN Faster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1atdA7dT0ujpIxkxwuQ9bJo2jghbKa4S8
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import json
import random
import numpy as np
import torch
import torch.utils.data
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.transforms import functional as F
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import cv2
import time

# Pour la reproductibilité
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

class CollembolesDataset(torch.utils.data.Dataset):
    def __init__(self, json_file, img_dir, transform=None, species_filter=None):
        """
        Args:
            json_file (string): Chemin vers le fichier JSON des annotations
            img_dir (string): Dossier contenant les images
            transform (callable, optional): Transformation à appliquer aux images
            species_filter (str, optional): Filtrer par espèce spécifique
        """
        with open(json_file, 'r') as f:
            self.annotations = json.load(f)

        # Conserver uniquement les images de l'espèce spécifiée si un filtre est appliqué
        if species_filter:
            self.annotations = {k: v for k, v in self.annotations.items()
                              if v.get("species") == species_filter}

        self.img_dir = img_dir
        self.transform = transform
        self.imgs = list(sorted(self.annotations.keys()))

        # Créer un dictionnaire de toutes les caractéristiques utilisées
        self.features = set()
        for img_name, ann in self.annotations.items():
            for feature in ann.get("features", {}).keys():
                self.features.add(feature)

        self.features = sorted(list(self.features))
        # Ajouter une classe de fond (background)
        self.feature_to_idx = {feature: i + 1 for i, feature in enumerate(self.features)}
        self.feature_to_idx["background"] = 0  # Classe de fond
        self.idx_to_feature = {v: k for k, v in self.feature_to_idx.items()}

        print(f"Dataset chargé avec {len(self.imgs)} images et {len(self.features)} caractéristiques")
        print(f"Caractéristiques: {self.features}")

    def __getitem__(self, idx):
        # Charger l'image
        img_name = self.imgs[idx]
        img_path = os.path.join(self.img_dir, img_name)
        img = Image.open(img_path).convert("RGB")

        # Obtenir les dimensions de l'image
        width, height = img.size

        # Préparer les annotations de cette image
        annotations = self.annotations[img_name]
        features_data = annotations.get("features", {})

        boxes = []
        labels = []

        # Pour chaque caractéristique dans cette image
        for feature_name, feature_info in features_data.items():
            if feature_name in self.feature_to_idx and "bbox" in feature_info:
                bbox = feature_info["bbox"]

                # Vérifier si le format est YOLO (center_x, center_y, width, height)
                if "center_x" in bbox and "center_y" in bbox and "width" in bbox and "height" in bbox:
                    # Convertir de format YOLO à format (x_min, y_min, x_max, y_max)
                    center_x = bbox["center_x"] * width
                    center_y = bbox["center_y"] * height
                    box_width = bbox["width"] * width
                    box_height = bbox["height"] * height

                    x_min = max(0, center_x - box_width / 2)
                    y_min = max(0, center_y - box_height / 2)
                    x_max = min(width, center_x + box_width / 2)
                    y_max = min(height, center_y + box_height / 2)

                    # Ignorer les boîtes trop petites ou invalides
                    if x_max <= x_min or y_max <= y_min or x_max <= 0 or y_max <= 0 or x_min >= width or y_min >= height:
                        continue

                    boxes.append([x_min, y_min, x_max, y_max])
                    labels.append(self.feature_to_idx[feature_name])

        # Convertir en tenseurs
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # Créer le dictionnaire des cibles
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = torch.tensor([idx])
        target["area"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        target["iscrowd"] = torch.zeros((len(boxes),), dtype=torch.int64)

        # Appliquer les transformations
        if self.transform is not None:
            img = self.transform(img)
        else:
            img = F.to_tensor(img)

        return img, target

    def __len__(self):
        return len(self.imgs)

    def get_img_name(self, idx):
        """Récupère le nom de l'image à partir de son index"""
        return self.imgs[idx]


# Fonction pour créer le modèle Faster R-CNN
def get_faster_rcnn_model(num_classes):
    # Charger un modèle Faster R-CNN pré-entraîné
    model = fasterrcnn_resnet50_fpn(pretrained=True)

    # Remplacer la boîte de classification avec une nouvelle adaptée au nombre de classes
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model


# Fonction d'entraînement
def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):
    model.train()

    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1.0 / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=warmup_factor, total_iters=warmup_iters
        )

    running_loss = 0.0
    running_loss_classifier = 0.0
    running_loss_box_reg = 0.0
    running_loss_objectness = 0.0
    running_loss_rpn_box_reg = 0.0

    for i, (images, targets) in enumerate(tqdm(data_loader)):
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Forward
        loss_dict = model(images, targets)

        losses = sum(loss for loss in loss_dict.values())

        # Backward
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        # Mise à jour du scheduler
        if lr_scheduler is not None:
            lr_scheduler.step()

        # Enregistrer les pertes
        running_loss += losses.item()
        running_loss_classifier += loss_dict['loss_classifier'].item()
        running_loss_box_reg += loss_dict['loss_box_reg'].item()
        running_loss_objectness += loss_dict['loss_objectness'].item()
        running_loss_rpn_box_reg += loss_dict['loss_rpn_box_reg'].item()

        if i % print_freq == 0:
            print(f"Epoch: [{epoch}][{i}/{len(data_loader)}], Loss: {losses.item():.4f}")

    # Calculer les moyennes
    epoch_loss = running_loss / len(data_loader)
    epoch_loss_classifier = running_loss_classifier / len(data_loader)
    epoch_loss_box_reg = running_loss_box_reg / len(data_loader)
    epoch_loss_objectness = running_loss_objectness / len(data_loader)
    epoch_loss_rpn_box_reg = running_loss_rpn_box_reg / len(data_loader)

    print(f"Epoch {epoch} terminée. Pertes moyennes:")
    print(f"  Total: {epoch_loss:.4f}")
    print(f"  Classifier: {epoch_loss_classifier:.4f}")
    print(f"  Box Reg: {epoch_loss_box_reg:.4f}")
    print(f"  Objectness: {epoch_loss_objectness:.4f}")
    print(f"  RPN Box Reg: {epoch_loss_rpn_box_reg:.4f}")

    return epoch_loss


# Fonction d'évaluation
def evaluate(model, data_loader, device):
    model.eval()

    detections = []
    ground_truths = []

    with torch.no_grad():
        for images, targets in tqdm(data_loader):
            images = list(img.to(device) for img in images)

            outputs = model(images)

            # Récupérer les résultats pour chaque image
            for i, output in enumerate(outputs):
                detections.append(output)
                ground_truths.append(targets[i])

    # Ici, vous pourriez calculer mAP, mais nous allons juste retourner les détections
    return detections, ground_truths


# Fonction pour prédire sur une image
def predict_image(model, image_path, threshold=0.5, device='cuda'):
    # Charger l'image
    image = Image.open(image_path).convert("RGB")
    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)

    # Prédiction
    model.eval()
    with torch.no_grad():
        prediction = model(image_tensor)

    # Filtrer les prédictions selon le seuil
    boxes = prediction[0]['boxes'][prediction[0]['scores'] > threshold]
    labels = prediction[0]['labels'][prediction[0]['scores'] > threshold]
    scores = prediction[0]['scores'][prediction[0]['scores'] > threshold]

    return image, boxes.cpu().numpy(), labels.cpu().numpy(), scores.cpu().numpy()


# Fonction pour visualiser les prédictions
def visualize_predictions(image, boxes, labels, scores, idx_to_feature, title=None, save_path=None):
    # Créer une copie de l'image pour dessiner dessus
    draw_image = image.copy()
    draw = ImageDraw.Draw(draw_image)

    # Couleurs pour différentes classes (cycle de 10 couleurs)
    colors = [
        (255, 0, 0),     # Rouge
        (0, 255, 0),     # Vert
        (0, 0, 255),     # Bleu
        (255, 255, 0),   # Jaune
        (255, 0, 255),   # Magenta
        (0, 255, 255),   # Cyan
        (128, 0, 0),     # Marron
        (0, 128, 0),     # Vert foncé
        (0, 0, 128),     # Bleu foncé
        (128, 128, 0)    # Olive
    ]

    # Dessiner chaque boîte
    for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):
        color = colors[label % len(colors)]

        # Dessiner le rectangle
        draw.rectangle(
            [(box[0], box[1]), (box[2], box[3])],
            outline=color,
            width=3
        )

        # Ajouter le nom de la classe et le score
        label_name = idx_to_feature[label]
        text = f"{label_name}: {score:.2f}"

        # Essayer d'utiliser une police si disponible
        try:
            font = ImageFont.truetype("arial.ttf", 16)
            draw.text((box[0], box[1] - 20), text, fill=color, font=font)
        except IOError:
            draw.text((box[0], box[1] - 20), text, fill=color)

    # Afficher l'image
    plt.figure(figsize=(12, 8))
    plt.imshow(draw_image)
    if title:
        plt.title(title)
    plt.axis('off')

    # Sauvegarder si un chemin est spécifié
    if save_path:
        draw_image.save(save_path)

    plt.tight_layout()
    plt.show()


# Fonction principale d'entraînement
def train_model(json_file, img_dir, species_filter=None, num_epochs=10, batch_size=2):
    # Créer le dataset et les dataloaders
    dataset = CollembolesDataset(json_file, img_dir, species_filter=species_filter)

    # Diviser en train/test (80%/20%)
    indices = torch.randperm(len(dataset)).tolist()
    train_size = int(len(dataset) * 0.8)

    train_dataset = torch.utils.data.Subset(dataset, indices[:train_size])
    test_dataset = torch.utils.data.Subset(dataset, indices[train_size:])

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x))
    )

    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x))
    )

    # Nombre de classes (caractéristiques + fond)
    num_classes = len(dataset.feature_to_idx)

    # Créer le modèle
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    model = get_faster_rcnn_model(num_classes)
    model.to(device)

    # Optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

    # Learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

    # Entraînement
    for epoch in range(num_epochs):
        loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
        lr_scheduler.step()

        # Évaluation
        if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:
            print("Évaluation du modèle...")
            detections, ground_truths = evaluate(model, test_loader, device)
            print(f"Évaluation terminée. {len(detections)} détections.")

    # Sauvegarder le modèle
    model_save_path = f"faster_rcnn_collemboles{'_' + species_filter if species_filter else ''}.pth"
    torch.save(model.state_dict(), model_save_path)
    print(f"Modèle sauvegardé: {model_save_path}")

    return model, dataset.idx_to_feature


# Fonction pour démontrer l'inférence sur quelques images
def demo_inference(model, idx_to_feature, img_dir, json_file, num_images=5, threshold=0.5, device='cuda'):
    # Charger les annotations pour vérification
    with open(json_file, 'r') as f:
        annotations = json.load(f)

    # Obtenir les chemins d'image
    img_paths = []
    for img_name in annotations.keys():
        img_path = os.path.join(img_dir, img_name)
        if os.path.exists(img_path):
            img_paths.append(img_path)

    # Sélectionner aléatoirement quelques images
    selected_paths = random.sample(img_paths, min(num_images, len(img_paths)))

    # Créer un dossier pour sauvegarder les résultats
    output_dir = "predictions"
    os.makedirs(output_dir, exist_ok=True)

    for i, img_path in enumerate(selected_paths):
        print(f"Prédiction sur image {i+1}/{len(selected_paths)}: {os.path.basename(img_path)}")

        # Prédire
        image, boxes, labels, scores = predict_image(model, img_path, threshold, device)

        # Visualiser
        img_name = os.path.basename(img_path)
        save_path = os.path.join(output_dir, f"pred_{img_name}")

        visualize_predictions(
            image, boxes, labels, scores, idx_to_feature,
            title=f"Prédictions: {img_name}",
            save_path=save_path
        )

        print(f"Résultat sauvegardé: {save_path}")

    return output_dir


# Exécution du script
if __name__ == "__main__":
    # Configuration
    json_file = "5.json"  # Chemin vers votre fichier JSON d'annotations
    img_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"  # Dossier contenant vos images

    # Vous pouvez filtrer par espèce pour entraîner un modèle par espèce
    # Exemple : species_filter = "Cer"
    species_filter = None  # None pour utiliser toutes les espèces

    # Paramètres d'entraînement
    num_epochs = 10
    batch_size = 2  # Réduire si problèmes de mémoire

    # Entraînement
    model, idx_to_feature = train_model(json_file, img_dir, species_filter, num_epochs, batch_size)
    print(idx_to_feature)
    # Démonstration
    output_dir = demo_inference(model, idx_to_feature, img_dir, json_file, num_images=5)
    print(f"Démonstration terminée. Résultats dans: {output_dir}")

output_dir = demo_inference(model, idx_to_feature, img_dir, json_file, num_images=20)
print(f"Démonstration terminée. Résultats dans: {output_dir}")

import os
import json
import random
import numpy as np
import torch
import torch.utils.data
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.transforms import functional as F
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import cv2
import time
from collections import defaultdict

# Pour la reproductibilité
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# Liste des espèces
SPECIES_LIST = [
    "AUTRE",    # 0
    "Cer",      # 1
    "CRY_THE",  # 2
    "HYP_MAN",  # 3
    "ISO_MIN",  # 4
    "LEP",      # 5
    "MET_AFF",  # 6
    "PAR_NOT",  # 7
    "FOND"      # 8
]

def analyze_species_feature_relationship(json_file):
    """
    Analyse les relations entre espèces et caractéristiques

    Args:
        json_file (str): Chemin vers le fichier JSON d'annotations

    Returns:
        dict: Mapping des caractéristiques par espèce
        dict: Mapping des espèces par caractéristique
    """
    print("Analyse des relations entre espèces et caractéristiques...")

    # Charger les annotations
    with open(json_file, 'r') as f:
        annotations = json.load(f)

    # Dictionnaires pour stocker les relations
    features_by_species = defaultdict(set)  # Pour chaque espèce, quelles caractéristiques sont présentes
    species_by_feature = defaultdict(set)   # Pour chaque caractéristique, dans quelles espèces est-elle présente
    feature_counts = defaultdict(int)       # Compteur pour chaque caractéristique

    # Parcourir les annotations
    for img_name, annotation in annotations.items():
        # Déterminer l'espèce
        species = annotation.get("species", "Unknown")

        # Extraire les caractéristiques pour cette image
        features = set()

        for feature_name in annotation.get("features", {}).keys():
            features.add(feature_name)
            feature_counts[feature_name] += 1

        # Mettre à jour les relations
        for feature in features:
            features_by_species[species].add(feature)
            species_by_feature[feature].add(species)

    # Convertir les sets en listes pour la sérialisation JSON
    features_by_species_list = {k: list(v) for k, v in features_by_species.items()}
    species_by_feature_list = {k: list(v) for k, v in species_by_feature.items()}

    # Sauvegarder les mappings pour référence
    with open('species_feature_mapping.json', 'w') as f:
        json.dump({
            'features_by_species': features_by_species_list,
            'species_by_feature': species_by_feature_list,
            'feature_counts': feature_counts
        }, f, indent=4)

    # Afficher les statistiques
    print("\nCaractéristiques par espèce:")
    for species, features in features_by_species.items():
        print(f"  {species}: {', '.join(features)}")

    print("\nEspèces par caractéristique:")
    for feature, species_list in species_by_feature.items():
        print(f"  {feature} ({feature_counts[feature]} occurrences): {', '.join(species_list)}")

    # Créer une visualisation si possible
    try:
        plt.figure(figsize=(12, 8))

        # Matrice de présence des caractéristiques par espèce
        all_species = sorted(features_by_species.keys())
        all_features = sorted(species_by_feature.keys())

        if len(all_species) > 0 and len(all_features) > 0:
            # Créer la matrice
            matrix = np.zeros((len(all_species), len(all_features)))

            for i, species in enumerate(all_species):
                for j, feature in enumerate(all_features):
                    if feature in features_by_species[species]:
                        matrix[i, j] = 1

            # Afficher la matrice
            plt.imshow(matrix, cmap='viridis', aspect='auto')
            plt.colorbar(label='Présence (1) / Absence (0)')
            plt.xlabel('Caractéristique')
            plt.ylabel('Espèce')
            plt.title('Relation entre espèces et caractéristiques')
            plt.xticks(np.arange(len(all_features)), all_features, rotation=45, ha='right')
            plt.yticks(np.arange(len(all_species)), all_species)
            plt.tight_layout()
            plt.savefig("species_feature_matrix.png")
            plt.close()
    except Exception as e:
        print(f"Erreur lors de la création de la visualisation: {e}")

    return features_by_species, species_by_feature


class CollembolesDataset(torch.utils.data.Dataset):
    def __init__(self, json_file, img_dir, transform=None, species_feature_mapping=None):
        """
        Args:
            json_file (string): Chemin vers le fichier JSON des annotations
            img_dir (string): Dossier contenant les images
            transform (callable, optional): Transformation à appliquer aux images
            species_feature_mapping (tuple, optional): Tuple (features_by_species, species_by_feature)
        """
        with open(json_file, 'r') as f:
            self.annotations = json.load(f)

        self.img_dir = img_dir
        self.transform = transform
        self.imgs = list(sorted(self.annotations.keys()))

        # Stocker le mapping des caractéristiques par espèce
        self.features_by_species = None
        self.species_by_feature = None

        if species_feature_mapping:
            self.features_by_species, self.species_by_feature = species_feature_mapping

        # Créer un dictionnaire de toutes les caractéristiques utilisées
        self.features = set()
        for img_name, ann in self.annotations.items():
            for feature in ann.get("features", {}).keys():
                self.features.add(feature)

        self.features = sorted(list(self.features))

        # Ajouter une classe de fond (background)
        self.feature_to_idx = {feature: i + 1 for i, feature in enumerate(self.features)}
        self.feature_to_idx["background"] = 0  # Classe de fond
        self.idx_to_feature = {v: k for k, v in self.feature_to_idx.items()}

        print(f"Dataset chargé avec {len(self.imgs)} images et {len(self.features)} caractéristiques")

        # Filtrer les images valides
        self.valid_imgs = []
        for img_name in self.imgs:
            img_path = os.path.join(self.img_dir, img_name)
            if os.path.exists(img_path) and self._has_valid_boxes(img_name):
                self.valid_imgs.append(img_name)

        print(f"Après filtrage: {len(self.valid_imgs)} images valides")

    def _has_valid_boxes(self, img_name):
        """Vérifie si l'image a au moins une boîte valide"""
        annotation = self.annotations[img_name]
        features_data = annotation.get("features", {})

        for feature_name, feature_info in features_data.items():
            if "bbox" in feature_info:
                return True

        return False

    def _get_relevant_features(self, species):
        """Retourne les caractéristiques pertinentes pour cette espèce"""
        if self.features_by_species and species in self.features_by_species:
            return set(self.features_by_species[species])

        # Si pas de mapping ou espèce inconnue, retourner toutes les caractéristiques
        return set(self.features)

    def __getitem__(self, idx):
        # Charger l'image
        img_name = self.valid_imgs[idx]
        img_path = os.path.join(self.img_dir, img_name)

        try:
            img = Image.open(img_path).convert("RGB")
        except Exception as e:
            print(f"Erreur lors du chargement de l'image {img_path}: {e}")
            # Créer une image noire en cas d'erreur
            img = Image.new('RGB', (100, 100), color='black')

        # Obtenir les dimensions de l'image
        width, height = img.size

        # Obtenir l'espèce
        species = self.annotations[img_name].get("species", "Unknown")

        # Obtenir les caractéristiques pertinentes pour cette espèce
        relevant_features = self._get_relevant_features(species)

        # Préparer les annotations de cette image
        annotations = self.annotations[img_name]
        features_data = annotations.get("features", {})

        boxes = []
        labels = []

        # Pour chaque caractéristique dans cette image
        for feature_name, feature_info in features_data.items():
            # Vérifier si cette caractéristique est pertinente pour cette espèce
            # On inclut toujours les antennes car elles sont présentes partout
            if feature_name in self.feature_to_idx and (feature_name.lower() == 'Antennes'):
                if "bbox" in feature_info:
                    bbox = feature_info["bbox"]

                    # Vérifier si le format est YOLO (center_x, center_y, width, height)
                    if "center_x" in bbox and "center_y" in bbox and "width" in bbox and "height" in bbox:
                        # Convertir de format YOLO à format (x_min, y_min, x_max, y_max)
                        center_x = bbox["center_x"] * width
                        center_y = bbox["center_y"] * height
                        box_width = bbox["width"] * width
                        box_height = bbox["height"] * height

                        x_min = max(0, center_x - box_width / 2)
                        y_min = max(0, center_y - box_height / 2)
                        x_max = min(width, center_x + box_width / 2)
                        y_max = min(height, center_y + box_height / 2)

                        # Ignorer les boîtes trop petites ou invalides
                        if x_max <= x_min or y_max <= y_min or x_max <= 0 or y_max <= 0 or x_min >= width or y_min >= height:
                            continue

                        boxes.append([x_min, y_min, x_max, y_max])
                        labels.append(self.feature_to_idx[feature_name])

        # S'assurer qu'il y a au moins une boîte valide
        if len(boxes) == 0:
            # Créer une boîte fictive pour éviter les erreurs
            boxes.append([0, 0, 10, 10])
            labels.append(0)  # Classe de fond

        # Convertir en tenseurs
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # Créer le dictionnaire des cibles
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = torch.tensor([idx])
        target["area"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        target["iscrowd"] = torch.zeros((len(boxes),), dtype=torch.int64)

        # Appliquer les transformations
        if self.transform is not None:
            img = self.transform(img)
        else:
            img = F.to_tensor(img)

        return img, target

    def __len__(self):
        return len(self.valid_imgs)

    def get_img_name(self, idx):
        """Récupère le nom de l'image à partir de son index"""
        return self.valid_imgs[idx]


# Fonction pour créer le modèle Faster R-CNN
def get_faster_rcnn_model(num_classes):
    # Charger un modèle Faster R-CNN pré-entraîné
    model = fasterrcnn_resnet50_fpn(pretrained=True)

    # Remplacer la boîte de classification avec une nouvelle adaptée au nombre de classes
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model


# Fonction d'entraînement
def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):
    model.train()

    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1.0 / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=warmup_factor, total_iters=warmup_iters
        )

    running_loss = 0.0
    running_loss_classifier = 0.0
    running_loss_box_reg = 0.0
    running_loss_objectness = 0.0
    running_loss_rpn_box_reg = 0.0

    progress_bar = tqdm(data_loader, desc=f"Epoch {epoch}")

    for i, (images, targets) in enumerate(progress_bar):
        # Vérifier que les targets sont valides
        valid_batch = True

        for target in targets:
            if len(target["boxes"]) == 0:
                valid_batch = False
                break

        if not valid_batch:
            print(f"Batch {i} ignoré car il contient des images sans boîtes valides")
            continue

        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Forward
        loss_dict = model(images, targets)

        losses = sum(loss for loss in loss_dict.values())

        # Backward
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        # Mise à jour du scheduler
        if lr_scheduler is not None:
            lr_scheduler.step()

        # Enregistrer les pertes
        running_loss += losses.item()
        running_loss_classifier += loss_dict['loss_classifier'].item()
        running_loss_box_reg += loss_dict['loss_box_reg'].item()
        running_loss_objectness += loss_dict['loss_objectness'].item()
        running_loss_rpn_box_reg += loss_dict['loss_rpn_box_reg'].item()

        if i % print_freq == 0:
            progress_bar.set_postfix(loss=losses.item())

    # Calculer les moyennes
    num_batches = len(data_loader)
    if num_batches > 0:
        epoch_loss = running_loss / num_batches
        epoch_loss_classifier = running_loss_classifier / num_batches
        epoch_loss_box_reg = running_loss_box_reg / num_batches
        epoch_loss_objectness = running_loss_objectness / num_batches
        epoch_loss_rpn_box_reg = running_loss_rpn_box_reg / num_batches

        print(f"Epoch {epoch} terminée. Pertes moyennes:")
        print(f"  Total: {epoch_loss:.4f}")
        print(f"  Classifier: {epoch_loss_classifier:.4f}")
        print(f"  Box Reg: {epoch_loss_box_reg:.4f}")
        print(f"  Objectness: {epoch_loss_objectness:.4f}")
        print(f"  RPN Box Reg: {epoch_loss_rpn_box_reg:.4f}")

        return epoch_loss
    else:
        print("Aucun batch valide dans cette époque")
        return 0.0


# Fonction d'évaluation
def evaluate(model, data_loader, device):
    model.eval()

    detections = []
    ground_truths = []

    with torch.no_grad():
        for images, targets in tqdm(data_loader, desc="Évaluation"):
            images = list(img.to(device) for img in images)

            outputs = model(images)

            # Récupérer les résultats pour chaque image
            for i, output in enumerate(outputs):
                detections.append(output)
                ground_truths.append(targets[i])

    # Ici, vous pourriez calculer mAP, mais nous allons juste retourner les détections
    return detections, ground_truths


# Fonction pour prédire sur une image
def predict_image(model, image_path, idx_to_feature, threshold=0.5, device='cuda'):
    # Charger l'image
    try:
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"Erreur lors du chargement de l'image {image_path}: {e}")
        # Créer une image noire en cas d'erreur
        image = Image.new('RGB', (100, 100), color='black')

    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)

    # Prédiction
    model.eval()
    with torch.no_grad():
        prediction = model(image_tensor)

    # Filtrer les prédictions selon le seuil
    boxes = prediction[0]['boxes'][prediction[0]['scores'] > threshold]
    labels = prediction[0]['labels'][prediction[0]['scores'] > threshold]
    scores = prediction[0]['scores'][prediction[0]['scores'] > threshold]

    return image, boxes.cpu().numpy(), labels.cpu().numpy(), scores.cpu().numpy()


# Fonction pour visualiser les prédictions
def visualize_predictions(image, boxes, labels, scores, idx_to_feature, title=None, save_path=None):
    # Créer une copie de l'image pour dessiner dessus
    draw_image = image.copy()
    draw = ImageDraw.Draw(draw_image)

    # Couleurs pour différentes classes (cycle de 10 couleurs)
    colors = [
        (255, 0, 0),     # Rouge
        (0, 255, 0),     # Vert
        (0, 0, 255),     # Bleu
        (255, 255, 0),   # Jaune
        (255, 0, 255),   # Magenta
        (0, 255, 255),   # Cyan
        (128, 0, 0),     # Marron
        (0, 128, 0),     # Vert foncé
        (0, 0, 128),     # Bleu foncé
        (128, 128, 0)    # Olive
    ]

    # Dessiner chaque boîte
    for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):
        color = colors[label % len(colors)]

        # Dessiner le rectangle
        draw.rectangle(
            [(box[0], box[1]), (box[2], box[3])],
            outline=color,
            width=3
        )

        # Ajouter le nom de la classe et le score
        label_name = idx_to_feature.get(label, f"Unknown-{label}")
        text = f"{label_name}: {score:.2f}"

        # Dessiner le texte directement
        draw.text((box[0], box[1] - 20), text, fill=color)

    # Afficher l'image
    plt.figure(figsize=(12, 8))
    plt.imshow(draw_image)
    if title:
        plt.title(title)
    plt.axis('off')

    # Sauvegarder si un chemin est spécifié
    if save_path:
        draw_image.save(save_path)

    plt.tight_layout()
    plt.show()


# Fonction principale d'entraînement
def train_model(json_file, img_dir, species_feature_mapping=None, num_epochs=5, batch_size=2):
    # Créer le dataset et les dataloaders
    dataset = CollembolesDataset(json_file, img_dir, species_feature_mapping=species_feature_mapping)

    # Vérifier s'il y a des images valides
    if len(dataset) == 0:
        print("Aucune image valide dans le dataset. Impossible d'entraîner le modèle.")
        return None, dataset.idx_to_feature

    # Diviser en train/test (80%/20%)
    indices = torch.randperm(len(dataset)).tolist()
    train_size = int(len(dataset) * 0.8)

    # Assurez-vous que train_size est d'au moins 1
    train_size = max(1, train_size)

    # S'il n'y a qu'une seule image, utilisez-la pour l'entraînement
    if len(dataset) == 1:
        train_indices = indices
        test_indices = indices
    else:
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]

    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    test_dataset = torch.utils.data.Subset(dataset, test_indices)

    # Définir une fonction de collate personnalisée qui filtre les images sans boîtes
    def collate_fn(batch):
        # Filtrer les éléments où il n'y a pas de boîtes
        filtered_batch = []
        for img, target in batch:
            if len(target["boxes"]) > 0:
                filtered_batch.append((img, target))

        # Si tous les éléments ont été filtrés, ajouter un élément factice
        if len(filtered_batch) == 0 and len(batch) > 0:
            img, target = batch[0]
            # Ajouter une boîte factice
            if len(target["boxes"]) == 0:
                target["boxes"] = torch.tensor([[0, 0, 10, 10]], dtype=torch.float32)
                target["labels"] = torch.tensor([0], dtype=torch.int64)  # Classe de fond
                target["area"] = torch.tensor([100], dtype=torch.float32)
                target["iscrowd"] = torch.tensor([0], dtype=torch.int64)
            filtered_batch.append((img, target))

        return tuple(zip(*filtered_batch))

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn
    )

    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn
    )

    # Nombre de classes (caractéristiques + fond)
    num_classes = len(dataset.feature_to_idx)

    # Créer le modèle
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    print(f"Utilisation du dispositif: {device}")

    model = get_faster_rcnn_model(num_classes)
    model.to(device)

    # Optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

    # Learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

    # Entraînement
    for epoch in range(num_epochs):
        try:
            loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
            lr_scheduler.step()

            # Évaluation
            if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:
                print("Évaluation du modèle...")
                detections, ground_truths = evaluate(model, test_loader, device)
                print(f"Évaluation terminée. {len(detections)} détections.")
        except Exception as e:
            print(f"Erreur lors de l'époque {epoch}: {e}")
            continue

    # Sauvegarder le modèle
    try:
        model_save_path = f"faster_rcnn_collemboles.pth"
        torch.save(model.state_dict(), model_save_path)
        print(f"Modèle sauvegardé: {model_save_path}")
    except Exception as e:
        print(f"Erreur lors de la sauvegarde du modèle: {e}")

    return model, dataset.idx_to_feature


# Fonction pour démontrer l'inférence sur quelques images
def demo_inference(model, idx_to_feature, img_dir, json_file, num_images=5, threshold=0.3, device='cuda'):
    if model is None:
        print("Pas de modèle disponible pour l'inférence.")
        return None

    # Charger les annotations pour vérification
    with open(json_file, 'r') as f:
        annotations = json.load(f)

    # Obtenir les chemins d'image
    img_paths = []
    for img_name in annotations.keys():
        img_path = os.path.join(img_dir, img_name)
        if os.path.exists(img_path):
            img_paths.append(img_path)

    if not img_paths:
        print("Aucune image valide trouvée.")
        return None

    # Sélectionner aléatoirement quelques images
    num_images = min(num_images, len(img_paths))
    if num_images == 0:
        print("Aucune image à démontrer.")
        return None

    selected_paths = random.sample(img_paths, num_images)

    # Créer un dossier pour sauvegarder les résultats
    output_dir = "predictions"
    os.makedirs(output_dir, exist_ok=True)

    for i, img_path in enumerate(selected_paths):
        print(f"Prédiction sur image {i+1}/{len(selected_paths)}: {os.path.basename(img_path)}")

        # Prédire
        try:
            image, boxes, labels, scores = predict_image(model, img_path, idx_to_feature, threshold, device)

            # Visualiser
            img_name = os.path.basename(img_path)
            save_path = os.path.join(output_dir, f"pred_{img_name}")

            visualize_predictions(
                image, boxes, labels, scores, idx_to_feature,
                title=f"Prédictions: {img_name}",
                save_path=save_path
            )

            print(f"  Nombre de caractéristiques détectées: {len(boxes)}")
            for j, (label, score) in enumerate(zip(labels, scores)):
                feature_name = idx_to_feature.get(label, f"Unknown-{label}")
                print(f"    {j+1}. {feature_name}: {score:.2f}")

            print(f"  Résultat sauvegardé: {save_path}")
        except Exception as e:
            print(f"Erreur lors de l'inférence sur {img_path}: {e}")

    return output_dir


if __name__ == "__main__":
    # Configuration
    json_file = "5.json"  # Chemin vers votre fichier JSON d'annotations
    img_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"  # Dossier contenant vos images

    # Vérifier les chemins d'accès
    print(f"Vérification des chemins d'accès:")
    print(f"  Fichier JSON: {os.path.exists(json_file)}")
    print(f"  Dossier d'images: {os.path.isdir(img_dir)}")

    # Paramètres d'entraînement
    num_epochs = 20  # Réduit pour éviter les problèmes
    batch_size = 2  # Réduit pour éviter les problèmes de mémoire

    # 1. Analyse du mapping espèce-caractéristique
    print("\n=== Analyse des relations espèce-caractéristique ===")
    species_feature_mapping = analyze_species_feature_relationship(json_file)

    # 2. Entraînement du modèle optimisé
    print("\n=== Entraînement du modèle optimisé ===")
    model, idx_to_feature = train_model(
        json_file, img_dir, species_feature_mapping, num_epochs, batch_size
    )
    print(idx_to_feature)
    # 3. Démonstration sur des images
    if model is not None:
        print("\n=== Démonstration de l'inférence ===")
        output_dir = demo_inference(model, idx_to_feature, img_dir, json_file, num_images=20)
        print(f"Démonstration terminée. Résultats dans: {output_dir}")

def demo_inference(model, idx_to_feature, img_dir, json_file, num_images=5, threshold=0.3, device='cuda'):
    if model is None:
        print("Pas de modèle disponible pour l'inférence.")
        return None

    # Charger la liste des images à exclure depuis le fichier JSON
    with open(json_file, 'r') as f:
        excluded_annotations = json.load(f)
    excluded_images = set(excluded_annotations.keys())

    # Définir les extensions d'image acceptées
    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}

    # Récupérer tous les chemins d'images dans le répertoire, en excluant ceux présents dans le JSON et les fichiers non-images
    img_paths = []
    for img_name in os.listdir(img_dir):
        # Vérifier l'extension
        ext = os.path.splitext(img_name)[1].lower()
        if ext not in valid_extensions:
            continue
        if img_name in excluded_images:
            continue  # On saute l'image si elle est présente dans le JSON
        img_path = os.path.join(img_dir, img_name)
        if os.path.exists(img_path):
            img_paths.append(img_path)

    if not img_paths:
        print("Aucune image valide trouvée (après exclusion et filtrage par extension).")
        return None

    # Sélectionner aléatoirement quelques images
    num_images = min(num_images, len(img_paths))
    if num_images == 0:
        print("Aucune image à démontrer.")
        return None

    selected_paths = random.sample(img_paths, num_images)

    # Créer un dossier pour sauvegarder les résultats
    output_dir = "predictions"
    os.makedirs(output_dir, exist_ok=True)

    for i, img_path in enumerate(selected_paths):
        print(f"Prédiction sur image {i+1}/{len(selected_paths)}: {os.path.basename(img_path)}")
        try:
            image, boxes, labels, scores = predict_image(model, img_path, idx_to_feature, threshold, device)

            # Filtrer pour ne garder que le score maximal pour chaque classe
            max_score_per_class = {}
            max_box_per_class = {}

            for box, label, score in zip(boxes, labels, scores):
                if label not in max_score_per_class or score > max_score_per_class[label]:
                    max_score_per_class[label] = score
                    max_box_per_class[label] = box

            # Convertir en listes pour la visualisation
            filtered_boxes = np.array(list(max_box_per_class.values()))
            filtered_labels = np.array(list(max_box_per_class.keys()))
            filtered_scores = np.array([max_score_per_class[label] for label in filtered_labels])

            # Visualiser et sauvegarder les résultats
            img_name = os.path.basename(img_path)
            save_path = os.path.join(output_dir, f"pred_{img_name}")

            visualize_predictions(
                image, filtered_boxes, filtered_labels, filtered_scores, idx_to_feature,
                title=f"Prédictions (Max par classe): {img_name}",
                save_path=save_path
            )

            print(f"  Nombre de caractéristiques détectées (max par classe): {len(filtered_boxes)}")
            for j, (label, score) in enumerate(zip(filtered_labels, filtered_scores)):
                feature_name = idx_to_feature.get(label, f"Unknown-{label}")
                print(f"    {j+1}. {feature_name}: {score:.2f}")

            print(f"  Résultat sauvegardé: {save_path}")
        except Exception as e:
            print(f"Erreur lors de l'inférence sur {img_path}: {e}")

    return output_dir



output_dir = demo_inference(model, idx_to_feature, img_dir, json_file, num_images=50)
print(f"Démonstration terminée. Résultats dans: {output_dir}")

!pip install tqdm

import os
import numpy as np
import matplotlib.pyplot as plt
import concurrent.futures

def process_image(img_path, model, idx_to_feature, threshold, device):
    """
    Traite une image pour obtenir l'ensemble des classes détectées (max par classe).
    Renvoie un ensemble de noms de caractéristiques.
    """
    try:
        image, boxes, labels, scores = predict_image(model, img_path, idx_to_feature, threshold, device)
        # Ne garder que la détection de score maximal par classe
        max_scores = {}
        for label, score in zip(labels, scores):
            if label not in max_scores or score > max_scores[label]:
                max_scores[label] = score
        # Retourne l'ensemble des noms de caractéristiques détectées dans l'image
        return { idx_to_feature.get(label, f"Unknown-{label}") for label in max_scores.keys() }
    except Exception as e:
        print(f"Erreur pour {os.path.basename(img_path)}: {e}")
        return set()

def stat_features_parallel(model, idx_to_feature, img_dir, threshold=0.3, device='cuda'):
    """
    Parcourt en parallèle les images du répertoire et calcule pour chaque classe
    le nombre total d'images dans lesquelles elle est détectée (sans moyennes).

    Affiche quelques prints de progression.
    """
    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}
    img_paths = [os.path.join(img_dir, f) for f in os.listdir(img_dir)
                 if os.path.splitext(f)[1].lower() in valid_extensions]

    total_counts = {}
    total_images = len(img_paths)
    print(f"Début du traitement de {total_images} images...")

    # Utilisation d'un ThreadPoolExecutor pour paralléliser le traitement
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = {executor.submit(process_image, path, model, idx_to_feature, threshold, device): path
                   for path in img_paths}
        for i, future in enumerate(concurrent.futures.as_completed(futures), 1):
            classes = future.result()
            print(f"Image {i}/{total_images} traitée.")
            for cl in classes:
                total_counts[cl] = total_counts.get(cl, 0) + 1

    print("Traitement terminé.")
    return total_counts

def plot_stats(total_counts):
    """
    Génère deux graphiques :
      - Un graphique en barres par espèce (nombre total d'images par classe).
      - Un graphique en camembert montrant la répartition par caractéristiques.
    """
    if not total_counts:
        print("Aucune statistique à afficher.")
        return

    classes = list(total_counts.keys())
    totals = [total_counts[c] for c in classes]

    # Graphique en barres
    plt.figure(figsize=(10, 6))
    bars = plt.bar(classes, totals)
    plt.xlabel("Classe (espèce)")
    plt.ylabel("Nombre total d'images")
    plt.title("Nombre total d'images détectées par espèce")
    plt.xticks(rotation=45, ha="right")
    for bar, total in zip(bars, totals):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{total}', ha='center', va='bottom')
    plt.tight_layout()
    plt.show()

    # Graphique en camembert
    plt.figure(figsize=(8, 8))
    plt.pie(totals, labels=classes, autopct='%1.1f%%', startangle=140)
    plt.title("Répartition par caractéristique")
    plt.tight_layout()
    plt.show()

# Exemple d'utilisation :
total_counts = stat_features_parallel(model, idx_to_feature, "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data", threshold=0.3, device='cuda')
plot_stats(total_counts)

import os
import json
import random
import numpy as np
import torch
import torch.utils.data
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.transforms import functional as F
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import cv2
import time
from collections import defaultdict

# Pour la reproductibilité
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# Liste des espèces
SPECIES_LIST = [
    "AUTRE",    # 0
    "Cer",      # 1
    "CRY_THE",  # 2
    "HYP_MAN",  # 3
    "ISO_MIN",  # 4
    "LEP",      # 5
    "MET_AFF",  # 6
    "PAR_NOT",  # 7
    "FOND"      # 8
]

class AntennesDataset(torch.utils.data.Dataset):
    def __init__(self, json_file, img_dir, transform=None):
        """
        Dataset spécialisé pour la détection des antennes

        Args:
            json_file (string): Chemin vers le fichier JSON des annotations
            img_dir (string): Dossier contenant les images
            transform (callable, optional): Transformation à appliquer aux images
        """
        with open(json_file, 'r') as f:
            self.annotations = json.load(f)

        self.img_dir = img_dir
        self.transform = transform
        self.imgs = list(sorted(self.annotations.keys()))

        # Pour la détection des antennes, nous avons uniquement 2 classes:
        # 0 = background, 1 = antenne
        self.num_classes = 2

        print(f"Dataset chargé avec {len(self.imgs)} images")

        # Filtrer les images valides qui contiennent des annotations d'antennes
        self.valid_imgs = []
        for img_name in self.imgs:
            img_path = os.path.join(self.img_dir, img_name)
            if os.path.exists(img_path) and self._has_valid_antenna_boxes(img_name):
                self.valid_imgs.append(img_name)

        print(f"Après filtrage: {len(self.valid_imgs)} images valides avec des antennes")

    def _has_valid_antenna_boxes(self, img_name):
        """Vérifie si l'image a au moins une boîte d'antenne valide"""
        annotation = self.annotations[img_name]
        features_data = annotation.get("features", {})

        # Vérifier spécifiquement pour la caractéristique "Antennes"
        if "Antennes" in features_data and "bbox" in features_data["Antennes"]:
            return True

        return False

    def __getitem__(self, idx):
        # Charger l'image
        img_name = self.valid_imgs[idx]
        img_path = os.path.join(self.img_dir, img_name)

        try:
            img = Image.open(img_path).convert("RGB")
        except Exception as e:
            print(f"Erreur lors du chargement de l'image {img_path}: {e}")
            # Créer une image noire en cas d'erreur
            img = Image.new('RGB', (100, 100), color='black')

        # Obtenir les dimensions de l'image
        width, height = img.size

        # Préparer les annotations de cette image
        annotations = self.annotations[img_name]
        features_data = annotations.get("features", {})

        boxes = []
        labels = []

        # Rechercher uniquement les antennes
        if "Antennes" in features_data and "bbox" in features_data["Antennes"]:
            feature_info = features_data["Antennes"]
            bbox = feature_info["bbox"]

            # Vérifier si le format est YOLO (center_x, center_y, width, height)
            if "center_x" in bbox and "center_y" in bbox and "width" in bbox and "height" in bbox:
                # Convertir de format YOLO à format (x_min, y_min, x_max, y_max)
                center_x = bbox["center_x"] * width
                center_y = bbox["center_y"] * height
                box_width = bbox["width"] * width
                box_height = bbox["height"] * height

                x_min = max(0, center_x - box_width / 2)
                y_min = max(0, center_y - box_height / 2)
                x_max = min(width, center_x + box_width / 2)
                y_max = min(height, center_y + box_height / 2)

                # Ignorer les boîtes trop petites ou invalides
                if not (x_max <= x_min or y_max <= y_min or x_max <= 0 or y_max <= 0 or x_min >= width or y_min >= height):
                    boxes.append([x_min, y_min, x_max, y_max])
                    labels.append(1)  # Classe 1 = antenne

        # S'assurer qu'il y a au moins une boîte valide
        if len(boxes) == 0:
            # Créer une boîte fictive pour éviter les erreurs
            boxes.append([0, 0, 10, 10])
            labels.append(0)  # Classe 0 = fond

        # Convertir en tenseurs
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # Créer le dictionnaire des cibles
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = torch.tensor([idx])
        target["area"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        target["iscrowd"] = torch.zeros((len(boxes),), dtype=torch.int64)

        # Appliquer les transformations
        if self.transform is not None:
            img = self.transform(img)
        else:
            img = F.to_tensor(img)

        return img, target

    def __len__(self):
        return len(self.valid_imgs)

    def get_img_name(self, idx):
        """Récupère le nom de l'image à partir de son index"""
        return self.valid_imgs[idx]


# Fonction pour créer le modèle Faster R-CNN
def get_faster_rcnn_model(num_classes=2):
    """
    Crée un modèle Faster R-CNN pour la détection d'antennes

    Args:
        num_classes (int): Nombre de classes (2 pour ce cas: fond et antenne)

    Returns:
        model: Modèle Faster R-CNN configuré
    """
    # Charger un modèle Faster R-CNN pré-entraîné
    model = fasterrcnn_resnet50_fpn(pretrained=True)

    # Remplacer la boîte de classification avec une nouvelle adaptée au nombre de classes
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model


# Fonction d'entraînement
def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):
    """
    Entraîne le modèle pendant une époque

    Args:
        model: Modèle à entraîner
        optimizer: Optimiseur
        data_loader: DataLoader pour les données d'entraînement
        device: Dispositif d'entraînement (CPU/GPU)
        epoch: Numéro de l'époque
        print_freq: Fréquence d'affichage des pertes

    Returns:
        float: Perte moyenne pour cette époque
    """
    model.train()

    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1.0 / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=warmup_factor, total_iters=warmup_iters
        )

    running_loss = 0.0
    running_loss_classifier = 0.0
    running_loss_box_reg = 0.0
    running_loss_objectness = 0.0
    running_loss_rpn_box_reg = 0.0

    progress_bar = tqdm(data_loader, desc=f"Epoch {epoch}")

    for i, (images, targets) in enumerate(progress_bar):
        # Vérifier que les targets sont valides
        valid_batch = True

        for target in targets:
            if len(target["boxes"]) == 0:
                valid_batch = False
                break

        if not valid_batch:
            print(f"Batch {i} ignoré car il contient des images sans boîtes valides")
            continue

        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Forward
        loss_dict = model(images, targets)

        # Appliquer un poids plus élevé aux pertes liées aux antennes
        antenna_weight = 1.5  # Ajuster si nécessaire
        for target in targets:
            if 1 in target["labels"]:  # S'il y a des antennes dans ce batch
                loss_dict['loss_classifier'] *= antenna_weight
                loss_dict['loss_box_reg'] *= antenna_weight

        losses = sum(loss for loss in loss_dict.values())

        # Backward
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        # Mise à jour du scheduler
        if lr_scheduler is not None:
            lr_scheduler.step()

        # Enregistrer les pertes
        running_loss += losses.item()
        running_loss_classifier += loss_dict['loss_classifier'].item()
        running_loss_box_reg += loss_dict['loss_box_reg'].item()
        running_loss_objectness += loss_dict['loss_objectness'].item()
        running_loss_rpn_box_reg += loss_dict['loss_rpn_box_reg'].item()

        if i % print_freq == 0:
            progress_bar.set_postfix(loss=losses.item())

    # Calculer les moyennes
    num_batches = len(data_loader)
    if num_batches > 0:
        epoch_loss = running_loss / num_batches
        epoch_loss_classifier = running_loss_classifier / num_batches
        epoch_loss_box_reg = running_loss_box_reg / num_batches
        epoch_loss_objectness = running_loss_objectness / num_batches
        epoch_loss_rpn_box_reg = running_loss_rpn_box_reg / num_batches

        print(f"Epoch {epoch} terminée. Pertes moyennes:")
        print(f"  Total: {epoch_loss:.4f}")
        print(f"  Classifier: {epoch_loss_classifier:.4f}")
        print(f"  Box Reg: {epoch_loss_box_reg:.4f}")
        print(f"  Objectness: {epoch_loss_objectness:.4f}")
        print(f"  RPN Box Reg: {epoch_loss_rpn_box_reg:.4f}")

        return epoch_loss
    else:
        print("Aucun batch valide dans cette époque")
        return 0.0


# Fonction pour calculer l'IoU
def calculate_iou(box, boxes):
    """
    Calcule l'IoU (Intersection over Union) entre une boîte et un ensemble de boîtes

    Args:
        box: Une boîte [x1, y1, x2, y2]
        boxes: Ensemble de boîtes [[x1, y1, x2, y2], ...]

    Returns:
        array: IoUs entre la boîte et chaque boîte de l'ensemble
    """
    # Calculer les coordonnées de l'intersection
    x1 = np.maximum(box[0], boxes[:, 0])
    y1 = np.maximum(box[1], boxes[:, 1])
    x2 = np.minimum(box[2], boxes[:, 2])
    y2 = np.minimum(box[3], boxes[:, 3])

    # Calculer l'aire de l'intersection
    w = np.maximum(0, x2 - x1)
    h = np.maximum(0, y2 - y1)
    intersection = w * h

    # Calculer l'aire de l'union
    box_area = (box[2] - box[0]) * (box[3] - box[1])
    boxes_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    union = box_area + boxes_area - intersection

    # Calculer l'IoU
    iou = intersection / union

    return iou


# Fonction d'évaluation
def evaluate_antennes(model, data_loader, device):
    """
    Évalue le modèle sur les données de test, spécifiquement pour les antennes

    Args:
        model: Modèle à évaluer
        data_loader: DataLoader pour les données d'évaluation
        device: Dispositif d'évaluation (CPU/GPU)

    Returns:
        dict: Métriques d'évaluation
    """
    model.eval()

    total_gt_antennes = 0
    total_detected_antennes = 0
    true_positives = 0
    false_positives = 0

    # Seuil IoU pour considérer une détection comme correcte
    iou_threshold = 0.5

    with torch.no_grad():
        for images, targets in tqdm(data_loader, desc="Évaluation"):
            images = list(img.to(device) for img in images)

            # Obtenir les prédictions
            outputs = model(images)

            # Pour chaque image
            for i, (output, target) in enumerate(zip(outputs, targets)):
                # Compter les antennes dans la vérité terrain
                gt_antenna_indices = (target["labels"] == 1).nonzero(as_tuple=True)[0]
                gt_antenna_boxes = target["boxes"][gt_antenna_indices].cpu().numpy()
                total_gt_antennes += len(gt_antenna_boxes)

                # Compter les antennes détectées (classe 1 avec score > 0.5)
                pred_scores = output["scores"].cpu().numpy()
                pred_labels = output["labels"].cpu().numpy()
                pred_boxes = output["boxes"].cpu().numpy()

                # Filtrer les prédictions d'antennes avec un score suffisant
                antenna_mask = (pred_labels == 1) & (pred_scores > 0.5)
                pred_antenna_boxes = pred_boxes[antenna_mask]
                total_detected_antennes += len(pred_antenna_boxes)

                # Calculer le nombre de vrais positifs
                if len(gt_antenna_boxes) > 0 and len(pred_antenna_boxes) > 0:
                    # Pour chaque boîte de vérité terrain
                    for gt_box in gt_antenna_boxes:
                        # Calculer IoU avec toutes les prédictions
                        ious = calculate_iou(gt_box, pred_antenna_boxes)

                        # Si au moins une prédiction a un IoU suffisant
                        if len(ious) > 0 and np.max(ious) >= iou_threshold:
                            true_positives += 1

                # Les faux positifs sont les détections qui ne correspondent à aucune vérité terrain
                false_positives += max(0, len(pred_antenna_boxes) - min(len(gt_antenna_boxes), true_positives))

    # Calculer les métriques
    precision = true_positives / total_detected_antennes if total_detected_antennes > 0 else 0
    recall = true_positives / total_gt_antennes if total_gt_antennes > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    metrics = {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "true_positives": true_positives,
        "false_positives": false_positives,
        "total_gt": total_gt_antennes,
        "total_detected": total_detected_antennes
    }

    # Afficher les métriques
    print(f"Évaluation de la détection d'antennes:")
    print(f"  Antennes dans vérité terrain: {total_gt_antennes}")
    print(f"  Antennes détectées: {total_detected_antennes}")
    print(f"  Vrais positifs: {true_positives}")
    print(f"  Faux positifs: {false_positives}")
    print(f"  Précision: {precision:.4f}")
    print(f"  Rappel: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")

    return metrics


# Fonction pour prédire sur une image
def predict_antennes(model, image_path, threshold=0.5, device='cuda'):
    """
    Prédit les antennes sur une image

    Args:
        model: Modèle entraîné
        image_path: Chemin vers l'image
        threshold: Seuil de confiance pour les détections
        device: Dispositif pour l'inférence (CPU/GPU)

    Returns:
        tuple: (image, boxes, scores) - L'image et les détections d'antennes
    """
    # Charger l'image
    try:
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"Erreur lors du chargement de l'image {image_path}: {e}")
        # Créer une image noire en cas d'erreur
        image = Image.new('RGB', (100, 100), color='black')

    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)

    # Prédiction
    model.eval()
    with torch.no_grad():
        prediction = model(image_tensor)

    # Filtrer les prédictions selon le seuil
    pred_boxes = prediction[0]['boxes'][prediction[0]['scores'] > threshold]
    pred_labels = prediction[0]['labels'][prediction[0]['scores'] > threshold]
    pred_scores = prediction[0]['scores'][prediction[0]['scores'] > threshold]

    # Ne garder que les antennes (classe 1)
    antenna_mask = pred_labels == 1
    boxes = pred_boxes[antenna_mask].cpu().numpy()
    scores = pred_scores[antenna_mask].cpu().numpy()

    return image, boxes, scores


# Fonction pour visualiser les prédictions d'antennes
def visualize_antennes(image, boxes, scores, title=None, save_path=None):
    """
    Visualise les détections d'antennes sur une image

    Args:
        image: Image PIL
        boxes: Boîtes de détection d'antennes
        scores: Scores de confiance des détections
        title: Titre de la visualisation
        save_path: Chemin pour sauvegarder l'image
    """
    # Créer une copie de l'image pour dessiner dessus
    draw_image = image.copy()
    draw = ImageDraw.Draw(draw_image)

    # Couleur pour les antennes (rouge)
    color = (255, 0, 0)

    # Dessiner chaque boîte d'antenne
    for i, (box, score) in enumerate(zip(boxes, scores)):
        # Dessiner le rectangle
        draw.rectangle(
            [(box[0], box[1]), (box[2], box[3])],
            outline=color,
            width=3
        )

        # Ajouter le score
        text = f"Antenne: {score:.2f}"
        draw.text((box[0], box[1] - 15), text, fill=color)

    # Afficher l'image
    plt.figure(figsize=(12, 8))
    plt.imshow(draw_image)
    if title:
        plt.title(title)
    plt.axis('off')

    # Sauvegarder si un chemin est spécifié
    if save_path:
        draw_image.save(save_path)

    plt.tight_layout()
    plt.show()


# Fonction principale d'entraînement
def train_antennes_model(json_file, img_dir, num_epochs=10, batch_size=2):
    """
    Entraîne un modèle de détection d'antennes

    Args:
        json_file: Chemin vers le fichier JSON d'annotations
        img_dir: Dossier contenant les images
        num_epochs: Nombre d'époques d'entraînement
        batch_size: Taille du batch

    Returns:
        model: Modèle entraîné
    """
    # Créer le dataset et les dataloaders
    dataset = AntennesDataset(json_file, img_dir)

    # Vérifier s'il y a des images valides
    if len(dataset) == 0:
        print("Aucune image valide avec des antennes dans le dataset. Impossible d'entraîner le modèle.")
        return None

    # Diviser en train/test (80%/20%)
    indices = torch.randperm(len(dataset)).tolist()
    train_size = int(len(dataset) * 0.8)

    # Assurez-vous que train_size est d'au moins 1
    train_size = max(1, train_size)

    # S'il n'y a qu'une seule image, utilisez-la pour l'entraînement
    if len(dataset) == 1:
        train_indices = indices
        test_indices = indices
    else:
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]

    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    test_dataset = torch.utils.data.Subset(dataset, test_indices)

    print(f"Split train/test: {len(train_dataset)}/{len(test_dataset)} images")

    # Définir une fonction de collate personnalisée qui filtre les images sans boîtes
    def collate_fn(batch):
        # Filtrer les éléments où il n'y a pas de boîtes
        filtered_batch = []
        for img, target in batch:
            if len(target["boxes"]) > 0:
                filtered_batch.append((img, target))

        # Si tous les éléments ont été filtrés, ajouter un élément factice
        if len(filtered_batch) == 0 and len(batch) > 0:
            img, target = batch[0]
            # Ajouter une boîte factice
            if len(target["boxes"]) == 0:
                target["boxes"] = torch.tensor([[0, 0, 10, 10]], dtype=torch.float32)
                target["labels"] = torch.tensor([0], dtype=torch.int64)  # Classe de fond
                target["area"] = torch.tensor([100], dtype=torch.float32)
                target["iscrowd"] = torch.tensor([0], dtype=torch.int64)
            filtered_batch.append((img, target))

        return tuple(zip(*filtered_batch))

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn
    )

    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn
    )

    # Créer le modèle (2 classes: fond et antenne)
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    print(f"Utilisation du dispositif: {device}")

    model = get_faster_rcnn_model(num_classes=2)
    model.to(device)

    # Optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

    # Learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

    # Historique des pertes
    loss_history = []

    # Entraînement
    for epoch in range(num_epochs):
        try:
            loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
            loss_history.append(loss)
            lr_scheduler.step()

            # Évaluation périodique
            if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:
                print("Évaluation du modèle...")
                metrics = evaluate_antennes(model, test_loader, device)
                print(f"Évaluation terminée.")
        except Exception as e:
            print(f"Erreur lors de l'époque {epoch}: {e}")
            continue

    # Tracer la courbe de perte
    plt.figure(figsize=(10, 6))
    plt.plot(loss_history)
    plt.xlabel('Époque')
    plt.ylabel('Perte')
    plt.title('Historique de la perte pendant l\'entraînement')
    plt.grid(True)
    plt.savefig("loss_history.png")
    plt.close()

    # Sauvegarder le modèle
    try:
        model_save_path = f"faster_rcnn_antennes.pth"
        torch.save(model.state_dict(), model_save_path)
        print(f"Modèle sauvegardé: {model_save_path}")
    except Exception as e:
        print(f"Erreur lors de la sauvegarde du modèle: {e}")

    return model


# Fonction pour démontrer l'inférence sur quelques images
def demo_antennes_detection(model, img_dir, json_file, num_images=5, threshold=0.3, device='cuda'):
    """
    Démontre la détection d'antennes sur quelques images

    Args:
        model: Modèle entraîné
        img_dir: Dossier contenant les images
        json_file: Fichier JSON d'annotations
        num_images: Nombre d'images à tester
        threshold: Seuil de confiance pour les détections
        device: Dispositif pour l'inférence (CPU/GPU)

    Returns:
        str: Dossier contenant les résultats
    """
    if model is None:
        print("Pas de modèle disponible pour l'inférence.")
        return None

    # Charger les annotations pour vérification
    with open(json_file, 'r') as f:
        annotations = json.load(f)

    # Obtenir les chemins d'image
    img_paths = []
    for img_name in annotations.keys():
        img_path = os.path.join(img_dir, img_name)
        if os.path.exists(img_path):
            # Vérifier si cette image contient des antennes dans les annotations
            features = annotations[img_name].get("features", {})
            if "Antennes" in features and "bbox" in features["Antennes"]:
                img_paths.append(img_path)

    if not img_paths:
        print("Aucune image valide avec des antennes trouvée.")
        return None

    # Sélectionner aléatoirement quelques images
    num_images = min(num_images, len(img_paths))
    if num_images == 0:
        print("Aucune image à démontrer.")
        return None

    selected_paths = random.sample(img_paths, num_images)

    # Créer un dossier pour sauvegarder les résultats
    output_dir = "predictions_antennes"
    os.makedirs(output_dir, exist_ok=True)

    for i, img_path in enumerate(selected_paths):
        print(f"Prédiction sur image {i+1}/{len(selected_paths)}: {os.path.basename(img_path)}")

        # Prédire
        try:
            image, boxes, scores = predict_antennes(model, img_path, threshold, device)

            # Visualiser
            img_name = os.path.basename(img_path)
            save_path = os.path.join(output_dir, f"pred_{img_name}")

            visualize_antennes(
                image, boxes, scores,
                title=f"Détection d'antennes: {img_name}",
                save_path=save_path
            )

            print(f"  Nombre d'antennes détectées: {len(boxes)}")
            for j, score in enumerate(scores):
                print(f"    {j+1}. Antenne: {score:.2f}")

            print(f"  Résultat sauvegardé: {save_path}")
        except Exception as e:
            print(f"Erreur lors de l'inférence sur {img_path}: {e}")

    return output_dir


if __name__ == "__main__":
    # Configuration
    json_file = "5.json"  # Chemin vers votre fichier JSON d'annotations
    img_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"  # Dossier contenant vos images

    # Vérifier les chemins d'accès
    print(f"Vérification des chemins d'accès:")
    print(f"  Fichier JSON: {os.path.exists(json_file)}")
    print(f"  Dossier d'images: {os.path.isdir(img_dir)}")

    # Paramètres d'entraînement
    # Paramètres d'entraînement
    num_epochs = 20
    batch_size = 2

    print("\n=== Entraînement du modèle de détection d'antennes ===")
    model = train_antennes_model(json_file, img_dir, num_epochs, batch_size)

    # Démonstration sur des images
    if model is not None:
        print("\n=== Démonstration de la détection d'antennes ===")
        output_dir = demo_antennes_detection(model, img_dir, json_file, num_images=10)
        print(f"Démonstration terminée. Résultats dans: {output_dir}")

!pip install grad-cam

import os
import json
import random
import numpy as np
import torch
import torch.utils.data
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.transforms import functional as F
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import cv2
import time
from collections import defaultdict

# Pour la reproductibilité
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# Liste des espèces
SPECIES_LIST = [
    "AUTRE",    # 0
    "Cer",      # 1
    "CRY_THE",  # 2
    "HYP_MAN",  # 3
    "ISO_MIN",  # 4
    "LEP",      # 5
    "MET_AFF",  # 6
    "PAR_NOT",  # 7
    "FOND"      # 8
]

class AntennesDataset(torch.utils.data.Dataset):
    def __init__(self, json_file, img_dir, transform=None):
        """
        Dataset spécialisé pour la détection des antennes

        Args:
            json_file (string): Chemin vers le fichier JSON des annotations
            img_dir (string): Dossier contenant les images
            transform (callable, optional): Transformation à appliquer aux images
        """
        with open(json_file, 'r') as f:
            self.annotations = json.load(f)

        self.img_dir = img_dir
        self.transform = transform
        self.imgs = list(sorted(self.annotations.keys()))

        # Pour la détection des antennes, nous avons uniquement 2 classes:
        # 0 = background, 1 = antenne
        self.num_classes = 2

        print(f"Dataset chargé avec {len(self.imgs)} images")

        # Filtrer les images valides qui contiennent des annotations d'antennes
        self.valid_imgs = []
        for img_name in self.imgs:
            img_path = os.path.join(self.img_dir, img_name)
            if os.path.exists(img_path) and self._has_valid_antenna_boxes(img_name):
                self.valid_imgs.append(img_name)

        print(f"Après filtrage: {len(self.valid_imgs)} images valides avec des antennes")

    def _has_valid_antenna_boxes(self, img_name):
        """Vérifie si l'image a au moins une boîte d'antenne valide"""
        annotation = self.annotations[img_name]
        features_data = annotation.get("features", {})

        # Vérifier spécifiquement pour la caractéristique "Antennes"
        if "Antennes" in features_data and "bbox" in features_data["Antennes"]:
            return True

        return False

    def __getitem__(self, idx):
        # Charger l'image
        img_name = self.valid_imgs[idx]
        img_path = os.path.join(self.img_dir, img_name)

        try:
            img = Image.open(img_path).convert("RGB")
        except Exception as e:
            print(f"Erreur lors du chargement de l'image {img_path}: {e}")
            # Créer une image noire en cas d'erreur
            img = Image.new('RGB', (100, 100), color='black')

        # Obtenir les dimensions de l'image
        width, height = img.size

        # Préparer les annotations de cette image
        annotations = self.annotations[img_name]
        features_data = annotations.get("features", {})

        boxes = []
        labels = []

        # Rechercher uniquement les antennes
        if "Antennes" in features_data and "bbox" in features_data["Antennes"]:
            feature_info = features_data["Antennes"]
            bbox = feature_info["bbox"]

            # Vérifier si le format est YOLO (center_x, center_y, width, height)
            if "center_x" in bbox and "center_y" in bbox and "width" in bbox and "height" in bbox:
                # Convertir de format YOLO à format (x_min, y_min, x_max, y_max)
                center_x = bbox["center_x"] * width
                center_y = bbox["center_y"] * height
                box_width = bbox["width"] * width
                box_height = bbox["height"] * height

                x_min = max(0, center_x - box_width / 2)
                y_min = max(0, center_y - box_height / 2)
                x_max = min(width, center_x + box_width / 2)
                y_max = min(height, center_y + box_height / 2)

                # Ignorer les boîtes trop petites ou invalides
                if not (x_max <= x_min or y_max <= y_min or x_max <= 0 or y_max <= 0 or x_min >= width or y_min >= height):
                    boxes.append([x_min, y_min, x_max, y_max])
                    labels.append(1)  # Classe 1 = antenne

        # S'assurer qu'il y a au moins une boîte valide
        if len(boxes) == 0:
            # Créer une boîte fictive pour éviter les erreurs
            boxes.append([0, 0, 10, 10])
            labels.append(0)  # Classe 0 = fond

        # Convertir en tenseurs
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # Créer le dictionnaire des cibles
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = torch.tensor([idx])
        target["area"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        target["iscrowd"] = torch.zeros((len(boxes),), dtype=torch.int64)

        # Appliquer les transformations
        if self.transform is not None:
            img = self.transform(img)
        else:
            img = F.to_tensor(img)

        return img, target

    def __len__(self):
        return len(self.valid_imgs)

    def get_img_name(self, idx):
        """Récupère le nom de l'image à partir de son index"""
        return self.valid_imgs[idx]


# Fonction pour créer le modèle Faster R-CNN
def get_faster_rcnn_model(num_classes=2):
    """
    Crée un modèle Faster R-CNN pour la détection d'antennes

    Args:
        num_classes (int): Nombre de classes (2 pour ce cas: fond et antenne)

    Returns:
        model: Modèle Faster R-CNN configuré
    """
    # Charger un modèle Faster R-CNN pré-entraîné
    model = fasterrcnn_resnet50_fpn(pretrained=True)

    # Remplacer la boîte de classification avec une nouvelle adaptée au nombre de classes
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model


# Fonction d'entraînement
def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):
    """
    Entraîne le modèle pendant une époque

    Args:
        model: Modèle à entraîner
        optimizer: Optimiseur
        data_loader: DataLoader pour les données d'entraînement
        device: Dispositif d'entraînement (CPU/GPU)
        epoch: Numéro de l'époque
        print_freq: Fréquence d'affichage des pertes

    Returns:
        float: Perte moyenne pour cette époque
    """
    model.train()

    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1.0 / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=warmup_factor, total_iters=warmup_iters
        )

    running_loss = 0.0
    running_loss_classifier = 0.0
    running_loss_box_reg = 0.0
    running_loss_objectness = 0.0
    running_loss_rpn_box_reg = 0.0

    progress_bar = tqdm(data_loader, desc=f"Epoch {epoch}")

    for i, (images, targets) in enumerate(progress_bar):
        # Vérifier que les targets sont valides
        valid_batch = True

        for target in targets:
            if len(target["boxes"]) == 0:
                valid_batch = False
                break

        if not valid_batch:
            print(f"Batch {i} ignoré car il contient des images sans boîtes valides")
            continue

        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Forward
        loss_dict = model(images, targets)

        # Appliquer un poids plus élevé aux pertes liées aux antennes
        antenna_weight = 1.5  # Ajuster si nécessaire
        for target in targets:
            if 1 in target["labels"]:  # S'il y a des antennes dans ce batch
                loss_dict['loss_classifier'] *= antenna_weight
                loss_dict['loss_box_reg'] *= antenna_weight

        losses = sum(loss for loss in loss_dict.values())

        # Backward
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        # Mise à jour du scheduler
        if lr_scheduler is not None:
            lr_scheduler.step()

        # Enregistrer les pertes
        running_loss += losses.item()
        running_loss_classifier += loss_dict['loss_classifier'].item()
        running_loss_box_reg += loss_dict['loss_box_reg'].item()
        running_loss_objectness += loss_dict['loss_objectness'].item()
        running_loss_rpn_box_reg += loss_dict['loss_rpn_box_reg'].item()

        if i % print_freq == 0:
            progress_bar.set_postfix(loss=losses.item())

    # Calculer les moyennes
    num_batches = len(data_loader)
    if num_batches > 0:
        epoch_loss = running_loss / num_batches
        epoch_loss_classifier = running_loss_classifier / num_batches
        epoch_loss_box_reg = running_loss_box_reg / num_batches
        epoch_loss_objectness = running_loss_objectness / num_batches
        epoch_loss_rpn_box_reg = running_loss_rpn_box_reg / num_batches

        print(f"Epoch {epoch} terminée. Pertes moyennes:")
        print(f"  Total: {epoch_loss:.4f}")
        print(f"  Classifier: {epoch_loss_classifier:.4f}")
        print(f"  Box Reg: {epoch_loss_box_reg:.4f}")
        print(f"  Objectness: {epoch_loss_objectness:.4f}")
        print(f"  RPN Box Reg: {epoch_loss_rpn_box_reg:.4f}")

        return epoch_loss
    else:
        print("Aucun batch valide dans cette époque")
        return 0.0


# Fonction pour calculer l'IoU
def calculate_iou(box, boxes):
    """
    Calcule l'IoU (Intersection over Union) entre une boîte et un ensemble de boîtes

    Args:
        box: Une boîte [x1, y1, x2, y2]
        boxes: Ensemble de boîtes [[x1, y1, x2, y2], ...]

    Returns:
        array: IoUs entre la boîte et chaque boîte de l'ensemble
    """
    # Calculer les coordonnées de l'intersection
    x1 = np.maximum(box[0], boxes[:, 0])
    y1 = np.maximum(box[1], boxes[:, 1])
    x2 = np.minimum(box[2], boxes[:, 2])
    y2 = np.minimum(box[3], boxes[:, 3])

    # Calculer l'aire de l'intersection
    w = np.maximum(0, x2 - x1)
    h = np.maximum(0, y2 - y1)
    intersection = w * h

    # Calculer l'aire de l'union
    box_area = (box[2] - box[0]) * (box[3] - box[1])
    boxes_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    union = box_area + boxes_area - intersection

    # Calculer l'IoU
    iou = intersection / union

    return iou


# Fonction d'évaluation
def evaluate_antennes(model, data_loader, device):
    """
    Évalue le modèle sur les données de test, spécifiquement pour les antennes

    Args:
        model: Modèle à évaluer
        data_loader: DataLoader pour les données d'évaluation
        device: Dispositif d'évaluation (CPU/GPU)

    Returns:
        dict: Métriques d'évaluation
    """
    model.eval()

    total_gt_antennes = 0
    total_detected_antennes = 0
    true_positives = 0
    false_positives = 0

    # Seuil IoU pour considérer une détection comme correcte
    iou_threshold = 0.5

    with torch.no_grad():
        for images, targets in tqdm(data_loader, desc="Évaluation"):
            images = list(img.to(device) for img in images)

            # Obtenir les prédictions
            outputs = model(images)

            # Pour chaque image
            for i, (output, target) in enumerate(zip(outputs, targets)):
                # Compter les antennes dans la vérité terrain
                gt_antenna_indices = (target["labels"] == 1).nonzero(as_tuple=True)[0]
                gt_antenna_boxes = target["boxes"][gt_antenna_indices].cpu().numpy()
                total_gt_antennes += len(gt_antenna_boxes)

                # Compter les antennes détectées (classe 1 avec score > 0.5)
                pred_scores = output["scores"].cpu().numpy()
                pred_labels = output["labels"].cpu().numpy()
                pred_boxes = output["boxes"].cpu().numpy()

                # Filtrer les prédictions d'antennes avec un score suffisant
                antenna_mask = (pred_labels == 1) & (pred_scores > 0.5)
                pred_antenna_boxes = pred_boxes[antenna_mask]
                total_detected_antennes += len(pred_antenna_boxes)

                # Calculer le nombre de vrais positifs
                if len(gt_antenna_boxes) > 0 and len(pred_antenna_boxes) > 0:
                    # Pour chaque boîte de vérité terrain
                    for gt_box in gt_antenna_boxes:
                        # Calculer IoU avec toutes les prédictions
                        ious = calculate_iou(gt_box, pred_antenna_boxes)

                        # Si au moins une prédiction a un IoU suffisant
                        if len(ious) > 0 and np.max(ious) >= iou_threshold:
                            true_positives += 1

                # Les faux positifs sont les détections qui ne correspondent à aucune vérité terrain
                false_positives += max(0, len(pred_antenna_boxes) - min(len(gt_antenna_boxes), true_positives))

    # Calculer les métriques
    precision = true_positives / total_detected_antennes if total_detected_antennes > 0 else 0
    recall = true_positives / total_gt_antennes if total_gt_antennes > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    metrics = {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "true_positives": true_positives,
        "false_positives": false_positives,
        "total_gt": total_gt_antennes,
        "total_detected": total_detected_antennes
    }

    # Afficher les métriques
    print(f"Évaluation de la détection d'antennes:")
    print(f"  Antennes dans vérité terrain: {total_gt_antennes}")
    print(f"  Antennes détectées: {total_detected_antennes}")
    print(f"  Vrais positifs: {true_positives}")
    print(f"  Faux positifs: {false_positives}")
    print(f"  Précision: {precision:.4f}")
    print(f"  Rappel: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")

    return metrics


# Fonction pour prédire sur une image
def predict_antennes(model, image_path, threshold=0.5, device='cuda'):
    """
    Prédit les antennes sur une image

    Args:
        model: Modèle entraîné
        image_path: Chemin vers l'image
        threshold: Seuil de confiance pour les détections
        device: Dispositif pour l'inférence (CPU/GPU)

    Returns:
        tuple: (image, boxes, scores) - L'image et les détections d'antennes
    """
    # Charger l'image
    try:
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"Erreur lors du chargement de l'image {image_path}: {e}")
        # Créer une image noire en cas d'erreur
        image = Image.new('RGB', (100, 100), color='black')

    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)

    # Prédiction
    model.eval()
    with torch.no_grad():
        prediction = model(image_tensor)

    # Filtrer les prédictions selon le seuil
    pred_boxes = prediction[0]['boxes'][prediction[0]['scores'] > threshold]
    pred_labels = prediction[0]['labels'][prediction[0]['scores'] > threshold]
    pred_scores = prediction[0]['scores'][prediction[0]['scores'] > threshold]

    # Ne garder que les antennes (classe 1)
    antenna_mask = pred_labels == 1
    boxes = pred_boxes[antenna_mask].cpu().numpy()
    scores = pred_scores[antenna_mask].cpu().numpy()

    return image, boxes, scores


# Fonction pour visualiser les prédictions d'antennes
def visualize_antennes(image, boxes, scores, title=None, save_path=None):
    """
    Visualise les détections d'antennes sur une image

    Args:
        image: Image PIL
        boxes: Boîtes de détection d'antennes
        scores: Scores de confiance des détections
        title: Titre de la visualisation
        save_path: Chemin pour sauvegarder l'image
    """
    # Créer une copie de l'image pour dessiner dessus
    draw_image = image.copy()
    draw = ImageDraw.Draw(draw_image)

    # Couleur pour les antennes (rouge)
    color = (255, 0, 0)

    # Dessiner chaque boîte d'antenne
    for i, (box, score) in enumerate(zip(boxes, scores)):
        # Dessiner le rectangle
        draw.rectangle(
            [(box[0], box[1]), (box[2], box[3])],
            outline=color,
            width=3
        )

        # Ajouter le score
        text = f"Antenne: {score:.2f}"
        draw.text((box[0], box[1] - 15), text, fill=color)

    # Afficher l'image
    plt.figure(figsize=(12, 8))
    plt.imshow(draw_image)
    if title:
        plt.title(title)
    plt.axis('off')

    # Sauvegarder si un chemin est spécifié
    if save_path:
        draw_image.save(save_path)

    plt.tight_layout()
    plt.show()


# Fonction principale d'entraînement avec modèle personnalisable
def train_antennes_model(model, json_file, img_dir, num_epochs=10, batch_size=2, device='cuda'):
    """
    Entraîne un modèle de détection d'antennes

    Args:
        model: Modèle à entraîner (doit être compatible avec Faster R-CNN)
        json_file: Chemin vers le fichier JSON d'annotations
        img_dir: Dossier contenant les images
        num_epochs: Nombre d'époques d'entraînement
        batch_size: Taille du batch
        device: Dispositif d'entraînement ('cuda' ou 'cpu')

    Returns:
        model: Modèle entraîné
        dict: Historique d'entraînement
    """
    # Créer le dataset et les dataloaders
    dataset = AntennesDataset(json_file, img_dir)

    # Vérifier s'il y a des images valides
    if len(dataset) == 0:
        print("Aucune image valide avec des antennes dans le dataset. Impossible d'entraîner le modèle.")
        return None, None

    # Diviser en train/test (80%/20%)
    indices = torch.randperm(len(dataset)).tolist()
    train_size = int(len(dataset) * 0.8)

    # Assurez-vous que train_size est d'au moins 1
    train_size = max(1, train_size)

    # S'il n'y a qu'une seule image, utilisez-la pour l'entraînement
    if len(dataset) == 1:
        train_indices = indices
        test_indices = indices
    else:
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]

    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    test_dataset = torch.utils.data.Subset(dataset, test_indices)

    print(f"Split train/test: {len(train_dataset)}/{len(test_dataset)} images")

    # Définir une fonction de collate personnalisée qui filtre les images sans boîtes
    def collate_fn(batch):
        # Filtrer les éléments où il n'y a pas de boîtes
        filtered_batch = []
        for img, target in batch:
            if len(target["boxes"]) > 0:
                filtered_batch.append((img, target))

        # Si tous les éléments ont été filtrés, ajouter un élément factice
        if len(filtered_batch) == 0 and len(batch) > 0:
            img, target = batch[0]
            # Ajouter une boîte factice
            if len(target["boxes"]) == 0:
                target["boxes"] = torch.tensor([[0, 0, 10, 10]], dtype=torch.float32)
                target["labels"] = torch.tensor([0], dtype=torch.int64)  # Classe de fond
                target["area"] = torch.tensor([100], dtype=torch.float32)
                target["iscrowd"] = torch.tensor([0], dtype=torch.int64)
            filtered_batch.append((img, target))

        return tuple(zip(*filtered_batch))

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn
    )

    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn
    )

    # Assurer que le modèle est sur le bon appareil
    device = torch.device(device)
    model.to(device)

    # Optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

    # Learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

    # Historique des pertes et métriques
    history = {
        'loss': [],
        'metrics': []
    }

    # Entraînement
    for epoch in range(num_epochs):
        try:
            loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
            history['loss'].append(loss)
            lr_scheduler.step()

            # Évaluation périodique
            if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:
                print("Évaluation du modèle...")
                metrics = evaluate_antennes(model, test_loader, device)
                history['metrics'].append(metrics)
                print(f"Évaluation terminée.")
        except Exception as e:
            print(f"Erreur lors de l'époque {epoch}: {e}")
            continue

    # Tracer la courbe de perte
    plt.figure(figsize=(10, 6))
    plt.plot(history['loss'])
    plt.xlabel('Époque')
    plt.ylabel('Perte')
    plt.title('Historique de la perte pendant l\'entraînement')
    plt.grid(True)
    plt.savefig("loss_history.png")
    plt.close()

    # Sauvegarder le modèle
    try:
        model_save_path = f"faster_rcnn_antennes.pth"
        torch.save(model.state_dict(), model_save_path)
        print(f"Modèle sauvegardé: {model_save_path}")
    except Exception as e:
        print(f"Erreur lors de la sauvegarde du modèle: {e}")

    return model, history


# Fonction pour démontrer l'inférence sur quelques images
def demo_antennes_detection(model, img_dir, json_file, num_images=5, threshold=0.3, device='cuda'):
    """
    Démontre la détection d'antennes sur quelques images

    Args:
        model: Modèle entraîné
        img_dir: Dossier contenant les images
        json_file: Fichier JSON d'annotations
        num_images: Nombre d'images à tester
        threshold: Seuil de confiance pour les détections
        device: Dispositif pour l'inférence (CPU/GPU)

    Returns:
        str: Dossier contenant les résultats
    """
    if model is None:
        print("Pas de modèle disponible pour l'inférence.")
        return None

    # Charger les annotations pour vérification
    with open(json_file, 'r') as f:
        annotations = json.load(f)

    # Obtenir les chemins d'image
    img_paths = []
    for img_name in annotations.keys():
        img_path = os.path.join(img_dir, img_name)
        if os.path.exists(img_path):
            # Vérifier si cette image contient des antennes dans les annotations
            features = annotations[img_name].get("features", {})
            if "Antennes" in features and "bbox" in features["Antennes"]:
                img_paths.append(img_path)

    if not img_paths:
        print("Aucune image valide avec des antennes trouvée.")
        return None

    # Sélectionner aléatoirement quelques images
    num_images = min(num_images, len(img_paths))
    if num_images == 0:
        print("Aucune image à démontrer.")
        return None

    selected_paths = random.sample(img_paths, num_images)

    # Créer un dossier pour sauvegarder les résultats
    output_dir = "predictions_antennes"
    os.makedirs(output_dir, exist_ok=True)

    for i, img_path in enumerate(selected_paths):
        print(f"Prédiction sur image {i+1}/{len(selected_paths)}: {os.path.basename(img_path)}")

        # Prédire
        try:
            image, boxes, scores = predict_antennes(model, img_path, threshold, device)

            # Visualiser
            img_name = os.path.basename(img_path)
            save_path = os.path.join(output_dir, f"pred_{img_name}")

            visualize_antennes(
                image, boxes, scores,
                title=f"Détection d'antennes: {img_name}",
                save_path=save_path
            )

            print(f"  Nombre d'antennes détectées: {len(boxes)}")
            for j, score in enumerate(scores):
                print(f"    {j+1}. Antenne: {score:.2f}")

            print(f"  Résultat sauvegardé: {save_path}")
        except Exception as e:
            print(f"Erreur lors de l'inférence sur {img_path}: {e}")

    return output_dir


if __name__ == "__main__":
    # Configuration
    json_file = "5.json"  # Chemin vers votre fichier JSON d'annotations
    img_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"  # Dossier contenant vos images

    # Vérifier les chemins d'accès
    print(f"Vérification des chemins d'accès:")
    print(f"  Fichier JSON: {os.path.exists(json_file)}")
    print(f"  Dossier d'images: {os.path.isdir(img_dir)}")

    # Paramètres d'entraînement
    num_epochs = 20
    batch_size = 2

    # Créer le dataset
    dataset = AntennesDataset(json_file, img_dir)

    # Vérifier s'il y a des images valides
    if len(dataset) == 0:
        print("Aucune image valide avec des antennes dans le dataset. Impossible de continuer.")
        exit()

    # Diviser en train/test (80%/20%)
    indices = torch.randperm(len(dataset)).tolist()
    train_size = int(len(dataset) * 0.8)

    # S'assurer que train_size est d'au moins 1
    train_size = max(1, train_size)

    # S'il n'y a qu'une seule image, utilisez-la pour l'entraînement
    if len(dataset) == 1:
        train_indices = indices
        test_indices = indices
    else:
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]

    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    test_dataset = torch.utils.data.Subset(dataset, test_indices)

    print(f"Split train/test: {len(train_dataset)}/{len(test_dataset)} images")

    # Définir une fonction de collate personnalisée qui filtre les images sans boîtes
    def collate_fn(batch):
        # Filtrer les éléments où il n'y a pas de boîtes
        filtered_batch = []
        for img, target in batch:
            if len(target["boxes"]) > 0:
                filtered_batch.append((img, target))

        # Si tous les éléments ont été filtrés, ajouter un élément factice
        if len(filtered_batch) == 0 and len(batch) > 0:
            img, target = batch[0]
            # Ajouter une boîte factice
            if len(target["boxes"]) == 0:
                target["boxes"] = torch.tensor([[0, 0, 10, 10]], dtype=torch.float32)
                target["labels"] = torch.tensor([0], dtype=torch.int64)  # Classe de fond
                target["area"] = torch.tensor([100], dtype=torch.float32)
                target["iscrowd"] = torch.tensor([0], dtype=torch.int64)
            filtered_batch.append((img, target))

        return tuple(zip(*filtered_batch))

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn
    )

    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn
    )

    # Détecter le dispositif
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    print(f"Utilisation du dispositif: {device}")

    # Créer un modèle pour la détection d'antennes uniquement (2 classes: fond et antenne)
    model = get_faster_rcnn_model(num_classes=2)
    model.to(device)

    # Optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

    # Learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

    # Historique des pertes
    loss_history = []

    # Entraînement
    print("\n=== Entraînement du modèle de détection d'antennes ===")
    for epoch in range(num_epochs):
        try:
            # Entraîner pour une époque
            loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
            loss_history.append(loss)
            lr_scheduler.step()

            # Évaluation périodique
            if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:
                print("Évaluation du modèle...")
                metrics = evaluate_antennes(model, test_loader, device)
                print(f"Évaluation terminée.")
        except Exception as e:
            print(f"Erreur lors de l'époque {epoch}: {e}")
            continue

    # Tracer la courbe de perte
    plt.figure(figsize=(10, 6))
    plt.plot(loss_history)
    plt.xlabel('Époque')
    plt.ylabel('Perte')
    plt.title('Historique de la perte pendant l\'entraînement')
    plt.grid(True)
    plt.savefig("loss_history.png")
    plt.close()

    # Sauvegarder le modèle
    try:
        model_save_path = f"faster_rcnn_antennes.pth"
        torch.save(model.state_dict(), model_save_path)
        print(f"Modèle sauvegardé: {model_save_path}")
    except Exception as e:
        print(f"Erreur lors de la sauvegarde du modèle: {e}")

    # Démonstration sur des images
    print("\n=== Démonstration de la détection d'antennes ===")
    output_dir = demo_antennes_detection(model, img_dir, json_file, num_images=10, threshold=0.3, device=device)
    print(f"Démonstration terminée. Résultats dans: {output_dir}")



