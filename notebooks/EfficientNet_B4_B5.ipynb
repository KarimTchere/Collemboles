{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 97238,
          "databundleVersionId": 11572320,
          "sourceType": "competition"
        },
        {
          "sourceId": 11208996,
          "sourceType": "datasetVersion",
          "datasetId": 6999118
        },
        {
          "sourceId": 11209176,
          "sourceType": "datasetVersion",
          "datasetId": 6999248
        },
        {
          "sourceId": 11210088,
          "sourceType": "datasetVersion",
          "datasetId": 6999884
        },
        {
          "sourceId": 11209758,
          "sourceType": "datasetVersion",
          "datasetId": 6999663
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Model EfficicientNet-B4-B5**"
      ],
      "metadata": {
        "id": "gL4QOq1bNz9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd; df = pd.read_csv(\"/kaggle/input/data-final-augmente/df_final_augmente.csv\")\n",
        "import os\n",
        "df['cropped_image_path'] = df['cropped_image_path'].replace({\n",
        "    '/content/drive/MyDrive/data/cropped/': '/kaggle/input/dataset/cropped/cropped/',\n",
        "    '/content/drive/MyDrive/data/augmentation/': '/kaggle/input/dataset/augmentation/augmentation/',\n",
        "    '/content/drive/MyDrive/data/fond/': '/kaggle/input/fond-crees/fond/'\n",
        "}, regex=True)\n",
        "\n",
        "df['path_exists'] = df['cropped_image_path'].apply(os.path.exists)\n",
        "\n",
        "invalid_paths = df[~df['path_exists']]\n",
        "if not invalid_paths.empty:\n",
        "    print(\"Les chemins suivants sont invalides :\")\n",
        "    print(invalid_paths[['cropped_image_path']])\n",
        "else:\n",
        "    print(\"Tous les chemins sont valides !\")\n",
        "df.head(3)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T19:33:58.880384Z",
          "iopub.execute_input": "2025-03-30T19:33:58.880742Z",
          "iopub.status.idle": "2025-03-30T19:34:05.105249Z",
          "shell.execute_reply.started": "2025-03-30T19:33:58.880709Z",
          "shell.execute_reply": "2025-03-30T19:34:05.104308Z"
        },
        "id": "p2VcWE8jNywv",
        "outputId": "e2e25d8f-dce7-4909-983e-7235b5ae5364"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Tous les chemins sont valides !\n",
          "output_type": "stream"
        },
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                  cropped_image_path label_collembole  \\\n0  /kaggle/input/dataset/cropped/cropped/0.001032...            AUTRE   \n1  /kaggle/input/dataset/cropped/cropped/0.001985...            AUTRE   \n2  /kaggle/input/dataset/cropped/cropped/0.005582...            AUTRE   \n\n   path_exists  \n0         True  \n1         True  \n2         True  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cropped_image_path</th>\n      <th>label_collembole</th>\n      <th>path_exists</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/dataset/cropped/cropped/0.001032...</td>\n      <td>AUTRE</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/dataset/cropped/cropped/0.001985...</td>\n      <td>AUTRE</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/dataset/cropped/cropped/0.005582...</td>\n      <td>AUTRE</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['cropped_image_path', 'label_collembole']]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T19:34:05.106058Z",
          "iopub.execute_input": "2025-03-30T19:34:05.106277Z",
          "iopub.status.idle": "2025-03-30T19:34:05.115150Z",
          "shell.execute_reply.started": "2025-03-30T19:34:05.106260Z",
          "shell.execute_reply": "2025-03-30T19:34:05.114123Z"
        },
        "id": "-4Ah3QBtNyw1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['label_collembole'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T19:34:05.116434Z",
          "iopub.execute_input": "2025-03-30T19:34:05.116660Z",
          "iopub.status.idle": "2025-03-30T19:34:05.138102Z",
          "shell.execute_reply.started": "2025-03-30T19:34:05.116633Z",
          "shell.execute_reply": "2025-03-30T19:34:05.137280Z"
        },
        "id": "9BpwMWteNyw2",
        "outputId": "01a8a3ec-75f8-4433-ee6b-4c03c407c1af"
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "label_collembole\nAUTRE      414\nFOND       300\nCer        200\nCRY_THE    150\nPAR_NOT    150\nLEP        150\nMET_AFF    150\nHYP_MAN    150\nISO_MIN    146\nName: count, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T19:34:05.138880Z",
          "iopub.execute_input": "2025-03-30T19:34:05.139141Z",
          "iopub.status.idle": "2025-03-30T19:34:05.156799Z",
          "shell.execute_reply.started": "2025-03-30T19:34:05.139121Z",
          "shell.execute_reply": "2025-03-30T19:34:05.156090Z"
        },
        "id": "DPjZdCKdNyw3",
        "outputId": "88972a40-a2fe-4f3b-e5b0-e394969cb83d"
      },
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                     cropped_image_path label_collembole\n0     /kaggle/input/dataset/cropped/cropped/0.001032...            AUTRE\n1     /kaggle/input/dataset/cropped/cropped/0.001985...            AUTRE\n2     /kaggle/input/dataset/cropped/cropped/0.005582...            AUTRE\n3     /kaggle/input/dataset/cropped/cropped/0.005608...          ISO_MIN\n4     /kaggle/input/dataset/cropped/cropped/0.005608...            AUTRE\n...                                                 ...              ...\n1805  /kaggle/input/dataset/augmentation/augmentatio...          ISO_MIN\n1806  /kaggle/input/dataset/augmentation/augmentatio...          ISO_MIN\n1807  /kaggle/input/dataset/augmentation/augmentatio...          ISO_MIN\n1808  /kaggle/input/dataset/augmentation/augmentatio...          ISO_MIN\n1809  /kaggle/input/dataset/augmentation/augmentatio...          ISO_MIN\n\n[1810 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cropped_image_path</th>\n      <th>label_collembole</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/dataset/cropped/cropped/0.001032...</td>\n      <td>AUTRE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/dataset/cropped/cropped/0.001985...</td>\n      <td>AUTRE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/dataset/cropped/cropped/0.005582...</td>\n      <td>AUTRE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/dataset/cropped/cropped/0.005608...</td>\n      <td>ISO_MIN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/dataset/cropped/cropped/0.005608...</td>\n      <td>AUTRE</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1805</th>\n      <td>/kaggle/input/dataset/augmentation/augmentatio...</td>\n      <td>ISO_MIN</td>\n    </tr>\n    <tr>\n      <th>1806</th>\n      <td>/kaggle/input/dataset/augmentation/augmentatio...</td>\n      <td>ISO_MIN</td>\n    </tr>\n    <tr>\n      <th>1807</th>\n      <td>/kaggle/input/dataset/augmentation/augmentatio...</td>\n      <td>ISO_MIN</td>\n    </tr>\n    <tr>\n      <th>1808</th>\n      <td>/kaggle/input/dataset/augmentation/augmentatio...</td>\n      <td>ISO_MIN</td>\n    </tr>\n    <tr>\n      <th>1809</th>\n      <td>/kaggle/input/dataset/augmentation/augmentatio...</td>\n      <td>ISO_MIN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1810 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EfficientNet-B4**"
      ],
      "metadata": {
        "id": "Yz8o9DFmOVj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet-pytorch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T19:35:17.402218Z",
          "iopub.execute_input": "2025-03-30T19:35:17.402520Z",
          "iopub.status.idle": "2025-03-30T19:35:23.962074Z",
          "shell.execute_reply.started": "2025-03-30T19:35:17.402498Z",
          "shell.execute_reply": "2025-03-30T19:35:23.961186Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "ngsEZIwGNyw8",
        "outputId": "5a87ad40-6ad8-413e-af16-ca9800ed045f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting efficientnet-pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch) (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch) (3.0.2)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=d2349be5fb451e0c6c36627d256bfac956200a3f4da03b9c0afc726786a52b8d\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from torchmetrics.classification import MulticlassF1Score\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T19:35:29.641131Z",
          "iopub.execute_input": "2025-03-30T19:35:29.641452Z",
          "iopub.status.idle": "2025-03-30T19:35:34.340064Z",
          "shell.execute_reply.started": "2025-03-30T19:35:29.641426Z",
          "shell.execute_reply": "2025-03-30T19:35:34.339134Z"
        },
        "id": "8-mIHWkgNyw-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Configuration initiale\n",
        "class Config:\n",
        "    SEED = 42\n",
        "    IMG_SIZE = 300  # Augmenté pour mieux capturer les détails\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 4\n",
        "    NUM_EPOCHS = 50\n",
        "    LR = 1e-4\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    TTA_STEPS = 5  # Test Time Augmentation steps\n",
        "\n",
        "pl.seed_everything(Config.SEED)\n",
        "\n",
        "# 2. Dataset et DataLoaders avec gestion améliorée du déséquilibre\n",
        "class CollemboleDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.iloc[idx]['cropped_image_path']\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.df.iloc[idx]['label_idx']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Augmentations plus robustes\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(Config.IMG_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(Config.IMG_SIZE + 50),\n",
        "    transforms.CenterCrop(Config.IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 3. Modèle avec gestion avancée des classes déséquilibrées\n",
        "class CollemboleModel(pl.LightningModule):\n",
        "    def __init__(self, num_classes, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=num_classes)\n",
        "        self.class_weights = class_weights\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        self.train_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
        "        self.val_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        f1 = self.train_f1(preds, y)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('train_f1', f1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        f1 = self.val_f1(preds, y)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_f1', f1, prog_bar=True)\n",
        "        return {'val_loss': loss, 'val_f1': f1, 'preds': preds, 'targets': y}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='max', factor=0.1, patience=3, verbose=True\n",
        "        )\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': scheduler,\n",
        "                'monitor': 'val_f1',\n",
        "            }\n",
        "        }\n",
        "\n",
        "# 4. Pipeline complet\n",
        "def main():\n",
        "    # Chargement des données\n",
        "    classes = df['label_collembole'].unique()\n",
        "    num_classes = len(classes)\n",
        "    class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "    idx_to_class = {i: cls for i, cls in enumerate(classes)}\n",
        "    df['label_idx'] = df['label_collembole'].map(class_to_idx)\n",
        "\n",
        "    # Calcul des poids de classe\n",
        "    class_counts = df['label_idx'].value_counts().sort_index().values\n",
        "    class_weights = 1. / class_counts\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "    # Split des données\n",
        "    train_df, val_df = train_test_split(\n",
        "        df, test_size=0.2, stratify=df['label_collembole'], random_state=Config.SEED\n",
        "    )\n",
        "\n",
        "    # Création des datasets\n",
        "    train_dataset = CollemboleDataset(train_df, transform=train_transform)\n",
        "    val_dataset = CollemboleDataset(val_df, transform=val_transform)\n",
        "\n",
        "    # Sampler pour le déséquilibre\n",
        "    sample_weights = class_weights[train_df['label_idx'].values]\n",
        "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=Config.BATCH_SIZE, sampler=sampler,\n",
        "        num_workers=Config.NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor=\"val_f1\", patience=7, mode=\"max\", verbose=True)\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        monitor=\"val_f1\", mode=\"max\", save_top_k=1,\n",
        "        filename=\"best_model-{epoch}-{val_f1:.2f}\"\n",
        "    )\n",
        "\n",
        "    # Entraînement\n",
        "    model = CollemboleModel(num_classes=num_classes, class_weights=class_weights)\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=Config.NUM_EPOCHS,\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        accelerator='auto',\n",
        "        devices=1,\n",
        "        precision=16,  # Mixed precision pour accélérer l'entraînement\n",
        "        deterministic=True\n",
        "    )\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "    # 5. Évaluation et prédictions avec TTA\n",
        "    class TestDataset(Dataset):\n",
        "        def __init__(self, test_dir, transform=None):\n",
        "            self.test_dir = test_dir\n",
        "            self.image_files = sorted([f for f in os.listdir(test_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.image_files)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            img_path = os.path.join(self.test_dir, self.image_files[idx])\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, self.image_files[idx]\n",
        "\n",
        "    # Chargement du meilleur modèle\n",
        "    best_model = CollemboleModel.load_from_checkpoint(\n",
        "        trainer.checkpoint_callback.best_model_path,\n",
        "        num_classes=num_classes,\n",
        "        class_weights=class_weights\n",
        "    ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    best_model.eval()\n",
        "\n",
        "    # Transformations pour TTA\n",
        "    tta_transforms = [\n",
        "        transforms.Compose([\n",
        "            transforms.Resize(Config.IMG_SIZE + 50),\n",
        "            transforms.CenterCrop(Config.IMG_SIZE),\n",
        "            transforms.RandomHorizontalFlip(p=1.0),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ]),\n",
        "        transforms.Compose([\n",
        "            transforms.Resize(Config.IMG_SIZE + 50),\n",
        "            transforms.CenterCrop(Config.IMG_SIZE),\n",
        "            transforms.RandomRotation(30),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "    ]\n",
        "\n",
        "    def predict_with_tta(model, image_path, n_tta=Config.TTA_STEPS):\n",
        "        original_image = Image.open(image_path).convert('RGB')\n",
        "        preds = []\n",
        "\n",
        "        # Prédiction originale\n",
        "        with torch.no_grad():\n",
        "            img = val_transform(original_image).unsqueeze(0).to(best_model.device)\n",
        "            logits = model(img)\n",
        "            preds.append(logits.softmax(dim=1))\n",
        "\n",
        "        # TTA\n",
        "        for _ in range(n_tta - 1):\n",
        "            tta_transform = tta_transforms[np.random.randint(0, len(tta_transforms))]\n",
        "            with torch.no_grad():\n",
        "                img = tta_transform(original_image).unsqueeze(0).to(best_model.device)\n",
        "                logits = model(img)\n",
        "                preds.append(logits.softmax(dim=1))\n",
        "\n",
        "        # Moyenne des prédictions\n",
        "        avg_probs = torch.mean(torch.cat(preds, dim=0), dim=0)\n",
        "        return avg_probs.argmax().item()\n",
        "\n",
        "    # Prédictions pour Kaggle\n",
        "    test_dir = \"/kaggle/input/datatest/datatest\"\n",
        "    test_dataset = TestDataset(test_dir)\n",
        "    filenames = test_dataset.image_files\n",
        "\n",
        "    predictions = []\n",
        "    for filename in tqdm(filenames, desc=\"Processing Test Images\"):\n",
        "        img_path = os.path.join(test_dir, filename)\n",
        "        pred_idx = predict_with_tta(best_model, img_path)\n",
        "        predictions.append(idx_to_class[pred_idx])\n",
        "\n",
        "    # Sauvegarde des résultats\n",
        "    submission = pd.DataFrame({\n",
        "        'image_filename': filenames,\n",
        "        'predicted_label': predictions\n",
        "    })\n",
        "    submission.to_csv('kaggle_submission_robust.csv', index=False)\n",
        "    print(\"Soumission Kaggle générée avec succès!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T20:07:27.885427Z",
          "iopub.execute_input": "2025-03-30T20:07:27.885733Z",
          "iopub.status.idle": "2025-03-30T20:26:42.073423Z",
          "shell.execute_reply.started": "2025-03-30T20:07:27.885711Z",
          "shell.execute_reply": "2025-03-30T20:26:42.072555Z"
        },
        "colab": {
          "referenced_widgets": [
            "",
            "53b5f399093b4dfa98c037eadb6ce214"
          ]
        },
        "id": "C5uyjx3HNyw-",
        "outputId": "913a0a94-b959-45a8-f186-e9947391db0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loaded pretrained weights for efficientnet-b4\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:572: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (46) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53b5f399093b4dfa98c037eadb6ce214"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Loaded pretrained weights for efficientnet-b4\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Processing Test Images: 100%|██████████| 1344/1344 [03:57<00:00,  5.67it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Soumission Kaggle générée avec succès!\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from torchvision.utils import make_grid\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def evaluate_model_performance(best_model, val_loader, idx_to_class):\n",
        "    \"\"\"Évalue les performances du modèle sur l'ensemble de validation\"\"\"\n",
        "    best_model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images, labels = batch\n",
        "            images = images.to(best_model.device)\n",
        "            labels = labels.to(best_model.device)\n",
        "\n",
        "            outputs = best_model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # 1. Rapport de classification\n",
        "    class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_targets, all_preds, target_names=class_names))\n",
        "\n",
        "    # 2. Matrice de confusion\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Matrice de confusion')\n",
        "    plt.xlabel('Prédictions')\n",
        "    plt.ylabel('Vraies étiquettes')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Visualisation des prédictions incorrectes\n",
        "    val_dataset = val_loader.dataset\n",
        "    incorrect_indices = [i for i, (pred, target) in enumerate(zip(all_preds, all_targets)) if pred != target]\n",
        "\n",
        "    print(f\"\\nNombre total d'erreurs: {len(incorrect_indices)}/{len(val_dataset)}\")\n",
        "    print(f\"Taux d'erreur: {len(incorrect_indices)/len(val_dataset):.2%}\")\n",
        "\n",
        "    # Afficher quelques exemples erronés\n",
        "    num_samples = min(9, len(incorrect_indices))\n",
        "    if num_samples > 0:\n",
        "        plt.figure(figsize=(15, 15))\n",
        "        for i, idx in enumerate(incorrect_indices[:num_samples]):\n",
        "            image, label = val_dataset[idx]\n",
        "            pred_label = all_preds[idx]\n",
        "\n",
        "            plt.subplot(3, 3, i+1)\n",
        "            image = image.numpy().transpose((1, 2, 0))\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            image = std * image + mean\n",
        "            image = np.clip(image, 0, 1)\n",
        "\n",
        "            plt.imshow(image)\n",
        "            plt.title(f\"Vrai: {idx_to_class[label]}\\nPrédit: {idx_to_class[pred_label]}\")\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # 4. Analyse des probabilités de prédiction\n",
        "    all_probs = np.array(all_probs)\n",
        "    max_probs = np.max(all_probs, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(max_probs, bins=20, kde=True)\n",
        "    plt.title('Distribution des probabilités maximales')\n",
        "    plt.xlabel('Probabilité maximale')\n",
        "    plt.ylabel('Fréquence')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    correct = np.array(all_preds) == np.array(all_targets)\n",
        "    sns.boxplot(x=correct, y=max_probs)\n",
        "    plt.title('Probabilités maximales: Correct vs Incorrect')\n",
        "    plt.xticks([0, 1], ['Incorrect', 'Correct'])\n",
        "    plt.ylabel('Probabilité maximale')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Analyse par classe\n",
        "    class_metrics = []\n",
        "    for class_idx in range(len(idx_to_class)):\n",
        "        class_mask = np.array(all_targets) == class_idx\n",
        "        class_correct = np.array(all_preds)[class_mask] == np.array(all_targets)[class_mask]\n",
        "        accuracy = np.mean(class_correct) if sum(class_mask) > 0 else 0\n",
        "        avg_prob = np.mean(max_probs[class_mask]) if sum(class_mask) > 0 else 0\n",
        "\n",
        "        class_metrics.append({\n",
        "            'Classe': idx_to_class[class_idx],\n",
        "            'Exemples': sum(class_mask),\n",
        "            'Précision': accuracy,\n",
        "            'Probabilité moyenne': avg_prob\n",
        "        })\n",
        "\n",
        "    metrics_df = pd.DataFrame(class_metrics)\n",
        "    print(\"\\nMétriques par classe:\")\n",
        "    display(metrics_df.sort_values('Précision', ascending=False))\n",
        "\n",
        "    return {\n",
        "        'predictions': all_preds,\n",
        "        'targets': all_targets,\n",
        "        'probabilities': all_probs,\n",
        "        'incorrect_indices': incorrect_indices,\n",
        "        'class_metrics': metrics_df\n",
        "    }\n",
        "\n",
        "# Utilisation:\n",
        "# Supposons que vous avez déjà:\n",
        "# - best_model: votre modèle entraîné chargé\n",
        "# - val_loader: le DataLoader pour l'ensemble de validation\n",
        "# - idx_to_class: le mapping des indices aux noms de classes\n",
        "\n",
        "# Charger le meilleur modèle (si ce n'est pas déjà fait)\n",
        "best_model = CollemboleModel.load_from_checkpoint(\n",
        "    trainer.checkpoint_callback.best_model_path,\n",
        "    num_classes=len(idx_to_class),\n",
        "    class_weights=class_weights\n",
        ").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Évaluer les performances\n",
        "results = evaluate_model_performance(best_model, val_loader, idx_to_class)\n",
        "\n",
        "# Analyse supplémentaire: Courbe ROC pour un problème multiclasse\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Préparation des données pour ROC\n",
        "y_test = label_binarize(results['targets'], classes=range(len(idx_to_class)))\n",
        "y_score = np.array(results['probabilities'])\n",
        "\n",
        "# Calcul des courbes ROC pour chaque classe\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(len(idx_to_class)):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Tracer toutes les courbes ROC\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = plt.cm.rainbow(np.linspace(0, 1, len(idx_to_class)))\n",
        "for i, color in zip(range(len(idx_to_class)), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label='{0} (AUC = {1:0.2f})'\n",
        "             ''.format(idx_to_class[i], roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taux de faux positifs')\n",
        "plt.ylabel('Taux de vrais positifs')\n",
        "plt.title('Courbes ROC par classe')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T20:36:40.035817Z",
          "iopub.execute_input": "2025-03-30T20:36:40.036243Z",
          "iopub.status.idle": "2025-03-30T20:36:40.084127Z",
          "shell.execute_reply.started": "2025-03-30T20:36:40.036215Z",
          "shell.execute_reply": "2025-03-30T20:36:40.082872Z"
        },
        "collapsed": true,
        "id": "uSWX4EhVNyxB",
        "outputId": "cc38411b-86a7-4fde-a98c-cb71d89df56c"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-490ee050be1d>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# Charger le meilleur modèle (si ce n'est pas déjà fait)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m best_model = CollemboleModel.load_from_checkpoint(\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_to_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mclass_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "s = pd.read_csv(\"/kaggle/working/kaggle_submission_robust.csv\")\n",
        "s"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T20:29:41.728494Z",
          "iopub.execute_input": "2025-03-30T20:29:41.728828Z",
          "iopub.status.idle": "2025-03-30T20:29:41.742967Z",
          "shell.execute_reply.started": "2025-03-30T20:29:41.728802Z",
          "shell.execute_reply": "2025-03-30T20:29:41.741953Z"
        },
        "id": "gL3_vEqYNyxD",
        "outputId": "9dbc51d6-35fb-4d78-94b7-2ca6a16fcfe3"
      },
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                         image_filename predicted_label\n0     0.000287271854623405430.157444760850390830.996...           AUTRE\n1     0.0010863036917236890.47680670826091410.132699...         HYP_MAN\n2     0.00353289859414340770.76539580712550110.01220...           AUTRE\n3     0.0042943058174115260.7384949814241050.9756010...            FOND\n4     0.0046691375801363180.53378260284225820.730659...            FOND\n...                                                 ...             ...\n1339  0.99716142613047390.197750336695433180.6459868...             Cer\n1340  0.9973257779891610.138331744890855870.49351583...            FOND\n1341  0.99770272541465340.482771084491531170.2954322...         PAR_NOT\n1342  0.9981505977730490.27455536007485760.980027279...           AUTRE\n1343  0.99898700725013080.41404113952437180.33379225...            FOND\n\n[1344 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_filename</th>\n      <th>predicted_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000287271854623405430.157444760850390830.996...</td>\n      <td>AUTRE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0010863036917236890.47680670826091410.132699...</td>\n      <td>HYP_MAN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00353289859414340770.76539580712550110.01220...</td>\n      <td>AUTRE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0042943058174115260.7384949814241050.9756010...</td>\n      <td>FOND</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0046691375801363180.53378260284225820.730659...</td>\n      <td>FOND</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1339</th>\n      <td>0.99716142613047390.197750336695433180.6459868...</td>\n      <td>Cer</td>\n    </tr>\n    <tr>\n      <th>1340</th>\n      <td>0.9973257779891610.138331744890855870.49351583...</td>\n      <td>FOND</td>\n    </tr>\n    <tr>\n      <th>1341</th>\n      <td>0.99770272541465340.482771084491531170.2954322...</td>\n      <td>PAR_NOT</td>\n    </tr>\n    <tr>\n      <th>1342</th>\n      <td>0.9981505977730490.27455536007485760.980027279...</td>\n      <td>AUTRE</td>\n    </tr>\n    <tr>\n      <th>1343</th>\n      <td>0.99898700725013080.41404113952437180.33379225...</td>\n      <td>FOND</td>\n    </tr>\n  </tbody>\n</table>\n<p>1344 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "s['predicted_label'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T20:29:59.140189Z",
          "iopub.execute_input": "2025-03-30T20:29:59.140483Z",
          "iopub.status.idle": "2025-03-30T20:29:59.147980Z",
          "shell.execute_reply.started": "2025-03-30T20:29:59.140463Z",
          "shell.execute_reply": "2025-03-30T20:29:59.146974Z"
        },
        "id": "1NyLHk_kNyxE",
        "outputId": "e9e4fe32-a227-4925-e847-84d492e24e01"
      },
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "predicted_label\nFOND       513\nAUTRE      262\nLEP        161\nCer        127\nCRY_THE     87\nISO_MIN     86\nPAR_NOT     60\nHYP_MAN     35\nMET_AFF     13\nName: count, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "s = s.rename(columns={'image_filename': 'idx', 'predicted_label': 'gt'})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T20:39:02.058568Z",
          "iopub.execute_input": "2025-03-30T20:39:02.058939Z",
          "iopub.status.idle": "2025-03-30T20:39:02.063815Z",
          "shell.execute_reply.started": "2025-03-30T20:39:02.058898Z",
          "shell.execute_reply": "2025-03-30T20:39:02.063123Z"
        },
        "id": "jRt123neNyxG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "s['idx'] = s['idx'].str.replace(r'\\.[^.]+$', '', regex=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T20:39:24.115445Z",
          "iopub.execute_input": "2025-03-30T20:39:24.115913Z",
          "iopub.status.idle": "2025-03-30T20:39:24.125948Z",
          "shell.execute_reply.started": "2025-03-30T20:39:24.115850Z",
          "shell.execute_reply": "2025-03-30T20:39:24.124947Z"
        },
        "id": "JXqJe9r3NyxH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "    'AUTRE': 0,\n",
        "    'Cer': 1,\n",
        "    'HYP_MAN': 3,\n",
        "    'ISO_MIN': 4,\n",
        "    'MET_AFF': 6,\n",
        "    'PAR_NOT': 7,\n",
        "    'CRY_THE': 2,\n",
        "    'LEP': 5,\n",
        "    'FOND': 8\n",
        "}\n",
        "\n",
        "s['gt'] = s['gt'].map(label_mapping)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T20:39:54.735813Z",
          "iopub.execute_input": "2025-03-30T20:39:54.736178Z",
          "iopub.status.idle": "2025-03-30T20:39:54.741670Z",
          "shell.execute_reply.started": "2025-03-30T20:39:54.736152Z",
          "shell.execute_reply": "2025-03-30T20:39:54.740651Z"
        },
        "id": "hSYPL6jdNyxH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "s.to_csv('model1_submission.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-30T20:40:24.499202Z",
          "iopub.execute_input": "2025-03-30T20:40:24.499515Z",
          "iopub.status.idle": "2025-03-30T20:40:24.507224Z",
          "shell.execute_reply.started": "2025-03-30T20:40:24.499489Z",
          "shell.execute_reply": "2025-03-30T20:40:24.506542Z"
        },
        "id": "d4rlyV48NyxH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EfficientNet-B5**"
      ],
      "metadata": {
        "id": "hYhCu815OhpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "        A.RandomResizedCrop(Config.IMG_SIZE, Config.IMG_SIZE, scale=(0.7, 1.0)),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.3),\n",
        "        A.Rotate(limit=30, p=0.5),\n",
        "        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.5),\n",
        "        A.GaussianBlur(blur_limit=(3, 7)),\n",
        "        A.GaussNoise(var_limit=(10, 50)),\n",
        "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.3),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "def get_val_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(Config.IMG_SIZE + 50, Config.IMG_SIZE + 50),\n",
        "        A.CenterCrop(Config.IMG_SIZE, Config.IMG_SIZE),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "class CollemboleDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.iloc[idx]['cropped_image_path']\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        label = self.df.iloc[idx]['label_idx']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "vVOSK0q6On_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "        A.RandomResizedCrop(Config.IMG_SIZE, Config.IMG_SIZE, scale=(0.7, 1.0)),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.3),\n",
        "        A.Rotate(limit=30, p=0.5),\n",
        "        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.5),\n",
        "        A.GaussianBlur(blur_limit=(3, 7)),\n",
        "        A.GaussNoise(var_limit=(10, 50)),\n",
        "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.3),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "def get_val_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(Config.IMG_SIZE + 50, Config.IMG_SIZE + 50),\n",
        "        A.CenterCrop(Config.IMG_SIZE, Config.IMG_SIZE),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "class CollemboleDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.iloc[idx]['cropped_image_path']\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        label = self.df.iloc[idx]['label_idx']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "\n",
        "        return image, label\n",
        "\n"
      ],
      "metadata": {
        "id": "DLxIbcifPpuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CollemboleModel(pl.LightningModule):\n",
        "    def __init__(self, num_classes, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b5', num_classes=num_classes)\n",
        "        self.class_weights = class_weights\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "\n",
        "        self.train_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
        "        self.val_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
        "\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.training_f1 = []\n",
        "        self.validation_f1 = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        f1 = self.train_f1(preds, y)\n",
        "\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_f1', f1, prog_bar=True)\n",
        "\n",
        "        # Stockage pour visualisation\n",
        "        self.training_loss.append(loss.detach().cpu().numpy())\n",
        "        self.training_f1.append(f1.detach().cpu().numpy())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        f1 = self.val_f1(preds, y)\n",
        "\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_f1', f1, prog_bar=True)\n",
        "\n",
        "        self.validation_loss.append(loss.detach().cpu().numpy())\n",
        "        self.validation_f1.append(f1.detach().cpu().numpy())\n",
        "\n",
        "        return {'val_loss': loss, 'val_f1': f1, 'preds': preds, 'targets': y}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.NUM_EPOCHS, eta_min=1e-6)\n",
        "        return [optimizer], [scheduler]"
      ],
      "metadata": {
        "id": "M6N-UOMHPrs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #%% Cell 4: Entraînement du modèle\n",
        "\n",
        "classes = df['label_collembole'].unique()\n",
        "num_classes = len(classes)\n",
        "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "idx_to_class = {i: cls for i, cls in enumerate(classes)}\n",
        "df['label_idx'] = df['label_collembole'].map(class_to_idx)\n",
        "\n",
        "# Calcul des poids de classe\n",
        "class_counts = df['label_idx'].value_counts().sort_index().values\n",
        "class_weights = 1 / (class_counts ** 0.75)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Split stratifié\n",
        "train_df, val_df = train_test_split(\n",
        "    df, test_size=0.15, stratify=df['label_collembole'], random_state=Config.SEED\n",
        ")\n",
        "\n",
        "# Datasets et DataLoaders\n",
        "train_dataset = CollemboleDataset(train_df, transform=get_train_transforms())\n",
        "val_dataset = CollemboleDataset(val_df, transform=get_val_transforms())\n",
        "\n",
        "# Sampler pour déséquilibre\n",
        "sample_weights = class_weights[train_df['label_idx'].values]\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=Config.BATCH_SIZE, sampler=sampler,\n",
        "    num_workers=Config.NUM_WORKERS, pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False,\n",
        "    num_workers=Config.NUM_WORKERS, pin_memory=True\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor=\"val_f1\", patience=8, mode=\"max\", verbose=True)\n",
        "checkpoint = ModelCheckpoint(\n",
        "    monitor=\"val_f1\", mode=\"max\", save_top_k=2,\n",
        "    filename=\"best_model-{epoch}-{val_f1:.3f}\"\n",
        ")\n",
        "\n",
        "# Entraînement\n",
        "model = CollemboleModel(num_classes=num_classes, class_weights=class_weights)\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=Config.NUM_EPOCHS,\n",
        "    callbacks=[early_stop, checkpoint],\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    precision=16 if Config.MIXED_PRECISION else 32,\n",
        "    deterministic=True,\n",
        "    gradient_clip_val=1.0\n",
        ")\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "# Sauvegarde du modèle final\n",
        "torch.save(model.state_dict(), 'collembole_model_final.pth')"
      ],
      "metadata": {
        "id": "a0yhrjiaPt-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 6: Prédictions et soumission\n",
        "best_model = CollemboleModel.load_from_checkpoint(\n",
        "    trainer.checkpoint_callback.best_model_path,\n",
        "    num_classes=num_classes,\n",
        "    class_weights=class_weights\n",
        ").eval().to('cuda')\n",
        "\n",
        "# Transformations TTA\n",
        "tta_transforms = [\n",
        "    A.Compose([\n",
        "        A.Resize(Config.IMG_SIZE + 50, Config.IMG_SIZE + 50),\n",
        "        A.CenterCrop(Config.IMG_SIZE, Config.IMG_SIZE),\n",
        "        A.HorizontalFlip(p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ]),\n",
        "    A.Compose([\n",
        "        A.Resize(Config.IMG_SIZE + 70, Config.IMG_SIZE + 70),\n",
        "        A.RandomCrop(Config.IMG_SIZE, Config.IMG_SIZE),\n",
        "        A.Rotate(limit=25, p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ]),\n",
        "    A.Compose([\n",
        "        A.Resize(Config.IMG_SIZE, Config.IMG_SIZE),\n",
        "        A.GaussianBlur(blur_limit=(5, 9), p=1.0),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "]\n",
        "\n",
        "def predict_tta(image_path, model, n_tta=Config.TTA_STEPS):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    preds = []\n",
        "\n",
        "    # Prédiction de base\n",
        "    with torch.no_grad():\n",
        "        aug = get_val_transforms()(image=image)['image'].unsqueeze(0).to('cuda')\n",
        "        logits = model(aug)\n",
        "        preds.append(logits.softmax(dim=1))\n",
        "\n",
        "    # TTA aléatoire\n",
        "    for _ in range(n_tta - 1):\n",
        "        tta = tta_transforms[np.random.randint(0, len(tta_transforms))]\n",
        "        with torch.no_grad():\n",
        "            aug = tta(image=image)['image'].unsqueeze(0).to('cuda')\n",
        "            logits = model(aug)\n",
        "            preds.append(logits.softmax(dim=1))\n",
        "\n",
        "    avg_probs = torch.mean(torch.cat(preds, dim=0), dim=0)\n",
        "    return avg_probs.argmax().item(), avg_probs.cpu().numpy()\n",
        "\n",
        "# Génération de la soumission\n",
        "test_dir = \"/kaggle/input/dataset/datatest/datatest\"\n",
        "test_files = sorted([f for f in os.listdir(test_dir) if f.endswith(('.jpg', '.png'))])\n",
        "\n",
        "predictions = []\n",
        "all_probs = []\n",
        "for file in tqdm(test_files, desc=\"Generating TTA Predictions\"):\n",
        "    img_path = os.path.join(test_dir, file)\n",
        "    pred_idx, probs = predict_tta(img_path, best_model)\n",
        "    predictions.append(idx_to_class[pred_idx])\n",
        "    all_probs.append(probs)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'image_filename': test_files,\n",
        "    'predicted_label': predictions\n",
        "})\n",
        "submission.to_csv('merci_final.csv', index=False)\n",
        "print(\"Soumission Kaggle générée avec succès!\")\n"
      ],
      "metadata": {
        "id": "8crlqiGpPwxN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}