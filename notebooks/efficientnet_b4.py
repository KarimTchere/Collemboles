# -*- coding: utf-8 -*-
"""Efficientnet-B4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/100oB_vjUw5OIt0nYIfxRz55OJcvkugVU
"""

# Monter Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""## 4.2"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
from sklearn.metrics import f1_score, confusion_matrix
import seaborn as sns
from tqdm import tqdm
import random

# Define constants
CLASS_MAPPING = {
    0: "AUTRE",
    1: "Cer",
    2: "CRY_THE",
    3: "HYP_MAN",
    4: "ISO_MIN",
    5: "LEP",
    6: "MET_AFF",
    7: "PAR_NOT",
    8: "FOND"
}

# 1. Data Preprocessing
class CollemboleDataProcessor:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.image_files = []
        self.labels_files = []
        self.crops = []
        self.crop_labels = []

    def find_files(self):
        """Find all image and corresponding text files"""
        for file in os.listdir(self.data_dir):
            if file.endswith('.jpg') or file.endswith('.png'):
                img_path = os.path.join(self.data_dir, file)
                txt_path = os.path.join(self.data_dir, os.path.splitext(file)[0] + '.txt')

                if os.path.exists(txt_path):
                    self.image_files.append(img_path)
                    self.labels_files.append(txt_path)

        print(f"Found {len(self.image_files)} images with corresponding label files")

    def parse_label_file(self, label_file):
        """Parse YOLO+ format label file and return bounding boxes with full agreement"""
        boxes = []
        labels = []

        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()

                # Vérifier qu'il y a au moins 5 parties (labels + 4 coordonnées)
                if len(parts) < 5:
                    continue

                # Le premier élément contient toujours les labels des experts
                expert_labels = parts[0].split('_')

                # Vérifier si tous les experts sont d'accord
                if len(set(expert_labels)) == 1:
                    # Récupérer la classe
                    class_id = int(expert_labels[0])

                    # Les 4 derniers éléments sont toujours les coordonnées
                    try:
                        # Essayer de convertir les 4 derniers éléments en flottants
                        x_center = float(parts[-4])
                        y_center = float(parts[-3])
                        width = float(parts[-2])
                        height = float(parts[-1])

                        # Vérifier que les valeurs sont dans des plages raisonnables
                        if 0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1:
                            boxes.append([x_center, y_center, width, height])
                            labels.append(class_id)
                        else:
                            print(f"Valeurs de coordonnées hors limites: {parts[-4:]} dans {label_file}")
                    except ValueError:
                        # Les 4 derniers éléments ne sont pas des nombres
                        print(f"Impossible de convertir les coordonnées en nombres: {parts[-4:]} dans {label_file}")

        return boxes, labels

    def generate_background_samples(self, image_file, boxes, num_samples=3, min_size=64, max_size=224):
        """Génère des échantillons de fond (FOND) qui ne chevauchent pas les boîtes existantes"""
        try:
            img = Image.open(image_file)
            img_width, img_height = img.size

            # Convertir les coordonnées relatives en absolues pour toutes les boîtes
            abs_boxes = []
            for box in boxes:
                x_center, y_center, width, height = box
                x1 = int((x_center - width/2) * img_width)
                y1 = int((y_center - height/2) * img_height)
                x2 = int((x_center + width/2) * img_width)
                y2 = int((y_center + height/2) * img_height)
                abs_boxes.append((x1, y1, x2, y2))

            background_crops = []
            attempts = 0
            max_attempts = 50  # Évite les boucles infinies

            while len(background_crops) < num_samples and attempts < max_attempts:
                attempts += 1

                # Taille aléatoire entre min_size et max_size
                crop_width = np.random.randint(min_size, min(max_size, img_width//2))
                crop_height = np.random.randint(min_size, min(max_size, img_height//2))

                # Position aléatoire
                x1 = np.random.randint(0, img_width - crop_width)
                y1 = np.random.randint(0, img_height - crop_height)
                x2 = x1 + crop_width
                y2 = y1 + crop_height

                # Vérifie si ce crop chevauche une boîte existante
                overlap = False
                for box_x1, box_y1, box_x2, box_y2 in abs_boxes:
                    if not (x2 < box_x1 or x1 > box_x2 or y2 < box_y1 or y1 > box_y2):
                        overlap = True
                        break

                # Si pas de chevauchement, ajouter comme échantillon de fond
                if not overlap:
                    crop = img.crop((x1, y1, x2, y2))
                    crop = crop.resize((224, 224), Image.LANCZOS)
                    background_crops.append(crop)

            return background_crops
        except Exception as e:
            print(f"Erreur lors de la génération d'échantillons de fond: {e}")
            return []

    def crop_images(self, resize_size=(224, 224)):
        """Extract crops from images using bounding boxes with full agreement and generate background samples"""
        self.find_files()

        for img_path, label_path in tqdm(zip(self.image_files, self.labels_files), total=len(self.image_files)):
            try:
                img = Image.open(img_path)
                img_width, img_height = img.size

                boxes, labels = self.parse_label_file(label_path)

                for box, label in zip(boxes, labels):
                    # Convert relative coordinates to absolute
                    x_center, y_center, width, height = box
                    x_center *= img_width
                    y_center *= img_height
                    width *= img_width
                    height *= img_height

                    # Vérifier que width et height sont positifs
                    if width <= 0 or height <= 0:
                        continue

                    # Calculate box coordinates
                    x1 = int(x_center - width / 2)
                    y1 = int(y_center - height / 2)
                    x2 = int(x_center + width / 2)
                    y2 = int(y_center + height / 2)

                    # Ensure coordinates are within image bounds
                    x1 = max(0, x1)
                    y1 = max(0, y1)
                    x2 = min(img_width, x2)
                    y2 = min(img_height, y2)

                    # S'assurer que x2 > x1 et y2 > y1
                    if x2 <= x1 or y2 <= y1:
                        continue

                    # Crop the image
                    crop = img.crop((x1, y1, x2, y2))

                    # Resize the crop
                    crop = crop.resize(resize_size, Image.LANCZOS)

                    self.crops.append(crop)
                    self.crop_labels.append(label)

                # Ne générer des échantillons de fond que pour certaines images
                # Limiter le nombre total d'échantillons de fond à environ 300
                if len(self.crop_labels) > 0 and self.crop_labels.count(8) < 300:
                    # Générer aléatoirement 0 ou 1 échantillon avec une probabilité de 0.3
                    if np.random.random() < 0.3:
                        bg_samples = self.generate_background_samples(img_path, boxes, num_samples=1)
                        for bg_sample in bg_samples:
                            self.crops.append(bg_sample)
                            self.crop_labels.append(8)  # Classe 8 = FOND

            except Exception as e:
                print(f"Error processing {img_path}: {e}")

        print(f"Generated {len(self.crops)} crops (including background samples)")

        # Afficher la distribution des classes
        unique_labels, counts = np.unique(self.crop_labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = CLASS_MAPPING.get(label, f"Class {label}")
            print(f"  {class_name} (ID: {label}): {count} samples")

        return self.crops, self.crop_labels

    def visualize_samples(self, n_samples=5):
        """Visualize random sample crops with their labels"""
        if not self.crops:
            print("No crops available. Run crop_images() first.")
            return

        # Sélectionner des échantillons de chaque classe si possible
        samples_by_class = {}
        for i, label in enumerate(self.crop_labels):
            if label not in samples_by_class:
                samples_by_class[label] = []
            if len(samples_by_class[label]) < n_samples:
                samples_by_class[label].append(i)

        # Créer une liste de tous les échantillons à visualiser
        all_samples = []
        for label, samples in samples_by_class.items():
            all_samples.extend(samples[:min(n_samples, len(samples))])

        # Si trop d'échantillons, sélectionner aléatoirement
        if len(all_samples) > n_samples * len(samples_by_class):
            all_samples = np.random.choice(all_samples, n_samples * len(samples_by_class), replace=False)

        # Créer la figure
        n_cols = min(5, len(all_samples))
        n_rows = (len(all_samples) + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))
        axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]

        for i, idx in enumerate(all_samples):
            if i < len(axes):
                axes[i].imshow(self.crops[idx])
                label = self.crop_labels[idx]
                class_name = CLASS_MAPPING.get(label, f"Class {label}")
                axes[i].set_title(f"{class_name} (ID: {label})")
                axes[i].axis('off')

        # Masquer les axes inutilisés
        for i in range(len(all_samples), len(axes)):
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

def balance_dataset(crops, labels, target_count=None, augment=True):
    """
    Équilibre les données pour avoir le même nombre d'échantillons par classe.

    Args:
        crops: Liste d'images (PIL.Image)
        labels: Liste des étiquettes correspondantes
        target_count: Nombre cible d'échantillons par classe (si None, utilise le maximum disponible)
        augment: Si True, utilise l'augmentation de données pour les classes sous-représentées
                Si False, duplique simplement les échantillons existants

    Returns:
        balanced_crops: Liste équilibrée d'images
        balanced_labels: Liste équilibrée d'étiquettes
    """
    import numpy as np
    from PIL import Image, ImageEnhance
    import random

    # Compter les échantillons par classe
    unique_labels = np.unique(labels)
    class_counts = {}
    class_indices = {}

    # Organiser les indices par classe
    for label in unique_labels:
        indices = [i for i, l in enumerate(labels) if l == label]
        class_counts[label] = len(indices)
        class_indices[label] = indices

    print("Distribution originale des classes:")
    for label, count in class_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    # Déterminer le nombre cible d'échantillons par classe
    if target_count is None:
        # Utiliser la classe majoritaire comme cible
        target_count = max(class_counts.values())

    print(f"Cible: {target_count} échantillons par classe")

    # Fonctions d'augmentation de données
    def augment_image(img):
        """Applique une augmentation aléatoire à une image"""
        if not isinstance(img, Image.Image):
            print(f"Warning: Expected PIL Image but got {type(img)}. Skipping augmentation.")
            return img

        # Choisir une augmentation aléatoire
        aug_type = random.choice(['rotate', 'flip'])  # Simplifié pour éviter les erreurs

        if aug_type == 'rotate':
            # Rotation aléatoire entre -15 et 15 degrés
            angle = random.uniform(-15, 15)
            return img.rotate(angle, resample=Image.BICUBIC, expand=False)

        elif aug_type == 'flip':
            # Retournement horizontal
            return img.transpose(Image.FLIP_LEFT_RIGHT)

    # Créer le jeu de données équilibré
    balanced_crops = []
    balanced_labels = []

    for label in unique_labels:
        # Récupérer les indices des échantillons de cette classe
        class_idx = class_indices[label]
        class_size = len(class_idx)

        # Si la classe a suffisamment d'échantillons, sous-échantillonner
        if class_size >= target_count:
            # Choisir aléatoirement target_count échantillons
            selected_indices = np.random.choice(class_idx, target_count, replace=False)
            for idx in selected_indices:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

        # Sinon, augmenter le nombre d'échantillons
        else:
            # Ajouter tous les échantillons existants
            for idx in class_idx:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

            # Compléter avec des échantillons augmentés ou dupliqués
            samples_to_add = target_count - class_size

            if augment:
                # Augmentation de données
                for _ in range(samples_to_add):
                    # Choisir un échantillon aléatoire à augmenter
                    random_idx = class_idx[random.randint(0, class_size - 1)]
                    original_img = crops[random_idx]

                    # Appliquer l'augmentation
                    augmented_img = augment_image(original_img)

                    balanced_crops.append(augmented_img)
                    balanced_labels.append(label)
            else:
                # Simple duplication d'échantillons existants
                for i in range(samples_to_add):
                    # Sélection circulaire des échantillons
                    idx = class_idx[i % class_size]
                    balanced_crops.append(crops[idx])
                    balanced_labels.append(label)

    # Mélanger le jeu de données
    combined = list(zip(balanced_crops, balanced_labels))
    random.shuffle(combined)
    balanced_crops, balanced_labels = zip(*combined)

    # Convertir en listes
    balanced_crops = list(balanced_crops)
    balanced_labels = list(balanced_labels)

    # Vérifier la distribution finale
    final_counts = {}
    for label in balanced_labels:
        if label not in final_counts:
            final_counts[label] = 0
        final_counts[label] += 1

    print("Distribution équilibrée des classes:")
    for label, count in final_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    return balanced_crops, balanced_labels

# 2. Dataset and DataLoader
class CollemboleDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

class TestDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                           if f.endswith('.jpg') or f.endswith('.png')]
        self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in self.image_files]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        image_id = self.image_ids[idx]

        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, image_id

# 3. Model
class CollemboleClassifier(nn.Module):
    def __init__(self, num_classes=9, pretrained=True):
        super(CollemboleClassifier, self).__init__()
        # Use a pre-trained model for faster convergence
        self.model = models.efficientnet_b4(pretrained=pretrained)

        # Replace the final classification layer
        in_features = self.model.classifier[1].in_features
        self.model.classifier[1] = nn.Linear(in_features, num_classes)

    def forward(self, x):
        return self.model(x)

# 4. Training function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device='cuda'):
    model = model.to(device)
    best_val_f1 = 0.0
    best_model_weights = None

    history = {
        'train_loss': [],
        'val_loss': [],
        'train_f1': [],
        'val_f1': []
    }

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []

        for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            # Statistics
            train_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            train_preds.extend(preds.cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        train_loss = train_loss / len(train_loader.dataset)
        train_f1 = f1_score(train_targets, train_preds, average='macro')

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []

        with torch.no_grad():
            for inputs, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation"):
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Forward pass
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                # Statistics
                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_preds.extend(preds.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        val_loss = val_loss / len(val_loader.dataset)
        val_f1 = f1_score(val_targets, val_preds, average='macro')

        # Update scheduler
        scheduler.step(val_loss)

        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_weights = model.state_dict().copy()

        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1)

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')
        print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')

    # Load best model weights
    model.load_state_dict(best_model_weights)

    return model, history

# 5. Evaluation and visualization
def evaluate_model(model, dataloader, device='cuda'):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, labels in tqdm(dataloader, desc="Evaluating"):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    # Calculate F1 score
    f1 = f1_score(all_targets, all_preds, average='macro')
    f1_per_class = f1_score(all_targets, all_preds, average=None)

    # Create confusion matrix
    cm = confusion_matrix(all_targets, all_preds)

    return f1, f1_per_class, cm

def plot_confusion_matrix(cm, class_mapping):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[class_mapping[i] for i in range(len(class_mapping))],
                yticklabels=[class_mapping[i] for i in range(len(class_mapping))])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_f1'], label='Train F1')
    plt.plot(history['val_f1'], label='Validation F1')
    plt.title('F1 Score over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

# 6. Prediction for test data
def predict_test_data(model, test_loader, device='cuda'):
    model.eval()
    predictions = {}
    all_probs = {}  # Pour stocker les probabilités

    with torch.no_grad():
        for inputs, image_ids in tqdm(test_loader, desc="Predicting test data"):
            inputs = inputs.to(device)

            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(probs, 1)

            for img_id, pred, prob in zip(image_ids, preds.cpu().numpy(), probs.cpu().numpy()):
                predictions[img_id] = int(pred)
                all_probs[img_id] = prob  # Stocker les probabilités pour tous les modèles

    return predictions, all_probs

def create_submission_file(predictions, output_file='submission.csv'):
    with open(output_file, 'w') as f:
        f.write('idx,gt\n')
        for img_id, pred in predictions.items():
            f.write(f'{img_id},{pred}\n')

    print(f"Submission file created: {output_file}")

# 7. Explainability
def analyze_expert_agreement(data_dir):
    """Analyze agreement between experts in the training data"""
    agreement_stats = {
        'full_agreement': 0,
        'majority_agreement': 0,
        'split_decision': 0,
        'total_boxes': 0
    }

    class_distribution = {}
    agreement_by_class = {}  # Distribution des accords par classe

    for file in os.listdir(data_dir):
        if file.endswith('.txt'):
            txt_path = os.path.join(data_dir, file)

            with open(txt_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) < 5:
                        continue

                    agreement_stats['total_boxes'] += 1

                    # Extract expert labels
                    expert_labels = parts[0].split('_')
                    unique_labels = set(expert_labels)

                    # Count occurrences of each label
                    label_counts = {}
                    for label in expert_labels:
                        if label not in label_counts:
                            label_counts[label] = 0
                        label_counts[label] += 1

                    # Find majority label
                    majority_label = max(label_counts.items(), key=lambda x: x[1])[0]
                    majority_count = label_counts[majority_label]

                    # Update statistics
                    if len(unique_labels) == 1:
                        agreement_stats['full_agreement'] += 1

                        # Update class distribution
                        class_id = int(majority_label)
                        if class_id not in class_distribution:
                            class_distribution[class_id] = 0
                        class_distribution[class_id] += 1

                        # Mettre à jour les statistiques d'accord par classe
                        if class_id not in agreement_by_class:
                            agreement_by_class[class_id] = {
                                'full': 0, 'majority': 0, 'split': 0, 'total': 0
                            }
                        agreement_by_class[class_id]['full'] += 1
                        agreement_by_class[class_id]['total'] += 1

                    elif majority_count >= 3:
                        agreement_stats['majority_agreement'] += 1

                        # Mettre à jour les statistiques d'accord par classe
                        class_id = int(majority_label)
                        if class_id not in agreement_by_class:
                            agreement_by_class[class_id] = {
                                'full': 0, 'majority': 0, 'split': 0, 'total': 0
                            }
                        agreement_by_class[class_id]['majority'] += 1
                        agreement_by_class[class_id]['total'] += 1

                    else:
                        agreement_stats['split_decision'] += 1

                        # Mettre à jour les statistiques d'accord par classe
                        class_id = int(majority_label)
                        if class_id not in agreement_by_class:
                            agreement_by_class[class_id] = {
                                'full': 0, 'majority': 0, 'split': 0, 'total': 0
                            }
                        agreement_by_class[class_id]['split'] += 1
                        agreement_by_class[class_id]['total'] += 1

    # Calculate percentages
    total = agreement_stats['total_boxes']
    agreement_stats['full_agreement_pct'] = agreement_stats['full_agreement'] / total * 100
    agreement_stats['majority_agreement_pct'] = agreement_stats['majority_agreement'] / total * 100
    agreement_stats['split_decision_pct'] = agreement_stats['split_decision'] / total * 100

    # Format class distribution
    class_dist_formatted = {CLASS_MAPPING.get(k, f"Class {k}"): v for k, v in class_distribution.items()}

    # Formater les statistiques d'accord par classe
    agreement_by_class_formatted = {}
    for class_id, stats in agreement_by_class.items():
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        agreement_by_class_formatted[class_name] = {
            'full': stats['full'],
            'majority': stats['majority'],
            'split': stats['split'],
            'total': stats['total'],
            'full_pct': stats['full'] / stats['total'] * 100 if stats['total'] > 0 else 0,
            'majority_pct': stats['majority'] / stats['total'] * 100 if stats['total'] > 0 else 0,
            'split_pct': stats['split'] / stats['total'] * 100 if stats['total'] > 0 else 0
        }

    return agreement_stats, class_dist_formatted, agreement_by_class_formatted

def plot_agreement_stats(agreement_stats):
    labels = ['Full Agreement', 'Majority Agreement', 'Split Decision']
    values = [
        agreement_stats['full_agreement_pct'],
        agreement_stats['majority_agreement_pct'],
        agreement_stats['split_decision_pct']
    ]

    plt.figure(figsize=(10, 6))
    plt.bar(labels, values, color=['green', 'blue', 'red'])
    plt.title('Expert Agreement Statistics')
    plt.xlabel('Agreement Type')
    plt.ylabel('Percentage (%)')
    plt.ylim(0, 100)

    for i, v in enumerate(values):
        plt.text(i, v + 1, f'{v:.1f}%', ha='center')

    plt.tight_layout()
    plt.show()

def plot_class_distribution(class_distribution):
    plt.figure(figsize=(12, 6))

    classes = list(class_distribution.keys())
    counts = list(class_distribution.values())

    plt.bar(classes, counts)
    plt.title('Class Distribution (Full Agreement Cases)')
    plt.xlabel('Class')
    plt.ylabel('Count')
    plt.xticks(rotation=45, ha='right')

    for i, v in enumerate(counts):
        plt.text(i, v + 5, str(v), ha='center')

    plt.tight_layout()
    plt.show()

def plot_agreement_by_class(agreement_by_class):
    """Visualiser les statistiques d'accord pour chaque classe"""
    classes = list(agreement_by_class.keys())
    full_agreement = [stats['full_pct'] for stats in agreement_by_class.values()]
    majority_agreement = [stats['majority_pct'] for stats in agreement_by_class.values()]
    split_decision = [stats['split_pct'] for stats in agreement_by_class.values()]

    plt.figure(figsize=(14, 8))

    x = np.arange(len(classes))
    width = 0.25

    plt.bar(x - width, full_agreement, width, label='Full Agreement', color='green')
    plt.bar(x, majority_agreement, width, label='Majority Agreement', color='blue')
    plt.bar(x + width, split_decision, width, label='Split Decision', color='red')

    plt.title('Expert Agreement by Class')
    plt.xlabel('Class')
    plt.ylabel('Percentage (%)')
    plt.xticks(x, classes, rotation=45, ha='right')
    plt.legend()

    plt.tight_layout()
    plt.show()

# 8. Label correction
def suggest_label_corrections(data_dir, model, transform, device='cuda'):
    """Suggest corrections for labels with disagreement using the trained model"""
    model.eval()

    correction_suggestions = []

    for file in tqdm(os.listdir(data_dir), desc="Analyzing label disagreements"):
        if not file.endswith('.txt'):
            continue

        img_file = os.path.splitext(file)[0] + '.jpg'
        img_path = os.path.join(data_dir, img_file)
        txt_path = os.path.join(data_dir, file)

        if not os.path.exists(img_path):
            img_file = os.path.splitext(file)[0] + '.png'
            img_path = os.path.join(data_dir, img_file)

            if not os.path.exists(img_path):
                continue

        # Read the image
        try:
            img = Image.open(img_path)
            img_width, img_height = img.size

            with open(txt_path, 'r') as f:
                for line_idx, line in enumerate(f):
                    parts = line.strip().split()
                    if len(parts) < 5:
                        continue

                    # Extract expert labels
                    expert_labels = parts[0].split('_')
                    unique_labels = set(expert_labels)

                    # Skip if all experts agree
                    if len(unique_labels) == 1:
                        continue

                    try:
                        # Extract bounding box (4 derniers éléments)
                        x_center = float(parts[-4])
                        y_center = float(parts[-3])
                        width = float(parts[-2])
                        height = float(parts[-1])

                        # Vérifier que les valeurs sont dans des plages raisonnables
                        if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1):
                            print(f"Valeurs de coordonnées hors limites: {parts[-4:]} dans {txt_path}")
                            continue

                        # Convert to pixel coordinates
                        x1 = int((x_center - width/2) * img_width)
                        y1 = int((y_center - height/2) * img_height)
                        x2 = int((x_center + width/2) * img_width)
                        y2 = int((y_center + height/2) * img_height)

                        # Ensure coordinates are within image bounds
                        x1 = max(0, x1)
                        y1 = max(0, y1)
                        x2 = min(img_width, x2)
                        y2 = min(img_height, y2)

                        # S'assurer que x2 > x1 et y2 > y1
                        if x2 <= x1 or y2 <= y1:
                            continue

                        # Crop the image
                        crop = img.crop((x1, y1, x2, y2))

                        # Prepare for model
                        input_tensor = transform(crop).unsqueeze(0).to(device)

                        # Get model prediction
                        with torch.no_grad():
                            output = model(input_tensor)
                            probs = torch.nn.functional.softmax(output, dim=1)
                            confidence, prediction = torch.max(probs, 1)

                            prediction = prediction.item()
                            confidence = confidence.item()

                        # Count expert votes
                        expert_votes = {}
                        for label in expert_labels:
                            if label not in expert_votes:
                                expert_votes[label] = 0
                            expert_votes[label] += 1

                        # Get majority label
                        majority_label = max(expert_votes.items(), key=lambda x: x[1])[0]
                        majority_count = expert_votes[majority_label]

                        correction_suggestions.append({
                            'image': os.path.basename(img_path),
                            'line': line_idx + 1,
                            'expert_labels': '_'.join(expert_labels),
                            'expert_votes': expert_votes,
                            'majority_label': int(majority_label),
                            'majority_count': majority_count,
                            'model_prediction': prediction,
                            'model_confidence': confidence,
                            'box_coords': [x_center, y_center, width, height]
                        })
                    except Exception as e:
                        print(f"Erreur lors du traitement d'une ligne dans {txt_path}: {e}")
        except Exception as e:
            print(f"Error processing {img_path}: {e}")

    # Convert to DataFrame for easier analysis
    corrections_df = pd.DataFrame(correction_suggestions)

    # Add recommendation based on model confidence
    corrections_df['recommendation'] = corrections_df.apply(
        lambda row: row['model_prediction'] if row['model_confidence'] > 0.8 else row['majority_label'],
        axis=1
    )

    # Ajouter des informations sur les classes
    corrections_df['majority_class'] = corrections_df['majority_label'].apply(
        lambda x: CLASS_MAPPING.get(x, f"Class {x}")
    )
    corrections_df['model_class'] = corrections_df['model_prediction'].apply(
        lambda x: CLASS_MAPPING.get(x, f"Class {x}")
    )

    return corrections_df

def visualize_correction_examples(data_dir, corrections_df, n_samples=5):
    """Visualise quelques exemples de corrections suggérées"""
    # Sélectionner aléatoirement des exemples
    if len(corrections_df) <= n_samples:
        samples = corrections_df
    else:
        samples = corrections_df.sample(n_samples)

    fig, axes = plt.subplots(1, n_samples, figsize=(15, 5))
    if n_samples == 1:
        axes = [axes]

    for i, (_, row) in enumerate(samples.iterrows()):
        if i >= len(axes):
            break

        img_file = row['image']
        img_path = os.path.join(data_dir, img_file)

        # Vérifier si l'image existe
        if not os.path.exists(img_path):
            continue

        try:
            img = Image.open(img_path)
            img_width, img_height = img.size

            # Extraire les coordonnées de la boîte
            x_center, y_center, width, height = row['box_coords']

            # Convertir en pixels
            x1 = int((x_center - width/2) * img_width)
            y1 = int((y_center - height/2) * img_height)
            x2 = int((x_center + width/2) * img_width)
            y2 = int((y_center + height/2) * img_height)

            # S'assurer que les coordonnées sont dans les limites de l'image
            x1 = max(0, x1)
            y1 = max(0, y1)
            x2 = min(img_width, x2)
            y2 = min(img_height, y2)

            # Extraire la région
            crop = img.crop((x1, y1, x2, y2))

            # Afficher l'image
            axes[i].imshow(crop)
            axes[i].set_title(f"Experts: {row['expert_labels']}\nMaj: {row['majority_class']}\nModel: {row['model_class']} ({row['model_confidence']:.2f})")
            axes[i].axis('off')
        except Exception as e:
            print(f"Error visualizing {img_file}: {e}")

    plt.tight_layout()
    plt.show()

# 9. Main pipeline
def main():
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Set paths
    train_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"
    test_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/datatest/datatest"

    # 1. Process training data
    print("Processing training data...")
    processor = CollemboleDataProcessor(train_dir)
    crops, labels = processor.crop_images(resize_size=(224, 224))
    processor.visualize_samples(n_samples=5)

    # 2. Équilibrer le jeu de données
    print("Équilibrage du jeu de données...")
    crops_balanced, labels_balanced = balance_dataset(crops, labels, target_count=200, augment=True)

    # 3. Analyze expert agreement
    print("Analyzing expert agreement...")
    agreement_stats, class_distribution, agreement_by_class = analyze_expert_agreement(train_dir)
    print(f"Agreement statistics: {agreement_stats}")
    plot_agreement_stats(agreement_stats)
    plot_class_distribution(class_distribution)
    plot_agreement_by_class(agreement_by_class)

    # 4. Prepare data transforms
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # 5. Create datasets and split into train/val
    X = crops_balanced
    y = np.array(labels_balanced)

    # Split dataset (80% train, 20% validation)
    from sklearn.model_selection import train_test_split
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    print(f"Training set: {len(X_train)} samples")
    print(f"Validation set: {len(X_val)} samples")

    # Create datasets
    train_dataset = CollemboleDataset(X_train, y_train, transform=train_transform)
    val_dataset = CollemboleDataset(X_val, y_val, transform=val_transform)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)

    # 6. Initialize model
    model = CollemboleClassifier(num_classes=len(CLASS_MAPPING), pretrained=True)

    # 7. Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)

    # 8. Train model
    print("Training model...")
    model, history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=20,
        device=device
    )

    # Save model
    torch.save(model.state_dict(), 'collembole_classifier.pth')

    # 9. Evaluate model
    print("Evaluating model...")
    f1, f1_per_class, cm = evaluate_model(model, val_loader, device)

    print(f"Overall F1 Score: {f1:.4f}")
    print("F1 Score per class:")
    for class_id, score in enumerate(f1_per_class):
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        print(f"  {class_name}: {score:.4f}")

    plot_confusion_matrix(cm, CLASS_MAPPING)
    plot_training_history(history)

    # 10. Suggest label corrections
    print("Suggesting label corrections...")
    corrections_df = suggest_label_corrections(train_dir, model, val_transform, device)
    print(f"Found {len(corrections_df)} potential corrections")
    print(corrections_df[['image', 'expert_labels', 'majority_class', 'model_class', 'model_confidence', 'recommendation']].head())

    # Visualiser quelques exemples de corrections suggérées
    visualize_correction_examples(train_dir, corrections_df, n_samples=5)

    # Save corrections
    corrections_df.to_csv('label_corrections.csv', index=False)

    # 11. Predict test data
    print("Predicting test data...")
    test_dataset = TestDataset(test_dir, transform=val_transform)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)

    predictions, all_probs = predict_test_data(model, test_loader, device)
    create_submission_file(predictions, 'submission.csv')

    print("Done!")

if __name__ == "__main__":
    main()

"""## 4.3"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
from sklearn.metrics import f1_score, confusion_matrix
import seaborn as sns
from tqdm import tqdm
import random

# Define constants
CLASS_MAPPING = {
    0: "AUTRE",
    1: "Cer",
    2: "CRY_THE",
    3: "HYP_MAN",
    4: "ISO_MIN",
    5: "LEP",
    6: "MET_AFF",
    7: "PAR_NOT",
    8: "FOND"
}

# 1. Data Preprocessing
class CollemboleDataProcessor:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.image_files = []
        self.labels_files = []
        self.crops = []
        self.crop_labels = []

    def find_files(self):
        """Find all image and corresponding text files"""
        for file in os.listdir(self.data_dir):
            if file.endswith('.jpg') or file.endswith('.png'):
                img_path = os.path.join(self.data_dir, file)
                txt_path = os.path.join(self.data_dir, os.path.splitext(file)[0] + '.txt')

                if os.path.exists(txt_path):
                    self.image_files.append(img_path)
                    self.labels_files.append(txt_path)

        print(f"Found {len(self.image_files)} images with corresponding label files")

    def parse_label_file(self, label_file):
        """Parse YOLO+ format label file and return bounding boxes with full agreement"""
        boxes = []
        labels = []

        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()

                # Vérifier qu'il y a au moins 5 parties (labels + 4 coordonnées)
                if len(parts) < 5:
                    continue

                # Le premier élément contient toujours les labels des experts
                expert_labels = parts[0].split('_')

                # Vérifier si tous les experts sont d'accord
                if len(set(expert_labels)) == 1:
                    # Récupérer la classe
                    class_id = int(expert_labels[0])

                    # Les 4 derniers éléments sont toujours les coordonnées
                    try:
                        # Essayer de convertir les 4 derniers éléments en flottants
                        x_center = float(parts[-4])
                        y_center = float(parts[-3])
                        width = float(parts[-2])
                        height = float(parts[-1])

                        # Vérifier que les valeurs sont dans des plages raisonnables
                        if 0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1:
                            boxes.append([x_center, y_center, width, height])
                            labels.append(class_id)
                        else:
                            print(f"Valeurs de coordonnées hors limites: {parts[-4:]} dans {label_file}")
                    except ValueError:
                        # Les 4 derniers éléments ne sont pas des nombres
                        print(f"Impossible de convertir les coordonnées en nombres: {parts[-4:]} dans {label_file}")

        return boxes, labels

    def generate_background_samples(self, image_file, boxes, num_samples=3, min_size=64, max_size=224):
        """Génère des échantillons de fond (FOND) qui ne chevauchent pas les boîtes existantes"""
        try:
            img = Image.open(image_file)
            img_width, img_height = img.size

            # Convertir les coordonnées relatives en absolues pour toutes les boîtes
            abs_boxes = []
            for box in boxes:
                x_center, y_center, width, height = box
                x1 = int((x_center - width/2) * img_width)
                y1 = int((y_center - height/2) * img_height)
                x2 = int((x_center + width/2) * img_width)
                y2 = int((y_center + height/2) * img_height)
                abs_boxes.append((x1, y1, x2, y2))

            background_crops = []
            attempts = 0
            max_attempts = 50  # Évite les boucles infinies

            while len(background_crops) < num_samples and attempts < max_attempts:
                attempts += 1

                # Taille aléatoire entre min_size et max_size
                crop_width = np.random.randint(min_size, min(max_size, img_width//2))
                crop_height = np.random.randint(min_size, min(max_size, img_height//2))

                # Position aléatoire
                x1 = np.random.randint(0, img_width - crop_width)
                y1 = np.random.randint(0, img_height - crop_height)
                x2 = x1 + crop_width
                y2 = y1 + crop_height

                # Vérifie si ce crop chevauche une boîte existante
                overlap = False
                for box_x1, box_y1, box_x2, box_y2 in abs_boxes:
                    if not (x2 < box_x1 or x1 > box_x2 or y2 < box_y1 or y1 > box_y2):
                        overlap = True
                        break

                # Si pas de chevauchement, ajouter comme échantillon de fond
                if not overlap:
                    crop = img.crop((x1, y1, x2, y2))
                    crop = crop.resize((224, 224), Image.LANCZOS)
                    background_crops.append(crop)

            return background_crops
        except Exception as e:
            print(f"Erreur lors de la génération d'échantillons de fond: {e}")
            return []

    def crop_images(self, resize_size=(224, 224)):
        """Extract crops from images using bounding boxes with full agreement and generate background samples"""
        self.find_files()

        for img_path, label_path in tqdm(zip(self.image_files, self.labels_files), total=len(self.image_files)):
            try:
                img = Image.open(img_path)
                img_width, img_height = img.size

                boxes, labels = self.parse_label_file(label_path)

                for box, label in zip(boxes, labels):
                    # Convert relative coordinates to absolute
                    x_center, y_center, width, height = box
                    x_center *= img_width
                    y_center *= img_height
                    width *= img_width
                    height *= img_height

                    # Vérifier que width et height sont positifs
                    if width <= 0 or height <= 0:
                        continue

                    # Calculate box coordinates
                    x1 = int(x_center - width / 2)
                    y1 = int(y_center - height / 2)
                    x2 = int(x_center + width / 2)
                    y2 = int(y_center + height / 2)

                    # Ensure coordinates are within image bounds
                    x1 = max(0, x1)
                    y1 = max(0, y1)
                    x2 = min(img_width, x2)
                    y2 = min(img_height, y2)

                    # S'assurer que x2 > x1 et y2 > y1
                    if x2 <= x1 or y2 <= y1:
                        continue

                    # Crop the image
                    crop = img.crop((x1, y1, x2, y2))

                    # Resize the crop
                    crop = crop.resize(resize_size, Image.LANCZOS)

                    self.crops.append(crop)
                    self.crop_labels.append(label)

                # Ne générer des échantillons de fond que pour certaines images
                # Limiter le nombre total d'échantillons de fond à environ 300
                if len(self.crop_labels) > 0 and self.crop_labels.count(8) < 300:
                    # Générer aléatoirement 0 ou 1 échantillon avec une probabilité de 0.3
                    if np.random.random() < 0.3:
                        bg_samples = self.generate_background_samples(img_path, boxes, num_samples=1)
                        for bg_sample in bg_samples:
                            self.crops.append(bg_sample)
                            self.crop_labels.append(8)  # Classe 8 = FOND

            except Exception as e:
                print(f"Error processing {img_path}: {e}")

        print(f"Generated {len(self.crops)} crops (including background samples)")

        # Afficher la distribution des classes
        unique_labels, counts = np.unique(self.crop_labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = CLASS_MAPPING.get(label, f"Class {label}")
            print(f"  {class_name} (ID: {label}): {count} samples")

        return self.crops, self.crop_labels

    def visualize_samples(self, n_samples=5):
        """Visualize random sample crops with their labels"""
        if not self.crops:
            print("No crops available. Run crop_images() first.")
            return

        # Sélectionner des échantillons de chaque classe si possible
        samples_by_class = {}
        for i, label in enumerate(self.crop_labels):
            if label not in samples_by_class:
                samples_by_class[label] = []
            if len(samples_by_class[label]) < n_samples:
                samples_by_class[label].append(i)

        # Créer une liste de tous les échantillons à visualiser
        all_samples = []
        for label, samples in samples_by_class.items():
            all_samples.extend(samples[:min(n_samples, len(samples))])

        # Si trop d'échantillons, sélectionner aléatoirement
        if len(all_samples) > n_samples * len(samples_by_class):
            all_samples = np.random.choice(all_samples, n_samples * len(samples_by_class), replace=False)

        # Créer la figure
        n_cols = min(5, len(all_samples))
        n_rows = (len(all_samples) + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))
        axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]

        for i, idx in enumerate(all_samples):
            if i < len(axes):
                axes[i].imshow(self.crops[idx])
                label = self.crop_labels[idx]
                class_name = CLASS_MAPPING.get(label, f"Class {label}")
                axes[i].set_title(f"{class_name} (ID: {label})")
                axes[i].axis('off')

        # Masquer les axes inutilisés
        for i in range(len(all_samples), len(axes)):
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

def balance_dataset(crops, labels, target_count=None, augment=True):
    """
    Équilibre les données pour avoir le même nombre d'échantillons par classe.

    Args:
        crops: Liste d'images (PIL.Image)
        labels: Liste des étiquettes correspondantes
        target_count: Nombre cible d'échantillons par classe (si None, utilise le maximum disponible)
        augment: Si True, utilise l'augmentation de données pour les classes sous-représentées
                Si False, duplique simplement les échantillons existants

    Returns:
        balanced_crops: Liste équilibrée d'images
        balanced_labels: Liste équilibrée d'étiquettes
    """
    import numpy as np
    from PIL import Image, ImageEnhance
    import random

    # Compter les échantillons par classe
    unique_labels = np.unique(labels)
    class_counts = {}
    class_indices = {}

    # Organiser les indices par classe
    for label in unique_labels:
        indices = [i for i, l in enumerate(labels) if l == label]
        class_counts[label] = len(indices)
        class_indices[label] = indices

    print("Distribution originale des classes:")
    for label, count in class_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    # Déterminer le nombre cible d'échantillons par classe
    if target_count is None:
        # Utiliser la classe majoritaire comme cible
        target_count = max(class_counts.values())

    print(f"Cible: {target_count} échantillons par classe")

    # Fonctions d'augmentation de données
    def augment_image(img):
        """Applique une augmentation aléatoire à une image"""
        if not isinstance(img, Image.Image):
            print(f"Warning: Expected PIL Image but got {type(img)}. Skipping augmentation.")
            return img

        # Choisir une augmentation aléatoire
        aug_type = random.choice(['rotate', 'flip'])  # Simplifié pour éviter les erreurs

        if aug_type == 'rotate':
            # Rotation aléatoire entre -15 et 15 degrés
            angle = random.uniform(-15, 15)
            return img.rotate(angle, resample=Image.BICUBIC, expand=False)

        elif aug_type == 'flip':
            # Retournement horizontal
            return img.transpose(Image.FLIP_LEFT_RIGHT)

    # Créer le jeu de données équilibré
    balanced_crops = []
    balanced_labels = []

    for label in unique_labels:
        # Récupérer les indices des échantillons de cette classe
        class_idx = class_indices[label]
        class_size = len(class_idx)

        # Si la classe a suffisamment d'échantillons, sous-échantillonner
        if class_size >= target_count:
            # Choisir aléatoirement target_count échantillons
            selected_indices = np.random.choice(class_idx, target_count, replace=False)
            for idx in selected_indices:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

        # Sinon, augmenter le nombre d'échantillons
        else:
            # Ajouter tous les échantillons existants
            for idx in class_idx:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

            # Compléter avec des échantillons augmentés ou dupliqués
            samples_to_add = target_count - class_size

            if augment:
                # Augmentation de données
                for _ in range(samples_to_add):
                    # Choisir un échantillon aléatoire à augmenter
                    random_idx = class_idx[random.randint(0, class_size - 1)]
                    original_img = crops[random_idx]

                    # Appliquer l'augmentation
                    augmented_img = augment_image(original_img)

                    balanced_crops.append(augmented_img)
                    balanced_labels.append(label)
            else:
                # Simple duplication d'échantillons existants
                for i in range(samples_to_add):
                    # Sélection circulaire des échantillons
                    idx = class_idx[i % class_size]
                    balanced_crops.append(crops[idx])
                    balanced_labels.append(label)

    # Mélanger le jeu de données
    combined = list(zip(balanced_crops, balanced_labels))
    random.shuffle(combined)
    balanced_crops, balanced_labels = zip(*combined)

    # Convertir en listes
    balanced_crops = list(balanced_crops)
    balanced_labels = list(balanced_labels)

    # Vérifier la distribution finale
    final_counts = {}
    for label in balanced_labels:
        if label not in final_counts:
            final_counts[label] = 0
        final_counts[label] += 1

    print("Distribution équilibrée des classes:")
    for label, count in final_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    return balanced_crops, balanced_labels

# Dataset classes that include image size information
class SizeAwareDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform
        # Store original image dimensions
        self.original_sizes = []
        for img in images:
            width, height = img.size
            self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        # Apply transformations to the image
        if self.transform:
            image = self.transform(image)

        # Normalize dimensions for network input (between 0 and 1)
        # This helps the network learn relative size relationships
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,  # Divide by a sufficiently large value
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        return image, size_tensor, label

class SizeAwareTestDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                           if f.endswith('.jpg') or f.endswith('.png')]
        self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in self.image_files]

        # Store original image dimensions
        self.original_sizes = []
        for image_path in self.image_files:
            with Image.open(image_path) as img:
                width, height = img.size
                self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        image_id = self.image_ids[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        # Normalize dimensions
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        return image, size_tensor, image_id

# Size-aware model
class SizeAwareCollemboleClassifier(nn.Module):
    def __init__(self, num_classes=9, pretrained=True):
        super(SizeAwareCollemboleClassifier, self).__init__()
        # Use EfficientNet-B4 for better performance
        self.base_model = models.efficientnet_b4(pretrained=pretrained)

        # Remove the final classification layer from the base model
        self.features = nn.Sequential(*list(self.base_model.children())[:-1])

        # Get the number of output features from EfficientNet
        in_features = self.base_model.classifier[1].in_features

        # Create a layer for image size features (2 values: width, height)
        self.size_encoder = nn.Sequential(
            nn.Linear(2, 64),
            nn.ReLU(),
            nn.Linear(64, 256),
            nn.ReLU()
        )

        # Combine visual features and size features
        self.classifier = nn.Sequential(
            nn.Linear(in_features + 256, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, num_classes)
        )

    def forward(self, x, img_size):
        # Extract visual features
        features = self.features(x)
        features = features.view(features.size(0), -1)

        # Encode size information
        size_features = self.size_encoder(img_size)

        # Concatenate features
        combined = torch.cat((features, size_features), dim=1)

        # Final classification
        output = self.classifier(combined)

        return output

# Training function
def train_model_with_size(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device='cuda'):
    model = model.to(device)
    best_val_f1 = 0.0
    best_model_weights = None

    history = {
        'train_loss': [],
        'val_loss': [],
        'train_f1': [],
        'val_f1': []
    }

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []

        for inputs, sizes, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs, sizes)
            loss = criterion(outputs, labels)

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            # Statistics
            train_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            train_preds.extend(preds.cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        train_loss = train_loss / len(train_loader.dataset)
        train_f1 = f1_score(train_targets, train_preds, average='macro')

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []

        with torch.no_grad():
            for inputs, sizes, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation"):
                inputs = inputs.to(device)
                sizes = sizes.to(device)
                labels = labels.to(device)

                # Forward pass
                outputs = model(inputs, sizes)
                loss = criterion(outputs, labels)

                # Statistics
                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_preds.extend(preds.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        val_loss = val_loss / len(val_loader.dataset)
        val_f1 = f1_score(val_targets, val_preds, average='macro')

        # Update scheduler
        scheduler.step(val_loss)

        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_weights = model.state_dict().copy()

        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1)

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')
        print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')

    # Load best model weights
    model.load_state_dict(best_model_weights)

    return model, history

# Evaluation function
def evaluate_model_with_size(model, dataloader, device='cuda'):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, sizes, labels in tqdm(dataloader, desc="Evaluating"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            outputs = model(inputs, sizes)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    # Calculate F1 score
    f1 = f1_score(all_targets, all_preds, average='macro')
    f1_per_class = f1_score(all_targets, all_preds, average=None)

    # Create confusion matrix
    cm = confusion_matrix(all_targets, all_preds)

    return f1, f1_per_class, cm

# Prediction function for test data
def predict_test_data_with_size(model, test_loader, device='cuda'):
    model.eval()
    predictions = {}
    all_probs = {}  # For storing probabilities

    with torch.no_grad():
        for inputs, sizes, image_ids in tqdm(test_loader, desc="Predicting test data"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)

            outputs = model(inputs, sizes)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(probs, 1)

            for img_id, pred, prob in zip(image_ids, preds.cpu().numpy(), probs.cpu().numpy()):
                predictions[img_id] = int(pred)
                all_probs[img_id] = prob

    return predictions, all_probs

# Visualization functions
def plot_confusion_matrix(cm, class_mapping):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[class_mapping[i] for i in range(len(class_mapping))],
                yticklabels=[class_mapping[i] for i in range(len(class_mapping))])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_f1'], label='Train F1')
    plt.plot(history['val_f1'], label='Validation F1')
    plt.title('F1 Score over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Utility function to create submission file
def create_submission_file(predictions, output_file='submission.csv'):
    with open(output_file, 'w') as f:
        f.write('idx,gt\n')
        for img_id, pred in predictions.items():
            f.write(f'{img_id},{pred}\n')

    print(f"Submission file created: {output_file}")

# Main function that puts everything together
def main():
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Set paths
    train_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"
    test_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/datatest/datatest"

    # 1. Process training data
    print("Processing training data...")
    processor = CollemboleDataProcessor(train_dir)
    crops, labels = processor.crop_images(resize_size=(224, 224))  # Keeping original crop size
    processor.visualize_samples(n_samples=5)

    # 2. Balance dataset
    print("Balancing dataset...")
    crops_balanced, labels_balanced = balance_dataset(crops, labels, target_count=200, augment=True)

    # 3. Prepare data transforms - Now we're resizing to 224x224 to match original model, but keeping original size info
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Keep at 224
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize(256),  # Standard resize
        transforms.CenterCrop(224),  # Keep at 224
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # 4. Create datasets and split into train/val
    X = crops_balanced
    y = np.array(labels_balanced)

    # Split dataset (80% train, 20% validation)
    from sklearn.model_selection import train_test_split
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    print(f"Training set: {len(X_train)} samples")
    print(f"Validation set: {len(X_val)} samples")

    # Create size-aware datasets
    train_dataset = SizeAwareDataset(X_train, y_train, transform=train_transform)
    val_dataset = SizeAwareDataset(X_val, y_val, transform=val_transform)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)  # Reduced batch size for B4
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)

    # 5. Initialize model
    model = SizeAwareCollemboleClassifier(num_classes=len(CLASS_MAPPING), pretrained=True)

    # 6. Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)  # Added weight decay
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)

    # 7. Train model
    print("Training model...")
    model, history = train_model_with_size(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=15,  # Increased for more complex model
        device=device
    )

    # Save model
    torch.save(model.state_dict(), 'collembole_classifier_b4_size_aware.pth')

    # 8. Evaluate model
    print("Evaluating model...")
    f1, f1_per_class, cm = evaluate_model_with_size(model, val_loader, device)

    print(f"Overall F1 Score: {f1:.4f}")
    print("F1 Score per class:")
    for class_id, score in enumerate(f1_per_class):
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        print(f"  {class_name}: {score:.4f}")

    plot_confusion_matrix(cm, CLASS_MAPPING)
    plot_training_history(history)

    # 9. Predict test data
    print("Predicting test data...")
    test_dataset = SizeAwareTestDataset(test_dir, transform=val_transform)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)

    predictions, all_probs = predict_test_data_with_size(model, test_loader, device)
    create_submission_file(predictions, 'submission_b4_size_aware.csv')

    print("Done!")

if __name__ == "__main__":
    main()

"""## *4.3.1*
Nouveau modèle
Création d'un modèle d'ensemble qui combine (EfficientNetV2-S plus récent et performant + EfficientNet-B4 avec couches BatchNorm) (pondération 60%-40%)
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
from sklearn.metrics import f1_score, confusion_matrix
import seaborn as sns
from tqdm import tqdm
import random

# Define constants
CLASS_MAPPING = {
    0: "AUTRE",
    1: "Cer",
    2: "CRY_THE",
    3: "HYP_MAN",
    4: "ISO_MIN",
    5: "LEP",
    6: "MET_AFF",
    7: "PAR_NOT",
    8: "FOND"
}

# 1. Data Preprocessing
class CollemboleDataProcessor:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.image_files = []
        self.labels_files = []
        self.crops = []
        self.crop_labels = []

    def find_files(self):
        """Find all image and corresponding text files"""
        for file in os.listdir(self.data_dir):
            if file.endswith('.jpg') or file.endswith('.png'):
                img_path = os.path.join(self.data_dir, file)
                txt_path = os.path.join(self.data_dir, os.path.splitext(file)[0] + '.txt')

                if os.path.exists(txt_path):
                    self.image_files.append(img_path)
                    self.labels_files.append(txt_path)

        print(f"Found {len(self.image_files)} images with corresponding label files")

    def parse_label_file(self, label_file):
        """Parse YOLO+ format label file and return bounding boxes with full agreement"""
        boxes = []
        labels = []

        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()

                # Vérifier qu'il y a au moins 5 parties (labels + 4 coordonnées)
                if len(parts) < 5:
                    continue

                # Le premier élément contient toujours les labels des experts
                expert_labels = parts[0].split('_')

                # Vérifier si tous les experts sont d'accord
                if len(set(expert_labels)) == 1:
                    # Récupérer la classe
                    class_id = int(expert_labels[0])

                    # Les 4 derniers éléments sont toujours les coordonnées
                    try:
                        # Essayer de convertir les 4 derniers éléments en flottants
                        x_center = float(parts[-4])
                        y_center = float(parts[-3])
                        width = float(parts[-2])
                        height = float(parts[-1])

                        # Vérifier que les valeurs sont dans des plages raisonnables
                        if 0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1:
                            boxes.append([x_center, y_center, width, height])
                            labels.append(class_id)
                        else:
                            print(f"Valeurs de coordonnées hors limites: {parts[-4:]} dans {label_file}")
                    except ValueError:
                        # Les 4 derniers éléments ne sont pas des nombres
                        print(f"Impossible de convertir les coordonnées en nombres: {parts[-4:]} dans {label_file}")

        return boxes, labels

    def generate_background_samples(self, image_file, boxes, num_samples=3, min_size=64, max_size=224):
        """Génère des échantillons de fond (FOND) qui ne chevauchent pas les boîtes existantes"""
        try:
            img = Image.open(image_file)
            img_width, img_height = img.size

            # Convertir les coordonnées relatives en absolues pour toutes les boîtes
            abs_boxes = []
            for box in boxes:
                x_center, y_center, width, height = box
                x1 = int((x_center - width/2) * img_width)
                y1 = int((y_center - height/2) * img_height)
                x2 = int((x_center + width/2) * img_width)
                y2 = int((y_center + height/2) * img_height)
                abs_boxes.append((x1, y1, x2, y2))

            background_crops = []
            attempts = 0
            max_attempts = 50  # Évite les boucles infinies

            while len(background_crops) < num_samples and attempts < max_attempts:
                attempts += 1

                # Taille aléatoire entre min_size et max_size
                crop_width = np.random.randint(min_size, min(max_size, img_width//2))
                crop_height = np.random.randint(min_size, min(max_size, img_height//2))

                # Position aléatoire
                x1 = np.random.randint(0, img_width - crop_width)
                y1 = np.random.randint(0, img_height - crop_height)
                x2 = x1 + crop_width
                y2 = y1 + crop_height

                # Vérifie si ce crop chevauche une boîte existante
                overlap = False
                for box_x1, box_y1, box_x2, box_y2 in abs_boxes:
                    if not (x2 < box_x1 or x1 > box_x2 or y2 < box_y1 or y1 > box_y2):
                        overlap = True
                        break

                # Si pas de chevauchement, ajouter comme échantillon de fond
                if not overlap:
                    crop = img.crop((x1, y1, x2, y2))
                    crop = crop.resize((224, 224), Image.LANCZOS)
                    background_crops.append(crop)

            return background_crops
        except Exception as e:
            print(f"Erreur lors de la génération d'échantillons de fond: {e}")
            return []

    def crop_images(self, resize_size=(224, 224)):
        """Extract crops from images using bounding boxes with full agreement and generate background samples"""
        self.find_files()

        for img_path, label_path in tqdm(zip(self.image_files, self.labels_files), total=len(self.image_files)):
            try:
                img = Image.open(img_path)
                img_width, img_height = img.size

                boxes, labels = self.parse_label_file(label_path)

                for box, label in zip(boxes, labels):
                    # Convert relative coordinates to absolute
                    x_center, y_center, width, height = box
                    x_center *= img_width
                    y_center *= img_height
                    width *= img_width
                    height *= img_height

                    # Vérifier que width et height sont positifs
                    if width <= 0 or height <= 0:
                        continue

                    # Calculate box coordinates
                    x1 = int(x_center - width / 2)
                    y1 = int(y_center - height / 2)
                    x2 = int(x_center + width / 2)
                    y2 = int(y_center + height / 2)

                    # Ensure coordinates are within image bounds
                    x1 = max(0, x1)
                    y1 = max(0, y1)
                    x2 = min(img_width, x2)
                    y2 = min(img_height, y2)

                    # S'assurer que x2 > x1 et y2 > y1
                    if x2 <= x1 or y2 <= y1:
                        continue

                    # Crop the image
                    crop = img.crop((x1, y1, x2, y2))

                    # Resize the crop
                    crop = crop.resize(resize_size, Image.LANCZOS)

                    self.crops.append(crop)
                    self.crop_labels.append(label)

                # Ne générer des échantillons de fond que pour certaines images
                # Limiter le nombre total d'échantillons de fond à environ 300
                if len(self.crop_labels) > 0 and self.crop_labels.count(8) < 300:
                    # Générer aléatoirement 0 ou 1 échantillon avec une probabilité de 0.3
                    if np.random.random() < 0.3:
                        bg_samples = self.generate_background_samples(img_path, boxes, num_samples=1)
                        for bg_sample in bg_samples:
                            self.crops.append(bg_sample)
                            self.crop_labels.append(8)  # Classe 8 = FOND

            except Exception as e:
                print(f"Error processing {img_path}: {e}")

        print(f"Generated {len(self.crops)} crops (including background samples)")

        # Afficher la distribution des classes
        unique_labels, counts = np.unique(self.crop_labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = CLASS_MAPPING.get(label, f"Class {label}")
            print(f"  {class_name} (ID: {label}): {count} samples")

        return self.crops, self.crop_labels

    def visualize_samples(self, n_samples=5):
        """Visualize random sample crops with their labels"""
        if not self.crops:
            print("No crops available. Run crop_images() first.")
            return

        # Sélectionner des échantillons de chaque classe si possible
        samples_by_class = {}
        for i, label in enumerate(self.crop_labels):
            if label not in samples_by_class:
                samples_by_class[label] = []
            if len(samples_by_class[label]) < n_samples:
                samples_by_class[label].append(i)

        # Créer une liste de tous les échantillons à visualiser
        all_samples = []
        for label, samples in samples_by_class.items():
            all_samples.extend(samples[:min(n_samples, len(samples))])

        # Si trop d'échantillons, sélectionner aléatoirement
        if len(all_samples) > n_samples * len(samples_by_class):
            all_samples = np.random.choice(all_samples, n_samples * len(samples_by_class), replace=False)

        # Créer la figure
        n_cols = min(5, len(all_samples))
        n_rows = (len(all_samples) + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))
        axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]

        for i, idx in enumerate(all_samples):
            if i < len(axes):
                axes[i].imshow(self.crops[idx])
                label = self.crop_labels[idx]
                class_name = CLASS_MAPPING.get(label, f"Class {label}")
                axes[i].set_title(f"{class_name} (ID: {label})")
                axes[i].axis('off')

        # Masquer les axes inutilisés
        for i in range(len(all_samples), len(axes)):
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

# Fonction d'augmentation améliorée pour balance_dataset
def augment_image_improved(img):
    """Applique une augmentation aléatoire plus avancée à une image"""
    if not isinstance(img, Image.Image):
        print(f"Warning: Expected PIL Image but got {type(img)}. Skipping augmentation.")
        return img

    # Liste élargie d'augmentations possibles
    aug_type = random.choice(['rotate', 'flip_h', 'flip_v', 'contrast', 'brightness', 'color', 'shear'])

    if aug_type == 'rotate':
        # Rotation aléatoire entre -30 et 30 degrés
        angle = random.uniform(-30, 30)
        return img.rotate(angle, resample=Image.BICUBIC, expand=False)

    elif aug_type == 'flip_h':
        # Retournement horizontal
        return img.transpose(Image.FLIP_LEFT_RIGHT)

    elif aug_type == 'flip_v':
        # Retournement vertical
        return img.transpose(Image.FLIP_TOP_BOTTOM)

    elif aug_type == 'contrast':
        # Modification du contraste
        factor = random.uniform(0.7, 1.5)
        enhancer = ImageEnhance.Contrast(img)
        return enhancer.enhance(factor)

    elif aug_type == 'brightness':
        # Modification de la luminosité
        factor = random.uniform(0.7, 1.3)
        enhancer = ImageEnhance.Brightness(img)
        return enhancer.enhance(factor)

    elif aug_type == 'color':
        # Modification de la saturation
        factor = random.uniform(0.7, 1.3)
        enhancer = ImageEnhance.Color(img)
        return enhancer.enhance(factor)

    elif aug_type == 'shear':
        # Cisaillement (simulé avec une transformation affine)
        width, height = img.size
        shear_factor = random.uniform(-0.2, 0.2)

        # Matrice de transformation pour le cisaillement horizontal
        transform_matrix = (1, shear_factor, -shear_factor * height/2,
                            0, 1, 0)

        return img.transform(img.size, Image.AFFINE, transform_matrix, Image.BICUBIC)

    return img

def balance_dataset(crops, labels, target_count=None, augment=True):
    """
    Équilibre les données pour avoir le même nombre d'échantillons par classe.

    Args:
        crops: Liste d'images (PIL.Image)
        labels: Liste des étiquettes correspondantes
        target_count: Nombre cible d'échantillons par classe (si None, utilise le maximum disponible)
        augment: Si True, utilise l'augmentation de données pour les classes sous-représentées
                Si False, duplique simplement les échantillons existants

    Returns:
        balanced_crops: Liste équilibrée d'images
        balanced_labels: Liste équilibrée d'étiquettes
    """
    # Compter les échantillons par classe
    unique_labels = np.unique(labels)
    class_counts = {}
    class_indices = {}

    # Organiser les indices par classe
    for label in unique_labels:
        indices = [i for i, l in enumerate(labels) if l == label]
        class_counts[label] = len(indices)
        class_indices[label] = indices

    print("Distribution originale des classes:")
    for label, count in class_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    # Déterminer le nombre cible d'échantillons par classe
    if target_count is None:
        # Utiliser la classe majoritaire comme cible
        target_count = max(class_counts.values())

    print(f"Cible: {target_count} échantillons par classe")

    # Créer le jeu de données équilibré
    balanced_crops = []
    balanced_labels = []

    for label in unique_labels:
        # Récupérer les indices des échantillons de cette classe
        class_idx = class_indices[label]
        class_size = len(class_idx)

        # Si la classe a suffisamment d'échantillons, sous-échantillonner
        if class_size >= target_count:
            # Choisir aléatoirement target_count échantillons
            selected_indices = np.random.choice(class_idx, target_count, replace=False)
            for idx in selected_indices:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

        # Sinon, augmenter le nombre d'échantillons
        else:
            # Ajouter tous les échantillons existants
            for idx in class_idx:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

            # Compléter avec des échantillons augmentés ou dupliqués
            samples_to_add = target_count - class_size

            if augment:
                # Augmentation de données améliorée
                for _ in range(samples_to_add):
                    # Choisir un échantillon aléatoire à augmenter
                    random_idx = class_idx[random.randint(0, class_size - 1)]
                    original_img = crops[random_idx]

                    # Appliquer l'augmentation améliorée
                    augmented_img = augment_image_improved(original_img)

                    balanced_crops.append(augmented_img)
                    balanced_labels.append(label)
            else:
                # Simple duplication d'échantillons existants
                for i in range(samples_to_add):
                    # Sélection circulaire des échantillons
                    idx = class_idx[i % class_size]
                    balanced_crops.append(crops[idx])
                    balanced_labels.append(label)

    # Mélanger le jeu de données
    combined = list(zip(balanced_crops, balanced_labels))
    random.shuffle(combined)
    balanced_crops, balanced_labels = zip(*combined)

    # Convertir en listes
    balanced_crops = list(balanced_crops)
    balanced_labels = list(balanced_labels)

    # Vérifier la distribution finale
    final_counts = {}
    for label in balanced_labels:
        if label not in final_counts:
            final_counts[label] = 0
        final_counts[label] += 1

    print("Distribution équilibrée des classes:")
    for label, count in final_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    return balanced_crops, balanced_labels

# Dataset classes that include image size information
# CORRECTION: Simplifier pour éviter le biais FOND
class SizeAwareDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform
        # Store original image dimensions
        self.original_sizes = []
        for img in images:
            width, height = img.size
            self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        # Apply transformations to the image
        if self.transform:
            image = self.transform(image)

        # Normalize dimensions for network input (between 0 and 1)
        # This helps the network learn relative size relationships
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,  # Divide by a sufficiently large value
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        return image, size_tensor, label

class SizeAwareTestDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                           if f.endswith('.jpg') or f.endswith('.png')]
        self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in self.image_files]

        # Store original image dimensions
        self.original_sizes = []
        for image_path in self.image_files:
            with Image.open(image_path) as img:
                width, height = img.size
                self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        image_id = self.image_ids[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        # Normalize dimensions
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        return image, size_tensor, image_id

# Modèle EfficientNet-B4 amélioré
class ImprovedEfficientNetB4(nn.Module):
    def __init__(self, num_classes=9, pretrained=True):
        super(ImprovedEfficientNetB4, self).__init__()
        # Version B4 originale
        self.base_model = models.efficientnet_b4(pretrained=pretrained)

        # Extraire les caractéristiques
        self.features = nn.Sequential(*list(self.base_model.children())[:-1])

        # Obtenir la taille des caractéristiques
        in_features = self.base_model.classifier[1].in_features

        # Encodeur de taille simplifié
        self.size_encoder = nn.Sequential(
            nn.Linear(2, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # Combinaison des caractéristiques
        self.classifier = nn.Sequential(
            nn.Linear(in_features + 128, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, x, size_features):
        # Extraire les caractéristiques visuelles
        visual_features = self.features(x)
        visual_features = torch.flatten(visual_features, 1)

        # Encoder les informations de taille
        size_encoded = self.size_encoder(size_features)

        # Combiner les caractéristiques
        combined = torch.cat((visual_features, size_encoded), dim=1)

        # Classification
        output = self.classifier(combined)

        return output

# Fonction pour définir des poids de classe pour équilibrer la perte
def get_class_weights(labels):
    class_counts = np.bincount(labels)
    # Ajuster les poids pour donner moins d'importance à la classe FOND (8)
    # et plus d'importance aux classes rares
    class_weights = 1.0 / class_counts
    # Donner un poids spécifique réduit à la classe FOND
    if 8 in range(len(class_weights)):
        class_weights[8] *= 0.5  # Réduire de moitié l'importance de FOND

    # Normaliser les poids
    class_weights = class_weights / np.sum(class_weights) * len(class_weights)
    return torch.tensor(class_weights, dtype=torch.float32)

# Fonction d'évaluation améliorée
def evaluate_model(model, dataloader, device='cuda'):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, sizes, labels in tqdm(dataloader, desc="Evaluating"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            outputs = model(inputs, sizes)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    # Calculate F1 score
    f1 = f1_score(all_targets, all_preds, average='macro')
    f1_per_class = f1_score(all_targets, all_preds, average=None)

    # Create confusion matrix
    cm = confusion_matrix(all_targets, all_preds)

    return f1, f1_per_class, cm

# Fonction d'entraînement améliorée
def train_model_with_size(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device='cuda'):
    model = model.to(device)
    best_val_f1 = 0.0
    best_model_weights = None

    history = {
        'train_loss': [],
        'val_loss': [],
        'train_f1': [],
        'val_f1': []
    }

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []

        for inputs, sizes, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs, sizes)
            loss = criterion(outputs, labels)

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            # Statistics
            train_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            train_preds.extend(preds.cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        train_loss = train_loss / len(train_loader.dataset)
        train_f1 = f1_score(train_targets, train_preds, average='macro')

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []

        with torch.no_grad():
            for inputs, sizes, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation"):
                inputs = inputs.to(device)
                sizes = sizes.to(device)
                labels = labels.to(device)

                # Forward pass
                outputs = model(inputs, sizes)
                loss = criterion(outputs, labels)

                # Statistics
                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_preds.extend(preds.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        val_loss = val_loss / len(val_loader.dataset)
        val_f1 = f1_score(val_targets, val_preds, average='macro')

        # Update scheduler
        scheduler.step(val_loss)

        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_weights = model.state_dict().copy()

        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1)

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')
        print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')

    # Load best model weights
    model.load_state_dict(best_model_weights)

    return model, history

# Fonction de prédiction mise à jour
def predict_test_data_with_size(model, test_loader, device='cuda'):
    model.eval()
    predictions = {}
    all_probs = {}  # Pour stocker les probabilités

    with torch.no_grad():
        for inputs, sizes, image_ids in tqdm(test_loader, desc="Predicting test data"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)

            outputs = model(inputs, sizes)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(probs, 1)

            for img_id, pred, prob in zip(image_ids, preds.cpu().numpy(), probs.cpu().numpy()):
                predictions[img_id] = int(pred)
                all_probs[img_id] = prob

    # Vérifier la distribution des prédictions
    pred_counts = {}
    for pred in predictions.values():
        if pred not in pred_counts:
            pred_counts[pred] = 0
        pred_counts[pred] += 1

    print("Distribution des prédictions:")
    for class_id, count in sorted(pred_counts.items()):
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        percentage = count / len(predictions) * 100
        print(f"  {class_name} (ID: {class_id}): {count} exemples ({percentage:.1f}%)")

    # Vérifier si toutes les prédictions sont FOND
    if len(pred_counts) == 1 and 8 in pred_counts:
        print("ATTENTION: Toutes les prédictions sont FOND. Il y a un problème avec le modèle.")

    return predictions, all_probs

# Utility function to create submission file
def create_submission_file(predictions, output_file='submission.csv'):
    with open(output_file, 'w') as f:
        f.write('idx,gt\n')
        for img_id, pred in predictions.items():
            f.write(f'{img_id},{pred}\n')

    print(f"Submission file created: {output_file}")

# Visualization functions
def plot_confusion_matrix(cm, class_mapping):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[class_mapping[i] for i in range(len(class_mapping))],
                yticklabels=[class_mapping[i] for i in range(len(class_mapping))])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_f1'], label='Train F1')
    plt.plot(history['val_f1'], label='Validation F1')
    plt.title('F1 Score over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Augmentation de données avancée pour transformations
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.3),
    transforms.RandomRotation(30),
    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),
    transforms.ColorJitter(
        brightness=0.3,
        contrast=0.4,
        saturation=0.3,
        hue=0.1
    ),
    transforms.RandomGrayscale(p=0.05),
    transforms.RandomAffine(
        degrees=0,
        translate=(0.1, 0.1),
        scale=(0.9, 1.1),
        shear=10
    ),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Fonction principale corrigée
def main():
    # Définir l'appareil
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Définir les chemins
    train_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"
    test_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/datatest/datatest"

    # 1. Traiter les données d'entraînement
    print("Processing training data...")
    processor = CollemboleDataProcessor(train_dir)
    crops, labels = processor.crop_images(resize_size=(224, 224))
    processor.visualize_samples(n_samples=5)

    # 2. Équilibrer le jeu de données
    print("Balancing dataset with improved augmentation...")
    crops_balanced, labels_balanced = balance_dataset(crops, labels, target_count=1000, augment=True)

    # 3. Créer des jeux de données et les diviser en train/val
    X = crops_balanced
    y = np.array(labels_balanced)

    # Split dataset (80% train, 20% validation)
    from sklearn.model_selection import train_test_split
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    print(f"Training set: {len(X_train)} samples")
    print(f"Validation set: {len(X_val)} samples")

    # 4. Créer des jeux de données avancés
    train_dataset = SizeAwareDataset(X_train, y_train, transform=train_transform)
    val_dataset = SizeAwareDataset(X_val, y_val, transform=val_transform)

    # 5. Créer les chargeurs de données
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)

    # 6. Initialiser le modèle
    model = ImprovedEfficientNetB4(num_classes=len(CLASS_MAPPING), pretrained=True)

    # 7. Configurer l'entraînement
    # Calculer les poids des classes pour équilibrer la perte
    class_weights = get_class_weights(labels_balanced).to(device)

    # Critère de perte pondéré
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    # Optimiseur avec poids de régularisation
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)

    # Scheduler pour ajuster le taux d'apprentissage
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

    # 8. Entraîner le modèle
    print("Training model...")
    model, history = train_model_with_size(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=20,
        device=device
    )

    # 9. Sauvegarder le modèle
    torch.save(model.state_dict(), 'improved_collembole_classifier.pth')

    # 10. Évaluer le modèle
    print("Evaluating model...")
    f1, f1_per_class, cm = evaluate_model(model, val_loader, device)

    print(f"Overall F1 Score: {f1:.4f}")
    print("F1 Score per class:")
    for class_id, score in enumerate(f1_per_class):
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        print(f"  {class_name}: {score:.4f}")

    # 11. Visualiser la matrice de confusion et l'historique d'entraînement
    try:
        plot_confusion_matrix(cm, CLASS_MAPPING)
        plot_training_history(history)
    except Exception as e:
        print(f"Error plotting results: {e}")

    # 12. Prédire sur les données de test
    print("Predicting test data...")
    test_dataset = SizeAwareTestDataset(test_dir, transform=val_transform)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)

    predictions, all_probs = predict_test_data_with_size(model, test_loader, device)

    # Analyser les prédictions pour détecter le problème de classe unique
    pred_classes = list(predictions.values())
    unique_classes = set(pred_classes)

    if len(unique_classes) == 1 and 8 in unique_classes:
        print("ERREUR: Le modèle ne prédit que la classe FOND. Problème détecté.")
        print("Tentative de correction avec des seuils de probabilité personnalisés...")

        # Si toutes les prédictions sont FOND, essayer d'utiliser les probabilités brutes
        corrected_predictions = {}
        for img_id, probs in all_probs.items():
            # Réduire artificiellement la probabilité de la classe FOND pour favoriser d'autres classes
            adjusted_probs = probs.copy()
            adjusted_probs[8] *= 0.5  # Réduire de moitié la probabilité de la classe FOND
            new_pred = np.argmax(adjusted_probs)
            corrected_predictions[img_id] = int(new_pred)

        # Vérifier la nouvelle distribution
        new_pred_counts = {}
        for pred in corrected_predictions.values():
            if pred not in new_pred_counts:
                new_pred_counts[pred] = 0
            new_pred_counts[pred] += 1

        print("Nouvelle distribution après correction:")
        for class_id, count in sorted(new_pred_counts.items()):
            class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
            percentage = count / len(corrected_predictions) * 100
            print(f"  {class_name} (ID: {class_id}): {count} exemples ({percentage:.1f}%)")

        # Si la correction a fonctionné (il y a maintenant plus d'une classe)
        if len(new_pred_counts) > 1:
            print("Correction réussie! Utilisation des prédictions corrigées.")
            create_submission_file(corrected_predictions, 'submission_corrected.csv')
        else:
            print("La correction n'a pas fonctionné. Tentative avec un modèle plus simple...")

            # Essayer avec un modèle plus simple qui ne tient pas compte de la taille
            simple_model = models.efficientnet_b4(pretrained=True)
            num_classes = len(CLASS_MAPPING)
            simple_model.classifier[1] = nn.Linear(simple_model.classifier[1].in_features, num_classes)
            simple_model = simple_model.to(device)

            # Critère et optimiseur pour le modèle simple
            simple_criterion = nn.CrossEntropyLoss()
            simple_optimizer = optim.Adam(simple_model.parameters(), lr=0.001)

            # Entraîner le modèle simple (avec une version simplifiée des données)
            simple_train_transform = transforms.Compose([
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(10),
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])

            simple_val_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])

            # Créer des datasets simplifiés
            class SimpleDataset(Dataset):
                def __init__(self, images, labels, transform=None):
                    self.images = images
                    self.labels = labels
                    self.transform = transform

                def __len__(self):
                    return len(self.images)

                def __getitem__(self, idx):
                    image = self.images[idx]
                    label = self.labels[idx]

                    if self.transform:
                        image = self.transform(image)

                    return image, label

            simple_train_dataset = SimpleDataset(X_train, y_train, transform=simple_train_transform)
            simple_val_dataset = SimpleDataset(X_val, y_val, transform=simple_val_transform)

            simple_train_loader = DataLoader(simple_train_dataset, batch_size=16, shuffle=True)
            simple_val_loader = DataLoader(simple_val_dataset, batch_size=16, shuffle=False)

            # Entraîner le modèle simple
            print("Entraînement d'un modèle plus simple...")
            simple_model.train()
            for epoch in range(10):
                running_loss = 0.0
                for inputs, labels in tqdm(simple_train_loader):
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    simple_optimizer.zero_grad()
                    outputs = simple_model(inputs)
                    loss = simple_criterion(outputs, labels)
                    loss.backward()
                    simple_optimizer.step()

                    running_loss += loss.item() * inputs.size(0)

                epoch_loss = running_loss / len(simple_train_loader.dataset)
                print(f"Epoch {epoch+1}/10, Loss: {epoch_loss:.4f}")

            # Prédire avec le modèle simple
            print("Prédiction avec le modèle simple...")
            simple_model.eval()

            class SimpleTestDataset(Dataset):
                def __init__(self, data_dir, transform=None):
                    self.data_dir = data_dir
                    self.transform = transform
                    self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                                      if f.endswith('.jpg') or f.endswith('.png')]
                    self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in self.image_files]

                def __len__(self):
                    return len(self.image_files)

                def __getitem__(self, idx):
                    image_path = self.image_files[idx]
                    image_id = self.image_ids[idx]

                    image = Image.open(image_path).convert('RGB')

                    if self.transform:
                        image = self.transform(image)

                    return image, image_id

            simple_test_dataset = SimpleTestDataset(test_dir, transform=simple_val_transform)
            simple_test_loader = DataLoader(simple_test_dataset, batch_size=16, shuffle=False)

            simple_predictions = {}
            with torch.no_grad():
                for inputs, image_ids in tqdm(simple_test_loader):
                    inputs = inputs.to(device)
                    outputs = simple_model(inputs)
                    _, preds = torch.max(outputs, 1)

                    for img_id, pred in zip(image_ids, preds.cpu().numpy()):
                        simple_predictions[img_id] = int(pred)

            # Vérifier la distribution des prédictions du modèle simple
            simple_pred_counts = {}
            for pred in simple_predictions.values():
                if pred not in simple_pred_counts:
                    simple_pred_counts[pred] = 0
                simple_pred_counts[pred] += 1

            print("Distribution des prédictions du modèle simple:")
            for class_id, count in sorted(simple_pred_counts.items()):
                class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
                percentage = count / len(simple_predictions) * 100
                print(f"  {class_name} (ID: {class_id}): {count} exemples ({percentage:.1f}%)")

            # Utiliser les prédictions du modèle simple si elles sont plus variées
            if len(simple_pred_counts) > 1:
                print("Utilisation des prédictions du modèle simple.")
                create_submission_file(simple_predictions, 'submission_simple_model.csv')
            else:
                print("Même le modèle simple prédit une seule classe. Utilisation du modèle original sans modifications.")
                # Dans ce cas, revenir à la version originale non modifiée
                create_submission_file(predictions, 'submission_original.csv')
    else:
        # Si les prédictions sont variées, utiliser le modèle amélioré
        create_submission_file(predictions, 'submission_improved.csv')

    print("Done!")

if __name__ == "__main__":
    main()

"""## 4.3.2"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
from sklearn.metrics import f1_score, confusion_matrix
import seaborn as sns
from tqdm import tqdm
import random

# Define constants
CLASS_MAPPING = {
    0: "AUTRE",
    1: "Cer",
    2: "CRY_THE",
    3: "HYP_MAN",
    4: "ISO_MIN",
    5: "LEP",
    6: "MET_AFF",
    7: "PAR_NOT",
    8: "FOND"
}

# 1. Data Preprocessing
class CollemboleDataProcessor:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.image_files = []
        self.labels_files = []
        self.crops = []
        self.crop_labels = []

    def find_files(self):
        """Find all image and corresponding text files"""
        for file in os.listdir(self.data_dir):
            if file.endswith('.jpg') or file.endswith('.png'):
                img_path = os.path.join(self.data_dir, file)
                txt_path = os.path.join(self.data_dir, os.path.splitext(file)[0] + '.txt')

                if os.path.exists(txt_path):
                    self.image_files.append(img_path)
                    self.labels_files.append(txt_path)

        print(f"Found {len(self.image_files)} images with corresponding label files")

    def parse_label_file(self, label_file):
        """Parse YOLO+ format label file and return bounding boxes with full agreement"""
        boxes = []
        labels = []

        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()

                # Vérifier qu'il y a au moins 5 parties (labels + 4 coordonnées)
                if len(parts) < 5:
                    continue

                # Le premier élément contient toujours les labels des experts
                expert_labels = parts[0].split('_')

                # Vérifier si tous les experts sont d'accord
                if len(set(expert_labels)) == 1:
                    # Récupérer la classe
                    class_id = int(expert_labels[0])

                    # Les 4 derniers éléments sont toujours les coordonnées
                    try:
                        # Essayer de convertir les 4 derniers éléments en flottants
                        x_center = float(parts[-4])
                        y_center = float(parts[-3])
                        width = float(parts[-2])
                        height = float(parts[-1])

                        # Vérifier que les valeurs sont dans des plages raisonnables
                        if 0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1:
                            boxes.append([x_center, y_center, width, height])
                            labels.append(class_id)
                        else:
                            print(f"Valeurs de coordonnées hors limites: {parts[-4:]} dans {label_file}")
                    except ValueError:
                        # Les 4 derniers éléments ne sont pas des nombres
                        print(f"Impossible de convertir les coordonnées en nombres: {parts[-4:]} dans {label_file}")

        return boxes, labels

    def generate_background_samples(self, image_file, boxes, num_samples=3, min_size=64, max_size=224):
        """Génère des échantillons de fond (FOND) qui ne chevauchent pas les boîtes existantes"""
        try:
            img = Image.open(image_file)
            img_width, img_height = img.size

            # Convertir les coordonnées relatives en absolues pour toutes les boîtes
            abs_boxes = []
            for box in boxes:
                x_center, y_center, width, height = box
                x1 = int((x_center - width/2) * img_width)
                y1 = int((y_center - height/2) * img_height)
                x2 = int((x_center + width/2) * img_width)
                y2 = int((y_center + height/2) * img_height)
                abs_boxes.append((x1, y1, x2, y2))

            background_crops = []
            attempts = 0
            max_attempts = 50  # Évite les boucles infinies

            while len(background_crops) < num_samples and attempts < max_attempts:
                attempts += 1

                # Taille aléatoire entre min_size et max_size
                crop_width = np.random.randint(min_size, min(max_size, img_width//2))
                crop_height = np.random.randint(min_size, min(max_size, img_height//2))

                # Position aléatoire
                x1 = np.random.randint(0, img_width - crop_width)
                y1 = np.random.randint(0, img_height - crop_height)
                x2 = x1 + crop_width
                y2 = y1 + crop_height

                # Vérifie si ce crop chevauche une boîte existante
                overlap = False
                for box_x1, box_y1, box_x2, box_y2 in abs_boxes:
                    if not (x2 < box_x1 or x1 > box_x2 or y2 < box_y1 or y1 > box_y2):
                        overlap = True
                        break

                # Si pas de chevauchement, ajouter comme échantillon de fond
                if not overlap:
                    crop = img.crop((x1, y1, x2, y2))
                    crop = crop.resize((224, 224), Image.LANCZOS)
                    background_crops.append(crop)

            return background_crops
        except Exception as e:
            print(f"Erreur lors de la génération d'échantillons de fond: {e}")
            return []

    def crop_images(self, resize_size=(224, 224)):
        """Extract crops from images using bounding boxes with full agreement and generate background samples"""
        self.find_files()

        for img_path, label_path in tqdm(zip(self.image_files, self.labels_files), total=len(self.image_files)):
            try:
                img = Image.open(img_path)
                img_width, img_height = img.size

                boxes, labels = self.parse_label_file(label_path)

                for box, label in zip(boxes, labels):
                    # Convert relative coordinates to absolute
                    x_center, y_center, width, height = box
                    x_center *= img_width
                    y_center *= img_height
                    width *= img_width
                    height *= img_height

                    # Vérifier que width et height sont positifs
                    if width <= 0 or height <= 0:
                        continue

                    # Calculate box coordinates
                    x1 = int(x_center - width / 2)
                    y1 = int(y_center - height / 2)
                    x2 = int(x_center + width / 2)
                    y2 = int(y_center + height / 2)

                    # Ensure coordinates are within image bounds
                    x1 = max(0, x1)
                    y1 = max(0, y1)
                    x2 = min(img_width, x2)
                    y2 = min(img_height, y2)

                    # S'assurer que x2 > x1 et y2 > y1
                    if x2 <= x1 or y2 <= y1:
                        continue

                    # Crop the image
                    crop = img.crop((x1, y1, x2, y2))

                    # Resize the crop
                    crop = crop.resize(resize_size, Image.LANCZOS)

                    self.crops.append(crop)
                    self.crop_labels.append(label)

                # Ne générer des échantillons de fond que pour certaines images
                # Limiter le nombre total d'échantillons de fond à environ 300
                if len(self.crop_labels) > 0 and self.crop_labels.count(8) < 300:
                    # Générer aléatoirement 0 ou 1 échantillon avec une probabilité de 0.3
                    if np.random.random() < 0.3:
                        bg_samples = self.generate_background_samples(img_path, boxes, num_samples=1)
                        for bg_sample in bg_samples:
                            self.crops.append(bg_sample)
                            self.crop_labels.append(8)  # Classe 8 = FOND

            except Exception as e:
                print(f"Error processing {img_path}: {e}")

        print(f"Generated {len(self.crops)} crops (including background samples)")

        # Afficher la distribution des classes
        unique_labels, counts = np.unique(self.crop_labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = CLASS_MAPPING.get(label, f"Class {label}")
            print(f"  {class_name} (ID: {label}): {count} samples")

        return self.crops, self.crop_labels

    def visualize_samples(self, n_samples=5):
        """Visualize random sample crops with their labels"""
        if not self.crops:
            print("No crops available. Run crop_images() first.")
            return

        # Sélectionner des échantillons de chaque classe si possible
        samples_by_class = {}
        for i, label in enumerate(self.crop_labels):
            if label not in samples_by_class:
                samples_by_class[label] = []
            if len(samples_by_class[label]) < n_samples:
                samples_by_class[label].append(i)

        # Créer une liste de tous les échantillons à visualiser
        all_samples = []
        for label, samples in samples_by_class.items():
            all_samples.extend(samples[:min(n_samples, len(samples))])

        # Si trop d'échantillons, sélectionner aléatoirement
        if len(all_samples) > n_samples * len(samples_by_class):
            all_samples = np.random.choice(all_samples, n_samples * len(samples_by_class), replace=False)

        # Créer la figure
        n_cols = min(5, len(all_samples))
        n_rows = (len(all_samples) + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))
        axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]

        for i, idx in enumerate(all_samples):
            if i < len(axes):
                axes[i].imshow(self.crops[idx])
                label = self.crop_labels[idx]
                class_name = CLASS_MAPPING.get(label, f"Class {label}")
                axes[i].set_title(f"{class_name} (ID: {label})")
                axes[i].axis('off')

        # Masquer les axes inutilisés
        for i in range(len(all_samples), len(axes)):
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

# Fonction d'augmentation améliorée pour balance_dataset
def augment_image_aggressive(img):
    """Applique une augmentation aléatoire plus agressive à une image"""
    if not isinstance(img, Image.Image):
        print(f"Warning: Expected PIL Image but got {type(img)}. Skipping augmentation.")
        return img

    # Liste d'augmentations possibles
    aug_types = ['rotate', 'flip_h', 'flip_v', 'contrast', 'brightness', 'color', 'shear']

    # Nombre d'augmentations à appliquer (entre 1 et 3)
    num_augs = random.randint(1, 3)

    # Sélectionner plusieurs augmentations aléatoires
    selected_augs = random.sample(aug_types, num_augs)

    # Appliquer les augmentations sélectionnées
    result_img = img.copy()

    for aug_type in selected_augs:
        if aug_type == 'rotate':
            # Rotation aléatoire entre -45 et 45 degrés
            angle = random.uniform(-45, 45)
            result_img = result_img.rotate(angle, resample=Image.BICUBIC, expand=False)

        elif aug_type == 'flip_h':
            # Retournement horizontal
            result_img = result_img.transpose(Image.FLIP_LEFT_RIGHT)

        elif aug_type == 'flip_v':
            # Retournement vertical
            result_img = result_img.transpose(Image.FLIP_TOP_BOTTOM)

        elif aug_type == 'contrast':
            # Modification du contraste plus agressive
            factor = random.uniform(0.5, 1.8)
            enhancer = ImageEnhance.Contrast(result_img)
            result_img = enhancer.enhance(factor)

        elif aug_type == 'brightness':
            # Modification de la luminosité plus agressive
            factor = random.uniform(0.5, 1.5)
            enhancer = ImageEnhance.Brightness(result_img)
            result_img = enhancer.enhance(factor)

        elif aug_type == 'color':
            # Modification de la saturation plus agressive
            factor = random.uniform(0.5, 1.8)
            enhancer = ImageEnhance.Color(result_img)
            result_img = enhancer.enhance(factor)

        elif aug_type == 'shear':
            # Cisaillement plus agressif
            width, height = result_img.size
            shear_factor = random.uniform(-0.3, 0.3)

            transform_matrix = (1, shear_factor, -shear_factor * height/2,
                                0, 1, 0)

            result_img = result_img.transform(result_img.size, Image.AFFINE, transform_matrix, Image.BICUBIC)

    # Ajouter du bruit aléatoire (optionnel)
    if random.random() < 0.2:
        # Convertir en tableau numpy
        img_array = np.array(result_img)
        # Ajouter du bruit gaussien
        noise = np.random.normal(0, 10, img_array.shape)
        img_array = np.clip(img_array + noise, 0, 255).astype(np.uint8)
        # Reconvertir en image PIL
        result_img = Image.fromarray(img_array)

    return result_img

def balance_dataset(crops, labels, target_count=None, augment=True):
    """
    Équilibre les données pour avoir le même nombre d'échantillons par classe.

    Args:
        crops: Liste d'images (PIL.Image)
        labels: Liste des étiquettes correspondantes
        target_count: Nombre cible d'échantillons par classe (si None, utilise le maximum disponible)
        augment: Si True, utilise l'augmentation de données pour les classes sous-représentées
                Si False, duplique simplement les échantillons existants

    Returns:
        balanced_crops: Liste équilibrée d'images
        balanced_labels: Liste équilibrée d'étiquettes
    """
    # Compter les échantillons par classe
    unique_labels = np.unique(labels)
    class_counts = {}
    class_indices = {}

    # Organiser les indices par classe
    for label in unique_labels:
        indices = [i for i, l in enumerate(labels) if l == label]
        class_counts[label] = len(indices)
        class_indices[label] = indices

    print("Distribution originale des classes:")
    for label, count in class_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    # Déterminer le nombre cible d'échantillons par classe
    if target_count is None:
        # Utiliser la classe majoritaire comme cible
        target_count = max(class_counts.values())

    print(f"Cible: {target_count} échantillons par classe")

    # Créer le jeu de données équilibré
    balanced_crops = []
    balanced_labels = []

    for label in unique_labels:
        # Récupérer les indices des échantillons de cette classe
        class_idx = class_indices[label]
        class_size = len(class_idx)

        # Si la classe a suffisamment d'échantillons, sous-échantillonner
        if class_size >= target_count:
            # Choisir aléatoirement target_count échantillons
            selected_indices = np.random.choice(class_idx, target_count, replace=False)
            for idx in selected_indices:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

        # Sinon, augmenter le nombre d'échantillons
        else:
            # Ajouter tous les échantillons existants
            for idx in class_idx:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

            # Compléter avec des échantillons augmentés ou dupliqués
            samples_to_add = target_count - class_size

            if augment:
                # Augmentation de données améliorée
                for _ in range(samples_to_add):
                    # Choisir un échantillon aléatoire à augmenter
                    random_idx = class_idx[random.randint(0, class_size - 1)]
                    original_img = crops[random_idx]

                    # Appliquer l'augmentation améliorée
                    augmented_img = augment_image_aggressive(original_img)

                    balanced_crops.append(augmented_img)
                    balanced_labels.append(label)
            else:
                # Simple duplication d'échantillons existants
                for i in range(samples_to_add):
                    # Sélection circulaire des échantillons
                    idx = class_idx[i % class_size]
                    balanced_crops.append(crops[idx])
                    balanced_labels.append(label)

    # Mélanger le jeu de données
    combined = list(zip(balanced_crops, balanced_labels))
    random.shuffle(combined)
    balanced_crops, balanced_labels = zip(*combined)

    # Convertir en listes
    balanced_crops = list(balanced_crops)
    balanced_labels = list(balanced_labels)

    # Vérifier la distribution finale
    final_counts = {}
    for label in balanced_labels:
        if label not in final_counts:
            final_counts[label] = 0
        final_counts[label] += 1

    print("Distribution équilibrée des classes:")
    for label, count in final_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    return balanced_crops, balanced_labels

# Dataset classes that include image size information
class SizeAwareDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform
        # Store original image dimensions
        self.original_sizes = []
        for img in images:
            width, height = img.size
            self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        # Apply transformations to the image
        if self.transform:
            image = self.transform(image)

        # Normalize dimensions for network input (between 0 and 1)
        # This helps the network learn relative size relationships
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,  # Divide by a sufficiently large value
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        return image, size_tensor, label

class SizeAwareTestDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                           if f.endswith('.jpg') or f.endswith('.png')]
        self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in self.image_files]

        # Store original image dimensions
        self.original_sizes = []
        for image_path in self.image_files:
            with Image.open(image_path) as img:
                width, height = img.size
                self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        image_id = self.image_ids[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        # Normalize dimensions
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        return image, size_tensor, image_id

# Modèle avec plus de régularisation
class RegularizedEfficientNetB4(nn.Module):
    def __init__(self, num_classes=9, pretrained=True):
        super(RegularizedEfficientNetB4, self).__init__()
        # Version B4 originale
        self.base_model = models.efficientnet_b4(pretrained=pretrained)

        # Extraire les caractéristiques
        self.features = nn.Sequential(*list(self.base_model.children())[:-1])

        # Obtenir la taille des caractéristiques
        in_features = self.base_model.classifier[1].in_features

        # Encodeur de taille simplifié avec plus de dropout
        self.size_encoder = nn.Sequential(
            nn.Linear(2, 32),  # Moins de neurones
            nn.BatchNorm1d(32),  # Ajout de BatchNorm
            nn.ReLU(),
            nn.Dropout(0.4),  # Plus de dropout
            nn.Linear(32, 64),  # Moins de neurones
            nn.BatchNorm1d(64),  # Ajout de BatchNorm
            nn.ReLU(),
            nn.Dropout(0.4)  # Plus de dropout
        )

        # Combinaison des caractéristiques avec plus de régularisation
        self.classifier = nn.Sequential(
            nn.Linear(in_features + 64, 256),  # Moins de neurones
            nn.BatchNorm1d(256),  # Ajout de BatchNorm
            nn.ReLU(),
            nn.Dropout(0.5),  # Plus de dropout
            nn.Linear(256, num_classes)
        )

    def forward(self, x, size_features):
        # Extraire les caractéristiques visuelles
        visual_features = self.features(x)
        visual_features = torch.flatten(visual_features, 1)

        # Encoder les informations de taille
        size_encoded = self.size_encoder(size_features)

        # Combiner les caractéristiques
        combined = torch.cat((visual_features, size_encoded), dim=1)

        # Classification
        output = self.classifier(combined)

        return output

# Fonction pour définir des poids de classe pour équilibrer la perte
def get_class_weights(labels):
    class_counts = np.bincount(labels)
    # Ajuster les poids pour donner moins d'importance à la classe FOND (8)
    # et plus d'importance aux classes rares
    class_weights = 1.0 / class_counts
    # Donner un poids spécifique réduit à la classe FOND
    if 8 in range(len(class_weights)):
        class_weights[8] *= 0.5  # Réduire de moitié l'importance de FOND

    # Normaliser les poids
    class_weights = class_weights / np.sum(class_weights) * len(class_weights)
    return torch.tensor(class_weights, dtype=torch.float32)

# Fonction d'entraînement avec early stopping plus strict
def train_model_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, scheduler,
                                   num_epochs=20, device='cuda', patience=5, min_delta=0.001):
    model = model.to(device)
    best_val_f1 = 0.0
    best_model_weights = None
    patience_counter = 0
    last_val_f1 = 0.0

    history = {
        'train_loss': [],
        'val_loss': [],
        'train_f1': [],
        'val_f1': [],
        'learning_rates': []
    }

    for epoch in range(num_epochs):
        # Phase d'entraînement
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []

        for inputs, sizes, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs, sizes)
            loss = criterion(outputs, labels)

            # Backward pass and optimize
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            # Statistics
            train_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            train_preds.extend(preds.cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        train_loss = train_loss / len(train_loader.dataset)
        train_f1 = f1_score(train_targets, train_preds, average='macro')

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []

        with torch.no_grad():
            for inputs, sizes, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation"):
                inputs = inputs.to(device)
                sizes = sizes.to(device)
                labels = labels.to(device)

                # Forward pass
                outputs = model(inputs, sizes)
                loss = criterion(outputs, labels)

                # Statistics
                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_preds.extend(preds.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        val_loss = val_loss / len(val_loader.dataset)
        val_f1 = f1_score(val_targets, val_preds, average='macro')

        # Get current learning rate
        current_lr = optimizer.param_groups[0]['lr']

        # Update scheduler
        scheduler.step(val_loss)

        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_weights = model.state_dict().copy()
            patience_counter = 0
        else:
            # Check if improvement is significant
            if val_f1 < last_val_f1 + min_delta:
                patience_counter += 1
            else:
                patience_counter = 0

        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1)
        history['learning_rates'].append(current_lr)

        # Update last val F1
        last_val_f1 = val_f1

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')
        print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, LR: {current_lr:.6f}')
        print(f'Patience counter: {patience_counter}/{patience}')

        # Check if training should be stopped
        if patience_counter >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break

        # Check for overfitting
        if train_f1 - val_f1 > 0.2:  # Si l'écart entre train et validation est trop grand
            print(f"Potential overfitting detected (train F1: {train_f1:.4f}, val F1: {val_f1:.4f})")

            # Réduire le taux d'apprentissage manuellement
            for param_group in optimizer.param_groups:
                param_group['lr'] = current_lr * 0.5
            print(f"Learning rate reduced to {optimizer.param_groups[0]['lr']:.6f}")

    # Load best model weights
    model.load_state_dict(best_model_weights)

    return model, history

# Fonction d'évaluation
def evaluate_model(model, dataloader, device='cuda'):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, sizes, labels in tqdm(dataloader, desc="Evaluating"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            outputs = model(inputs, sizes)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    # Calculate F1 score
    f1 = f1_score(all_targets, all_preds, average='macro')
    f1_per_class = f1_score(all_targets, all_preds, average=None)

    # Create confusion matrix
    cm = confusion_matrix(all_targets, all_preds)

    return f1, f1_per_class, cm

# Fonction de prédiction mise à jour
def predict_test_data_with_size(model, test_loader, device='cuda'):
    model.eval()
    predictions = {}
    all_probs = {}  # Pour stocker les probabilités

    with torch.no_grad():
        for inputs, sizes, image_ids in tqdm(test_loader, desc="Predicting test data"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)

            outputs = model(inputs, sizes)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(probs, 1)

            for img_id, pred, prob in zip(image_ids, preds.cpu().numpy(), probs.cpu().numpy()):
                predictions[img_id] = int(pred)
                all_probs[img_id] = prob

    # Vérifier la distribution des prédictions
    pred_counts = {}
    for pred in predictions.values():
        if pred not in pred_counts:
            pred_counts[pred] = 0
        pred_counts[pred] += 1

    print("Distribution des prédictions:")
    for class_id, count in sorted(pred_counts.items()):
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        percentage = count / len(predictions) * 100
        print(f"  {class_name} (ID: {class_id}): {count} exemples ({percentage:.1f}%)")

    # Vérifier si toutes les prédictions sont FOND
    if len(pred_counts) == 1 and 8 in pred_counts:
        print("ATTENTION: Toutes les prédictions sont FOND. Il y a un problème avec le modèle.")

    return predictions, all_probs

# Utility function to create submission file
def create_submission_file(predictions, output_file='submission.csv'):
    with open(output_file, 'w') as f:
        f.write('idx,gt\n')
        for img_id, pred in predictions.items():
            f.write(f'{img_id},{pred}\n')

    print(f"Submission file created: {output_file}")

# Visualization functions
def plot_confusion_matrix(cm, class_mapping):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[class_mapping[i] for i in range(len(class_mapping))],
                yticklabels=[class_mapping[i] for i in range(len(class_mapping))])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    plt.figure(figsize=(15, 10))

    # 1. Graphique des pertes
    plt.subplot(2, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 2. Graphique des F1 scores
    plt.subplot(2, 2, 2)
    plt.plot(history['train_f1'], label='Train F1')
    plt.plot(history['val_f1'], label='Validation F1')
    plt.title('F1 Score over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 3. Graphique du taux d'apprentissage
    if 'learning_rates' in history:
        plt.subplot(2, 2, 3)
        plt.plot(history['learning_rates'], marker='o')
        plt.title('Learning Rate over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.grid(True, alpha=0.3)

    # 4. Graphique de la différence entre train et val F1
    if len(history['train_f1']) > 0 and len(history['val_f1']) > 0:
        plt.subplot(2, 2, 4)
        f1_diff = [train - val for train, val in zip(history['train_f1'], history['val_f1'])]
        plt.plot(f1_diff, color='orange', marker='.')
        plt.axhline(y=0.2, color='r', linestyle='--', alpha=0.3, label='Overfitting Threshold')
        plt.title('Train-Val F1 Difference (Overfitting Indicator)')
        plt.xlabel('Epoch')
        plt.ylabel('Difference')
        plt.legend()
        plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# Augmentation de données avancée pour transformations
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.3),
    transforms.RandomRotation(45),  # Rotation plus importante
    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),  # Plage de mise à l'échelle plus large
    transforms.ColorJitter(
        brightness=0.5,  # Augmentation des variations de luminosité
        contrast=0.5,    # Augmentation des variations de contraste
        saturation=0.5,  # Augmentation des variations de saturation
        hue=0.2          # Augmentation des variations de teinte
    ),
    transforms.RandomGrayscale(p=0.1),  # Plus de probabilité de grayscale
    transforms.RandomAffine(
        degrees=0,
        translate=(0.15, 0.15),  # Plus de translation
        scale=(0.8, 1.2),       # Plus de variation d'échelle
        shear=15                 # Plus de cisaillement
    ),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Fonction principale modifiée
def main():
    # Définir l'appareil
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Définir les chemins
    train_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"
    test_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/datatest/datatest"

    # 1. Traiter les données d'entraînement
    print("Processing training data...")
    processor = CollemboleDataProcessor(train_dir)
    crops, labels = processor.crop_images(resize_size=(224, 224))
    processor.visualize_samples(n_samples=5)

    # 2. Équilibrer le jeu de données avec augmentation agressive
    print("Balancing dataset with aggressive augmentation...")
    # Utiliser 500 samples par classe au lieu de 1000 pour réduire le surapprentissage
    crops_balanced, labels_balanced = balance_dataset(crops, labels, target_count=500, augment=True)

    # 3. Créer des jeux de données et les diviser en train/val avec une plus grande proportion de validation
    X = crops_balanced
    y = np.array(labels_balanced)

    # Split dataset (70% train, 30% validation) - plus de données pour la validation
    from sklearn.model_selection import train_test_split
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

    print(f"Training set: {len(X_train)} samples")
    print(f"Validation set: {len(X_val)} samples")

    # 4. Créer des jeux de données avancés
    train_dataset = SizeAwareDataset(X_train, y_train, transform=train_transform)
    val_dataset = SizeAwareDataset(X_val, y_val, transform=val_transform)

    # 5. Créer les chargeurs de données avec une plus petite taille de batch
    train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False, num_workers=4)

    # 6. Initialiser le modèle avec plus de régularisation
    model = RegularizedEfficientNetB4(num_classes=len(CLASS_MAPPING), pretrained=True)

    # 7. Configurer l'entraînement
    # Calculer les poids des classes pour équilibrer la perte
    class_weights = get_class_weights(labels_balanced).to(device)

    # Critère de perte pondéré avec label smoothing
    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)

    # Optimiseur avec poids de régularisation plus importants
    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-3)

    # Scheduler pour ajuster le taux d'apprentissage plus agressivement
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.2, patience=2, verbose=True, min_lr=1e-6
    )

    # 8. Entraîner le modèle avec early stopping plus strict
    print("Training model with early stopping...")
    model, history = train_model_with_early_stopping(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=30,  # Plus d'époques potentielles, mais early stopping va probablement arrêter avant
        device=device,
        patience=3,     # Moins de patience pour arrêter plus tôt
        min_delta=0.001 # Amélioration minimale requise
    )

    # 9. Sauvegarder le modèle
    torch.save(model.state_dict(), 'regularized_collembole_classifier.pth')

    # 10. Évaluer le modèle
    print("Evaluating model...")
    f1, f1_per_class, cm = evaluate_model(model, val_loader, device)

    print(f"Overall F1 Score: {f1:.4f}")
    print("F1 Score per class:")
    for class_id, score in enumerate(f1_per_class):
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        print(f"  {class_name}: {score:.4f}")

    # 11. Visualiser la matrice de confusion et l'historique d'entraînement
    try:
        plot_confusion_matrix(cm, CLASS_MAPPING)
        plot_training_history(history)
    except Exception as e:
        print(f"Error plotting results: {e}")

    # 12. Prédire sur les données de test
    print("Predicting test data...")
    test_dataset = SizeAwareTestDataset(test_dir, transform=val_transform)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)

    predictions, all_probs = predict_test_data_with_size(model, test_loader, device)

    # Analyser les prédictions pour détecter le problème de classe unique
    pred_classes = list(predictions.values())
    unique_classes = set(pred_classes)

    if len(unique_classes) == 1 and 8 in unique_classes:
        print("ERREUR: Le modèle ne prédit que la classe FOND. Problème détecté.")
        print("Tentative de correction avec des seuils de probabilité personnalisés...")

        # Si toutes les prédictions sont FOND, essayer d'utiliser les probabilités brutes
        corrected_predictions = {}
        for img_id, probs in all_probs.items():
            # Réduire artificiellement la probabilité de la classe FOND pour favoriser d'autres classes
            adjusted_probs = probs.copy()
            adjusted_probs[8] *= 0.5  # Réduire de moitié la probabilité de la classe FOND
            new_pred = np.argmax(adjusted_probs)
            corrected_predictions[img_id] = int(new_pred)

        # Vérifier la nouvelle distribution
        new_pred_counts = {}
        for pred in corrected_predictions.values():
            if pred not in new_pred_counts:
                new_pred_counts[pred] = 0
            new_pred_counts[pred] += 1

        print("Nouvelle distribution après correction:")
        for class_id, count in sorted(new_pred_counts.items()):
            class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
            percentage = count / len(corrected_predictions) * 100
            print(f"  {class_name} (ID: {class_id}): {count} exemples ({percentage:.1f}%)")

        # Si la correction a fonctionné (il y a maintenant plus d'une classe)
        if len(new_pred_counts) > 1:
            print("Correction réussie! Utilisation des prédictions corrigées.")
            create_submission_file(corrected_predictions, 'submission_corrected.csv')
        else:
            print("La correction n'a pas fonctionné. Tentative avec un modèle plus simple...")

            # Essayer avec un modèle plus simple qui ne tient pas compte de la taille
            simple_model = models.efficientnet_b4(pretrained=True)
            num_classes = len(CLASS_MAPPING)
            simple_model.classifier[1] = nn.Linear(simple_model.classifier[1].in_features, num_classes)
            simple_model = simple_model.to(device)

            # Critère et optimiseur pour le modèle simple
            simple_criterion = nn.CrossEntropyLoss()
            simple_optimizer = optim.Adam(simple_model.parameters(), lr=0.001)

            # Entraîner le modèle simple (avec une version simplifiée des données)
            simple_train_transform = transforms.Compose([
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(10),
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])

            simple_val_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])

            # Créer des datasets simplifiés
            class SimpleDataset(Dataset):
                def __init__(self, images, labels, transform=None):
                    self.images = images
                    self.labels = labels
                    self.transform = transform

                def __len__(self):
                    return len(self.images)

                def __getitem__(self, idx):
                    image = self.images[idx]
                    label = self.labels[idx]

                    if self.transform:
                        image = self.transform(image)

                    return image, label

            simple_train_dataset = SimpleDataset(X_train, y_train, transform=simple_train_transform)
            simple_val_dataset = SimpleDataset(X_val, y_val, transform=simple_val_transform)

            simple_train_loader = DataLoader(simple_train_dataset, batch_size=16, shuffle=True)
            simple_val_loader = DataLoader(simple_val_dataset, batch_size=16, shuffle=False)

            # Entraîner le modèle simple
            print("Entraînement d'un modèle plus simple...")
            simple_model.train()
            for epoch in range(10):
                running_loss = 0.0
                for inputs, labels in tqdm(simple_train_loader):
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    simple_optimizer.zero_grad()
                    outputs = simple_model(inputs)
                    loss = simple_criterion(outputs, labels)
                    loss.backward()
                    simple_optimizer.step()

                    running_loss += loss.item() * inputs.size(0)

                epoch_loss = running_loss / len(simple_train_loader.dataset)
                print(f"Epoch {epoch+1}/10, Loss: {epoch_loss:.4f}")

            # Prédire avec le modèle simple
            print("Prédiction avec le modèle simple...")
            simple_model.eval()

            class SimpleTestDataset(Dataset):
                def __init__(self, data_dir, transform=None):
                    self.data_dir = data_dir
                    self.transform = transform
                    self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                                      if f.endswith('.jpg') or f.endswith('.png')]
                    self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in self.image_files]

                def __len__(self):
                    return len(self.image_files)

                def __getitem__(self, idx):
                    image_path = self.image_files[idx]
                    image_id = self.image_ids[idx]

                    image = Image.open(image_path).convert('RGB')

                    if self.transform:
                        image = self.transform(image)

                    return image, image_id

            simple_test_dataset = SimpleTestDataset(test_dir, transform=simple_val_transform)
            simple_test_loader = DataLoader(simple_test_dataset, batch_size=16, shuffle=False)

            simple_predictions = {}
            with torch.no_grad():
                for inputs, image_ids in tqdm(simple_test_loader):
                    inputs = inputs.to(device)
                    outputs = simple_model(inputs)
                    _, preds = torch.max(outputs, 1)

                    for img_id, pred in zip(image_ids, preds.cpu().numpy()):
                        simple_predictions[img_id] = int(pred)

            # Vérifier la distribution des prédictions du modèle simple
            simple_pred_counts = {}
            for pred in simple_predictions.values():
                if pred not in simple_pred_counts:
                    simple_pred_counts[pred] = 0
                simple_pred_counts[pred] += 1

            print("Distribution des prédictions du modèle simple:")
            for class_id, count in sorted(simple_pred_counts.items()):
                class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
                percentage = count / len(simple_predictions) * 100
                print(f"  {class_name} (ID: {class_id}): {count} exemples ({percentage:.1f}%)")

            # Utiliser les prédictions du modèle simple si elles sont plus variées
            if len(simple_pred_counts) > 1:
                print("Utilisation des prédictions du modèle simple.")
                create_submission_file(simple_predictions, 'submission_simple_model.csv')
            else:
                print("Même le modèle simple prédit une seule classe. Utilisation du modèle original sans modifications.")
                # Dans ce cas, revenir à la version originale non modifiée
                create_submission_file(predictions, 'submission_original.csv')
    else:
        # Si les prédictions sont variées, utiliser le modèle amélioré
        create_submission_file(predictions, 'submission_improved.csv')

    print("Done!")

if __name__ == "__main__":
    main()

"""## 4.3.1.1
Nouveau paradigme:bounding boxes avec accord complet ou majoritaire
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
from sklearn.metrics import f1_score, confusion_matrix
from collections import Counter
import seaborn as sns
from tqdm import tqdm
import random

# Define constants
CLASS_MAPPING = {
    0: "AUTRE",
    1: "Cer",
    2: "CRY_THE",
    3: "HYP_MAN",
    4: "ISO_MIN",
    5: "LEP",
    6: "MET_AFF",
    7: "PAR_NOT",
    8: "FOND"
}

# 1. Data Preprocessing
class CollemboleDataProcessor:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.image_files = []
        self.labels_files = []
        self.crops = []
        self.crop_labels = []

    def find_files(self):
        """Find all image and corresponding text files"""
        for file in os.listdir(self.data_dir):
            if file.endswith('.jpg') or file.endswith('.png'):
                img_path = os.path.join(self.data_dir, file)
                txt_path = os.path.join(self.data_dir, os.path.splitext(file)[0] + '.txt')

                if os.path.exists(txt_path):
                    self.image_files.append(img_path)
                    self.labels_files.append(txt_path)

        print(f"Found {len(self.image_files)} images with corresponding label files")

    def parse_label_file(self, label_file):
        """Parse YOLO+ format label file et retourne les bounding boxes avec accord complet ou majoritaire"""
        boxes = []
        labels = []

        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()

                # Vérifier qu'il y a au moins 5 parties (labels + 4 coordonnées)
                if len(parts) < 5:
                    continue

                # Récupérer les labels des experts
                expert_labels = parts[0].split('_')
                # Compter les occurences de chaque label
                label_counter = Counter(expert_labels)
                # Trouver le label le plus fréquent et son compte
                majority_label, count = label_counter.most_common(1)[0]

                # Vérifier qu'il y a une majorité (plus de la moitié des votes)
                if count > len(expert_labels) / 2:
                    try:
                        class_id = int(majority_label)
                    except ValueError:
                        print(f"Impossible de convertir le label '{majority_label}' en entier dans {label_file}")
                        continue

                    # Les 4 derniers éléments sont les coordonnées
                    try:
                        x_center = float(parts[-4])
                        y_center = float(parts[-3])
                        width = float(parts[-2])
                        height = float(parts[-1])

                        # Vérifier que les valeurs sont dans des plages raisonnables
                        if 0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1:
                            boxes.append([x_center, y_center, width, height])
                            labels.append(class_id)
                        else:
                            print(f"Valeurs de coordonnées hors limites: {parts[-4:]} dans {label_file}")
                    except ValueError:
                        # Les 4 derniers éléments ne sont pas des nombres
                        print(f"Impossible de convertir les coordonnées en nombres: {parts[-4:]} dans {label_file}")
                else:
                    # Si aucune majorité n'est trouvée, on ignore la ligne
                    print(f"Aucune majorité trouvée parmi les labels {expert_labels} dans {label_file}")

        return boxes, labels

    def parse_label_fileANCIEN(self, label_file):
        """Parse YOLO+ format label file and return bounding boxes with full agreement"""
        boxes = []
        labels = []

        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()

                # Vérifier qu'il y a au moins 5 parties (labels + 4 coordonnées)
                if len(parts) < 5:
                    continue

                # Le premier élément contient toujours les labels des experts
                expert_labels = parts[0].split('_')

                # Vérifier si tous les experts sont d'accord
                if len(set(expert_labels)) == 1 :
                    # Récupérer la classe
                    class_id = int(expert_labels[0])

                    # Les 4 derniers éléments sont toujours les coordonnées
                    try:
                        # Essayer de convertir les 4 derniers éléments en flottants
                        x_center = float(parts[-4])
                        y_center = float(parts[-3])
                        width = float(parts[-2])
                        height = float(parts[-1])

                        # Vérifier que les valeurs sont dans des plages raisonnables
                        if 0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1:
                            boxes.append([x_center, y_center, width, height])
                            labels.append(class_id)
                        else:
                            print(f"Valeurs de coordonnées hors limites: {parts[-4:]} dans {label_file}")
                    except ValueError:
                        # Les 4 derniers éléments ne sont pas des nombres
                        print(f"Impossible de convertir les coordonnées en nombres: {parts[-4:]} dans {label_file}")

        return boxes, labels

    def generate_background_samples(self, image_file, boxes, num_samples=3, min_size=64, max_size=224):
        """Génère des échantillons de fond (FOND) qui ne chevauchent pas les boîtes existantes"""
        try:
            img = Image.open(image_file)
            img_width, img_height = img.size

            # Convertir les coordonnées relatives en absolues pour toutes les boîtes
            abs_boxes = []
            for box in boxes:
                x_center, y_center, width, height = box
                x1 = int((x_center - width/2) * img_width)
                y1 = int((y_center - height/2) * img_height)
                x2 = int((x_center + width/2) * img_width)
                y2 = int((y_center + height/2) * img_height)
                abs_boxes.append((x1, y1, x2, y2))

            background_crops = []
            attempts = 0
            max_attempts = 50  # Évite les boucles infinies

            while len(background_crops) < num_samples and attempts < max_attempts:
                attempts += 1

                # Taille aléatoire entre min_size et max_size
                crop_width = np.random.randint(min_size, min(max_size, img_width//2))
                crop_height = np.random.randint(min_size, min(max_size, img_height//2))

                # Position aléatoire
                x1 = np.random.randint(0, img_width - crop_width)
                y1 = np.random.randint(0, img_height - crop_height)
                x2 = x1 + crop_width
                y2 = y1 + crop_height

                # Vérifie si ce crop chevauche une boîte existante
                overlap = False
                for box_x1, box_y1, box_x2, box_y2 in abs_boxes:
                    if not (x2 < box_x1 or x1 > box_x2 or y2 < box_y1 or y1 > box_y2):
                        overlap = True
                        break

                # Si pas de chevauchement, ajouter comme échantillon de fond
                if not overlap:
                    crop = img.crop((x1, y1, x2, y2))
                    crop = crop.resize((224, 224), Image.LANCZOS)
                    background_crops.append(crop)

            return background_crops
        except Exception as e:
            print(f"Erreur lors de la génération d'échantillons de fond: {e}")
            return []

    def crop_images(self, resize_size=(224, 224)):
        """Extract crops from images using bounding boxes with full agreement and generate background samples"""
        self.find_files()

        for img_path, label_path in tqdm(zip(self.image_files, self.labels_files), total=len(self.image_files)):
            try:
                img = Image.open(img_path)
                img_width, img_height = img.size

                boxes, labels = self.parse_label_file(label_path)

                for box, label in zip(boxes, labels):
                    # Convert relative coordinates to absolute
                    x_center, y_center, width, height = box
                    x_center *= img_width
                    y_center *= img_height
                    width *= img_width
                    height *= img_height

                    # Vérifier que width et height sont positifs
                    if width <= 0 or height <= 0:
                        continue

                    # Calculate box coordinates
                    x1 = int(x_center - width / 2)
                    y1 = int(y_center - height / 2)
                    x2 = int(x_center + width / 2)
                    y2 = int(y_center + height / 2)

                    # Ensure coordinates are within image bounds
                    x1 = max(0, x1)
                    y1 = max(0, y1)
                    x2 = min(img_width, x2)
                    y2 = min(img_height, y2)

                    # S'assurer que x2 > x1 et y2 > y1
                    if x2 <= x1 or y2 <= y1:
                        continue

                    # Crop the image
                    crop = img.crop((x1, y1, x2, y2))

                    # Resize the crop
                    crop = crop.resize(resize_size, Image.LANCZOS)

                    self.crops.append(crop)
                    self.crop_labels.append(label)

                # Ne générer des échantillons de fond que pour certaines images
                # Limiter le nombre total d'échantillons de fond à environ 300
                if len(self.crop_labels) > 0 and self.crop_labels.count(8) < 300:
                    # Générer aléatoirement 0 ou 1 échantillon avec une probabilité de 0.3
                    if np.random.random() < 0.3:
                        bg_samples = self.generate_background_samples(img_path, boxes, num_samples=1)
                        for bg_sample in bg_samples:
                            self.crops.append(bg_sample)
                            self.crop_labels.append(8)  # Classe 8 = FOND

            except Exception as e:
                print(f"Error processing {img_path}: {e}")

        print(f"Generated {len(self.crops)} crops (including background samples)")

        # Afficher la distribution des classes
        unique_labels, counts = np.unique(self.crop_labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = CLASS_MAPPING.get(label, f"Class {label}")
            print(f"  {class_name} (ID: {label}): {count} samples")

        return self.crops, self.crop_labels

    def visualize_samples(self, n_samples=5):
        """Visualize random sample crops with their labels"""
        if not self.crops:
            print("No crops available. Run crop_images() first.")
            return

        # Sélectionner des échantillons de chaque classe si possible
        samples_by_class = {}
        for i, label in enumerate(self.crop_labels):
            if label not in samples_by_class:
                samples_by_class[label] = []
            if len(samples_by_class[label]) < n_samples:
                samples_by_class[label].append(i)

        # Créer une liste de tous les échantillons à visualiser
        all_samples = []
        for label, samples in samples_by_class.items():
            all_samples.extend(samples[:min(n_samples, len(samples))])

        # Si trop d'échantillons, sélectionner aléatoirement
        if len(all_samples) > n_samples * len(samples_by_class):
            all_samples = np.random.choice(all_samples, n_samples * len(samples_by_class), replace=False)

        # Créer la figure
        n_cols = min(5, len(all_samples))
        n_rows = (len(all_samples) + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))
        axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]

        for i, idx in enumerate(all_samples):
            if i < len(axes):
                axes[i].imshow(self.crops[idx])
                label = self.crop_labels[idx]
                class_name = CLASS_MAPPING.get(label, f"Class {label}")
                axes[i].set_title(f"{class_name} (ID: {label})")
                axes[i].axis('off')

        # Masquer les axes inutilisés
        for i in range(len(all_samples), len(axes)):
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

def balance_dataset(crops, labels, target_count=None, augment=True):
    """
    Équilibre les données pour avoir le même nombre d'échantillons par classe.

    Args:
        crops: Liste d'images (PIL.Image)
        labels: Liste des étiquettes correspondantes
        target_count: Nombre cible d'échantillons par classe (si None, utilise le maximum disponible)
        augment: Si True, utilise l'augmentation de données pour les classes sous-représentées
                Si False, duplique simplement les échantillons existants

    Returns:
        balanced_crops: Liste équilibrée d'images
        balanced_labels: Liste équilibrée d'étiquettes
    """
    import numpy as np
    from PIL import Image, ImageEnhance
    import random

    # Compter les échantillons par classe
    unique_labels = np.unique(labels)
    class_counts = {}
    class_indices = {}

    # Organiser les indices par classe
    for label in unique_labels:
        indices = [i for i, l in enumerate(labels) if l == label]
        class_counts[label] = len(indices)
        class_indices[label] = indices

    print("Distribution originale des classes:")
    for label, count in class_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    # Déterminer le nombre cible d'échantillons par classe
    if target_count is None:
        # Utiliser la classe majoritaire comme cible
        target_count = max(class_counts.values())

    print(f"Cible: {target_count} échantillons par classe")

    # Fonctions d'augmentation de données
    def augment_image(img):
        """Applique une augmentation aléatoire à une image"""
        if not isinstance(img, Image.Image):
            print(f"Warning: Expected PIL Image but got {type(img)}. Skipping augmentation.")
            return img

        # Choisir une augmentation aléatoire
        aug_type = random.choice(['rotate', 'flip'])  # Simplifié pour éviter les erreurs

        if aug_type == 'rotate':
            # Rotation aléatoire entre -15 et 15 degrés
            angle = random.uniform(-15, 15)
            return img.rotate(angle, resample=Image.BICUBIC, expand=False)

        elif aug_type == 'flip':
            # Retournement horizontal
            return img.transpose(Image.FLIP_LEFT_RIGHT)

    # Créer le jeu de données équilibré
    balanced_crops = []
    balanced_labels = []

    for label in unique_labels:
        # Récupérer les indices des échantillons de cette classe
        class_idx = class_indices[label]
        class_size = len(class_idx)

        # Si la classe a suffisamment d'échantillons, sous-échantillonner
        if class_size >= target_count:
            # Choisir aléatoirement target_count échantillons
            selected_indices = np.random.choice(class_idx, target_count, replace=False)
            for idx in selected_indices:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

        # Sinon, augmenter le nombre d'échantillons
        else:
            # Ajouter tous les échantillons existants
            for idx in class_idx:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

            # Compléter avec des échantillons augmentés ou dupliqués
            samples_to_add = target_count - class_size

            if augment:
                # Augmentation de données
                for _ in range(samples_to_add):
                    # Choisir un échantillon aléatoire à augmenter
                    random_idx = class_idx[random.randint(0, class_size - 1)]
                    original_img = crops[random_idx]

                    # Appliquer l'augmentation
                    augmented_img = augment_image(original_img)

                    balanced_crops.append(augmented_img)
                    balanced_labels.append(label)
            else:
                # Simple duplication d'échantillons existants
                for i in range(samples_to_add):
                    # Sélection circulaire des échantillons
                    idx = class_idx[i % class_size]
                    balanced_crops.append(crops[idx])
                    balanced_labels.append(label)

    # Mélanger le jeu de données
    combined = list(zip(balanced_crops, balanced_labels))
    random.shuffle(combined)
    balanced_crops, balanced_labels = zip(*combined)

    # Convertir en listes
    balanced_crops = list(balanced_crops)
    balanced_labels = list(balanced_labels)

    # Vérifier la distribution finale
    final_counts = {}
    for label in balanced_labels:
        if label not in final_counts:
            final_counts[label] = 0
        final_counts[label] += 1

    print("Distribution équilibrée des classes:")
    for label, count in final_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    return balanced_crops, balanced_labels

# Dataset classes that include image size information
class SizeAwareDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform
        # Store original image dimensions
        self.original_sizes = []
        for img in images:
            width, height = img.size
            self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        # Apply transformations to the image
        if self.transform:
            image = self.transform(image)

        # Normalize dimensions for network input (between 0 and 1)
        # This helps the network learn relative size relationships
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,  # Divide by a sufficiently large value
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        return image, size_tensor, label

class SizeAwareTestDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                           if f.endswith('.jpg') or f.endswith('.png')]
        self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in self.image_files]

        # Store original image dimensions
        self.original_sizes = []
        for image_path in self.image_files:
            with Image.open(image_path) as img:
                width, height = img.size
                self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        image_id = self.image_ids[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        # Normalize dimensions
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        return image, size_tensor, image_id

# Size-aware model
class SizeAwareCollemboleClassifier(nn.Module):
    def __init__(self, num_classes=9, pretrained=True):
        super(SizeAwareCollemboleClassifier, self).__init__()
        # Use EfficientNet-B4 for better performance
        self.base_model = models.efficientnet_b4(pretrained=pretrained)

        # Remove the final classification layer from the base model
        self.features = nn.Sequential(*list(self.base_model.children())[:-1])

        # Get the number of output features from EfficientNet
        in_features = self.base_model.classifier[1].in_features

        # Create a layer for image size features (2 values: width, height)
        self.size_encoder = nn.Sequential(
            nn.Linear(2, 64),
            nn.ReLU(),
            nn.Linear(64, 256),
            nn.ReLU()
        )

        # Combine visual features and size features
        self.classifier = nn.Sequential(
            nn.Linear(in_features + 256, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, num_classes)
        )

    def forward(self, x, img_size):
        # Extract visual features
        features = self.features(x)
        features = features.view(features.size(0), -1)

        # Encode size information
        size_features = self.size_encoder(img_size)

        # Concatenate features
        combined = torch.cat((features, size_features), dim=1)

        # Final classification
        output = self.classifier(combined)

        return output

# Training function
def train_model_with_size(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device='cuda'):
    model = model.to(device)
    best_val_f1 = 0.0
    best_model_weights = None

    history = {
        'train_loss': [],
        'val_loss': [],
        'train_f1': [],
        'val_f1': []
    }

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []

        for inputs, sizes, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs, sizes)
            loss = criterion(outputs, labels)

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            # Statistics
            train_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            train_preds.extend(preds.cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        train_loss = train_loss / len(train_loader.dataset)
        train_f1 = f1_score(train_targets, train_preds, average='macro')

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []

        with torch.no_grad():
            for inputs, sizes, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation"):
                inputs = inputs.to(device)
                sizes = sizes.to(device)
                labels = labels.to(device)

                # Forward pass
                outputs = model(inputs, sizes)
                loss = criterion(outputs, labels)

                # Statistics
                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_preds.extend(preds.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        val_loss = val_loss / len(val_loader.dataset)
        val_f1 = f1_score(val_targets, val_preds, average='macro')

        # Update scheduler
        scheduler.step(val_loss)

        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_weights = model.state_dict().copy()

        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1)

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')
        print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')

    # Load best model weights
    model.load_state_dict(best_model_weights)

    return model, history

# Evaluation function
def evaluate_model_with_size(model, dataloader, device='cuda'):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, sizes, labels in tqdm(dataloader, desc="Evaluating"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            outputs = model(inputs, sizes)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    # Calculate F1 score
    f1 = f1_score(all_targets, all_preds, average='macro')
    f1_per_class = f1_score(all_targets, all_preds, average=None)

    # Create confusion matrix
    cm = confusion_matrix(all_targets, all_preds)

    return f1, f1_per_class, cm

# Prediction function for test data
def predict_test_data_with_size(model, test_loader, device='cuda'):
    model.eval()
    predictions = {}
    all_probs = {}  # For storing probabilities

    with torch.no_grad():
        for inputs, sizes, image_ids in tqdm(test_loader, desc="Predicting test data"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)

            outputs = model(inputs, sizes)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(probs, 1)

            for img_id, pred, prob in zip(image_ids, preds.cpu().numpy(), probs.cpu().numpy()):
                predictions[img_id] = int(pred)
                all_probs[img_id] = prob

    return predictions, all_probs

# Visualization functions
def plot_confusion_matrix(cm, class_mapping):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[class_mapping[i] for i in range(len(class_mapping))],
                yticklabels=[class_mapping[i] for i in range(len(class_mapping))])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_f1'], label='Train F1')
    plt.plot(history['val_f1'], label='Validation F1')
    plt.title('F1 Score over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Utility function to create submission file
def create_submission_file(predictions, output_file='submission.csv'):
    with open(output_file, 'w') as f:
        f.write('idx,gt\n')
        for img_id, pred in predictions.items():
            f.write(f'{img_id},{pred}\n')

    print(f"Submission file created: {output_file}")

# Main function that puts everything together
def main():
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Set paths
    train_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"
    test_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/datatest/datatest"

    # 1. Process training data
    print("Processing training data...")
    processor = CollemboleDataProcessor(train_dir)
    crops, labels = processor.crop_images(resize_size=(224, 224))  # Keeping original crop size
    processor.visualize_samples(n_samples=5)

    # 2. Balance dataset
    print("Balancing dataset...")
    crops_balanced, labels_balanced = balance_dataset(crops, labels, target_count=200, augment=True)

    # 3. Prepare data transforms - Now we're resizing to 224x224 to match original model, but keeping original size info
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Keep at 224
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize(256),  # Standard resize
        transforms.CenterCrop(224),  # Keep at 224
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # 4. Create datasets and split into train/val
    X = crops_balanced
    y = np.array(labels_balanced)

    # Split dataset (80% train, 20% validation)
    from sklearn.model_selection import train_test_split
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    print(f"Training set: {len(X_train)} samples")
    print(f"Validation set: {len(X_val)} samples")

    # Create size-aware datasets
    train_dataset = SizeAwareDataset(X_train, y_train, transform=train_transform)
    val_dataset = SizeAwareDataset(X_val, y_val, transform=val_transform)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)  # Reduced batch size for B4
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)

    # 5. Initialize model
    model = SizeAwareCollemboleClassifier(num_classes=len(CLASS_MAPPING), pretrained=True)

    # 6. Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)  # Added weight decay
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)

    # 7. Train model
    print("Training model...")
    model, history = train_model_with_size(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=15,  # Increased for more complex model
        device=device
    )

    # Save model
    torch.save(model.state_dict(), 'collembole_classifier_b4_size_aware.pth')

    # 8. Evaluate model
    print("Evaluating model...")
    f1, f1_per_class, cm = evaluate_model_with_size(model, val_loader, device)

    print(f"Overall F1 Score: {f1:.4f}")
    print("F1 Score per class:")
    for class_id, score in enumerate(f1_per_class):
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        print(f"  {class_name}: {score:.4f}")

    plot_confusion_matrix(cm, CLASS_MAPPING)
    plot_training_history(history)

    # 9. Predict test data
    print("Predicting test data...")
    test_dataset = SizeAwareTestDataset(test_dir, transform=val_transform)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)

    predictions, all_probs = predict_test_data_with_size(model, test_loader, device)
    create_submission_file(predictions, 'submission_b4_size_aware.csv')

    print("Done!")

if __name__ == "__main__":
    main()

"""## 4.4"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
from sklearn.metrics import f1_score, confusion_matrix
import seaborn as sns
from tqdm import tqdm
import random

# Define constants
CLASS_MAPPING = {
    0: "AUTRE",
    1: "Cer",
    2: "CRY_THE",
    3: "HYP_MAN",
    4: "ISO_MIN",
    5: "LEP",
    6: "MET_AFF",
    7: "PAR_NOT",
    8: "FOND"
}

# 1. Data Preprocessing
class CollemboleDataProcessor:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.image_files = []
        self.labels_files = []
        self.crops = []
        self.crop_labels = []

    def find_files(self):
        """Find all image and corresponding text files"""
        for file in os.listdir(self.data_dir):
            if file.endswith('.jpg') or file.endswith('.png'):
                img_path = os.path.join(self.data_dir, file)
                txt_path = os.path.join(self.data_dir, os.path.splitext(file)[0] + '.txt')

                if os.path.exists(txt_path):
                    self.image_files.append(img_path)
                    self.labels_files.append(txt_path)

        print(f"Found {len(self.image_files)} images with corresponding label files")

    def parse_label_file(self, label_file):
        """Parse YOLO+ format label file and return bounding boxes with full agreement"""
        boxes = []
        labels = []

        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()

                # Vérifier qu'il y a au moins 5 parties (labels + 4 coordonnées)
                if len(parts) < 5:
                    continue

                # Le premier élément contient toujours les labels des experts
                expert_labels = parts[0].split('_')

                # Vérifier si tous les experts sont d'accord
                if len(set(expert_labels)) == 1:
                    # Récupérer la classe
                    class_id = int(expert_labels[0])

                    # Les 4 derniers éléments sont toujours les coordonnées
                    try:
                        # Essayer de convertir les 4 derniers éléments en flottants
                        x_center = float(parts[-4])
                        y_center = float(parts[-3])
                        width = float(parts[-2])
                        height = float(parts[-1])

                        # Vérifier que les valeurs sont dans des plages raisonnables
                        if 0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1:
                            boxes.append([x_center, y_center, width, height])
                            labels.append(class_id)
                        else:
                            print(f"Valeurs de coordonnées hors limites: {parts[-4:]} dans {label_file}")
                    except ValueError:
                        # Les 4 derniers éléments ne sont pas des nombres
                        print(f"Impossible de convertir les coordonnées en nombres: {parts[-4:]} dans {label_file}")

        return boxes, labels

    def generate_background_samples(self, image_file, boxes, num_samples=3, min_size=64, max_size=224):
        """Génère des échantillons de fond (FOND) qui ne chevauchent pas les boîtes existantes"""
        try:
            img = Image.open(image_file)
            img_width, img_height = img.size

            # Convertir les coordonnées relatives en absolues pour toutes les boîtes
            abs_boxes = []
            for box in boxes:
                x_center, y_center, width, height = box
                x1 = int((x_center - width/2) * img_width)
                y1 = int((y_center - height/2) * img_height)
                x2 = int((x_center + width/2) * img_width)
                y2 = int((y_center + height/2) * img_height)
                abs_boxes.append((x1, y1, x2, y2))

            background_crops = []
            attempts = 0
            max_attempts = 50  # Évite les boucles infinies

            while len(background_crops) < num_samples and attempts < max_attempts:
                attempts += 1

                # Taille aléatoire entre min_size et max_size
                crop_width = np.random.randint(min_size, min(max_size, img_width//2))
                crop_height = np.random.randint(min_size, min(max_size, img_height//2))

                # Position aléatoire
                x1 = np.random.randint(0, img_width - crop_width)
                y1 = np.random.randint(0, img_height - crop_height)
                x2 = x1 + crop_width
                y2 = y1 + crop_height

                # Vérifie si ce crop chevauche une boîte existante
                overlap = False
                for box_x1, box_y1, box_x2, box_y2 in abs_boxes:
                    if not (x2 < box_x1 or x1 > box_x2 or y2 < box_y1 or y1 > box_y2):
                        overlap = True
                        break

                # Si pas de chevauchement, ajouter comme échantillon de fond
                if not overlap:
                    crop = img.crop((x1, y1, x2, y2))
                    crop = crop.resize((224, 224), Image.LANCZOS)
                    background_crops.append(crop)

            return background_crops
        except Exception as e:
            print(f"Erreur lors de la génération d'échantillons de fond: {e}")
            return []

    def crop_images(self, resize_size=(224, 224)):
        """Extract crops from images using bounding boxes with full agreement and generate background samples"""
        self.find_files()

        for img_path, label_path in tqdm(zip(self.image_files, self.labels_files), total=len(self.image_files)):
            try:
                img = Image.open(img_path)
                img_width, img_height = img.size

                boxes, labels = self.parse_label_file(label_path)

                for box, label in zip(boxes, labels):
                    # Convert relative coordinates to absolute
                    x_center, y_center, width, height = box
                    x_center *= img_width
                    y_center *= img_height
                    width *= img_width
                    height *= img_height

                    # Vérifier que width et height sont positifs
                    if width <= 0 or height <= 0:
                        continue

                    # Calculate box coordinates
                    x1 = int(x_center - width / 2)
                    y1 = int(y_center - height / 2)
                    x2 = int(x_center + width / 2)
                    y2 = int(y_center + height / 2)

                    # Ensure coordinates are within image bounds
                    x1 = max(0, x1)
                    y1 = max(0, y1)
                    x2 = min(img_width, x2)
                    y2 = min(img_height, y2)

                    # S'assurer que x2 > x1 et y2 > y1
                    if x2 <= x1 or y2 <= y1:
                        continue

                    # Crop the image
                    crop = img.crop((x1, y1, x2, y2))

                    # Resize the crop
                    crop = crop.resize(resize_size, Image.LANCZOS)

                    self.crops.append(crop)
                    self.crop_labels.append(label)

                # Ne générer des échantillons de fond que pour certaines images
                # Limiter le nombre total d'échantillons de fond à environ 300
                if len(self.crop_labels) > 0 and self.crop_labels.count(8) < 300:
                    # Générer aléatoirement 0 ou 1 échantillon avec une probabilité de 0.3
                    if np.random.random() < 0.3:
                        bg_samples = self.generate_background_samples(img_path, boxes, num_samples=1)
                        for bg_sample in bg_samples:
                            self.crops.append(bg_sample)
                            self.crop_labels.append(8)  # Classe 8 = FOND

            except Exception as e:
                print(f"Error processing {img_path}: {e}")

        print(f"Generated {len(self.crops)} crops (including background samples)")

        # Afficher la distribution des classes
        unique_labels, counts = np.unique(self.crop_labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = CLASS_MAPPING.get(label, f"Class {label}")
            print(f"  {class_name} (ID: {label}): {count} samples")

        return self.crops, self.crop_labels

    def visualize_samples(self, n_samples=5):
        """Visualize random sample crops with their labels"""
        if not self.crops:
            print("No crops available. Run crop_images() first.")
            return

        # Sélectionner des échantillons de chaque classe si possible
        samples_by_class = {}
        for i, label in enumerate(self.crop_labels):
            if label not in samples_by_class:
                samples_by_class[label] = []
            if len(samples_by_class[label]) < n_samples:
                samples_by_class[label].append(i)

        # Créer une liste de tous les échantillons à visualiser
        all_samples = []
        for label, samples in samples_by_class.items():
            all_samples.extend(samples[:min(n_samples, len(samples))])

        # Si trop d'échantillons, sélectionner aléatoirement
        if len(all_samples) > n_samples * len(samples_by_class):
            all_samples = np.random.choice(all_samples, n_samples * len(samples_by_class), replace=False)

        # Créer la figure
        n_cols = min(5, len(all_samples))
        n_rows = (len(all_samples) + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))
        axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]

        for i, idx in enumerate(all_samples):
            if i < len(axes):
                axes[i].imshow(self.crops[idx])
                label = self.crop_labels[idx]
                class_name = CLASS_MAPPING.get(label, f"Class {label}")
                axes[i].set_title(f"{class_name} (ID: {label})")
                axes[i].axis('off')

        # Masquer les axes inutilisés
        for i in range(len(all_samples), len(axes)):
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

def balance_dataset(crops, labels, target_count=None, augment=True):
    """
    Équilibre les données pour avoir le même nombre d'échantillons par classe.

    Args:
        crops: Liste d'images (PIL.Image)
        labels: Liste des étiquettes correspondantes
        target_count: Nombre cible d'échantillons par classe (si None, utilise le maximum disponible)
        augment: Si True, utilise l'augmentation de données pour les classes sous-représentées
                Si False, duplique simplement les échantillons existants

    Returns:
        balanced_crops: Liste équilibrée d'images
        balanced_labels: Liste équilibrée d'étiquettes
    """
    import numpy as np
    from PIL import Image, ImageEnhance
    import random

    # Compter les échantillons par classe
    unique_labels = np.unique(labels)
    class_counts = {}
    class_indices = {}

    # Organiser les indices par classe
    for label in unique_labels:
        indices = [i for i, l in enumerate(labels) if l == label]
        class_counts[label] = len(indices)
        class_indices[label] = indices

    print("Distribution originale des classes:")
    for label, count in class_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    # Déterminer le nombre cible d'échantillons par classe
    if target_count is None:
        # Utiliser la classe majoritaire comme cible
        target_count = max(class_counts.values())

    print(f"Cible: {target_count} échantillons par classe")

    # Fonctions d'augmentation de données
    def augment_image(img):
        """Applique une augmentation aléatoire à une image"""
        if not isinstance(img, Image.Image):
            print(f"Warning: Expected PIL Image but got {type(img)}. Skipping augmentation.")
            return img

        # Choisir une augmentation aléatoire
        aug_type = random.choice(['rotate', 'flip'])  # Simplifié pour éviter les erreurs

        if aug_type == 'rotate':
            # Rotation aléatoire entre -15 et 15 degrés
            angle = random.uniform(-15, 15)
            return img.rotate(angle, resample=Image.BICUBIC, expand=False)

        elif aug_type == 'flip':
            # Retournement horizontal
            return img.transpose(Image.FLIP_LEFT_RIGHT)

    # Créer le jeu de données équilibré
    balanced_crops = []
    balanced_labels = []

    for label in unique_labels:
        # Récupérer les indices des échantillons de cette classe
        class_idx = class_indices[label]
        class_size = len(class_idx)

        # Si la classe a suffisamment d'échantillons, sous-échantillonner
        if class_size >= target_count:
            # Choisir aléatoirement target_count échantillons
            selected_indices = np.random.choice(class_idx, target_count, replace=False)
            for idx in selected_indices:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

        # Sinon, augmenter le nombre d'échantillons
        else:
            # Ajouter tous les échantillons existants
            for idx in class_idx:
                balanced_crops.append(crops[idx])
                balanced_labels.append(label)

            # Compléter avec des échantillons augmentés ou dupliqués
            samples_to_add = target_count - class_size

            if augment:
                # Augmentation de données
                for _ in range(samples_to_add):
                    # Choisir un échantillon aléatoire à augmenter
                    random_idx = class_idx[random.randint(0, class_size - 1)]
                    original_img = crops[random_idx]

                    # Appliquer l'augmentation
                    augmented_img = augment_image(original_img)

                    balanced_crops.append(augmented_img)
                    balanced_labels.append(label)
            else:
                # Simple duplication d'échantillons existants
                for i in range(samples_to_add):
                    # Sélection circulaire des échantillons
                    idx = class_idx[i % class_size]
                    balanced_crops.append(crops[idx])
                    balanced_labels.append(label)

    # Mélanger le jeu de données
    combined = list(zip(balanced_crops, balanced_labels))
    random.shuffle(combined)
    balanced_crops, balanced_labels = zip(*combined)

    # Convertir en listes
    balanced_crops = list(balanced_crops)
    balanced_labels = list(balanced_labels)

    # Vérifier la distribution finale
    final_counts = {}
    for label in balanced_labels:
        if label not in final_counts:
            final_counts[label] = 0
        final_counts[label] += 1

    print("Distribution équilibrée des classes:")
    for label, count in final_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    return balanced_crops, balanced_labels

# Dataset classes that include image size information
class SizeAwareDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform
        # Store original image dimensions
        self.original_sizes = []
        for img in images:
            width, height = img.size
            self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        # Apply transformations to the image
        if self.transform:
            image = self.transform(image)

        # Pour la classe FOND (8), on utilise un vecteur de taille spécial
        # Pour éviter le biais dû à la génération artificielle
        if label == 8:  # FOND
            # Valeur neutre qui n'introduit pas de biais pour la classe FOND
            # On utilise la moyenne des tailles des images non-FOND
            size_tensor = torch.tensor([0.5, 0.5], dtype=torch.float32)
        else:
            # Normalize dimensions for network input (between 0 and 1)
            # This helps the network learn relative size relationships
            size_tensor = torch.tensor([
                original_size[0] / 1000.0,  # Divide by a sufficiently large value
                original_size[1] / 1000.0
            ], dtype=torch.float32)

        return image, size_tensor, label

class SizeAwareTestDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                           if f.endswith('.jpg') or f.endswith('.png')]
        self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in self.image_files]

        # Store original image dimensions
        self.original_sizes = []
        for image_path in self.image_files:
            with Image.open(image_path) as img:
                width, height = img.size
                self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        image_id = self.image_ids[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        # Pour les images de test, on n'a pas d'information sur la classe
        # On utilise simplement les dimensions normalisées
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        return image, size_tensor, image_id

# Size-aware model
class ImprovedSizeAwareClassifier(nn.Module):
    def __init__(self, num_classes=9, pretrained=True):
        super(ImprovedSizeAwareClassifier, self).__init__()
        # Use EfficientNet-B4 for better performance
        self.base_model = models.efficientnet_b4(pretrained=pretrained)

        # Remove the final classification layer from the base model
        self.features = nn.Sequential(*list(self.base_model.children())[:-1])

        # Get the number of output features from EfficientNet
        in_features = self.base_model.classifier[1].in_features

        # Create a layer for image size features (2 values: width, height)
        self.size_encoder = nn.Sequential(
            nn.Linear(2, 64),
            nn.ReLU(),
            nn.Linear(64, 256),
            nn.ReLU()
        )

        # Combine visual features and size features
        self.feature_combiner = nn.Sequential(
            nn.Linear(in_features + 256, 512),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # Final classifier
        self.classifier = nn.Linear(512, num_classes)

    def forward(self, x, img_size):
        # Extract visual features
        features = self.features(x)
        features = features.view(features.size(0), -1)

        # Encode size information
        size_features = self.size_encoder(img_size)

        # Concatenate features
        combined = torch.cat((features, size_features), dim=1)

        # Process combined features
        processed_features = self.feature_combiner(combined)

        # Final classification
        output = self.classifier(processed_features)

        return output

# Training function
def train_model_with_size(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device='cuda'):
    model = model.to(device)
    best_val_f1 = 0.0
    best_model_weights = None

    history = {
        'train_loss': [],
        'val_loss': [],
        'train_f1': [],
        'val_f1': []
    }

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []

        for inputs, sizes, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs, sizes)
            loss = criterion(outputs, labels)

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            # Statistics
            train_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            train_preds.extend(preds.cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        train_loss = train_loss / len(train_loader.dataset)
        train_f1 = f1_score(train_targets, train_preds, average='macro')

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []

        with torch.no_grad():
            for inputs, sizes, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation"):
                inputs = inputs.to(device)
                sizes = sizes.to(device)
                labels = labels.to(device)

                # Forward pass
                outputs = model(inputs, sizes)
                loss = criterion(outputs, labels)

                # Statistics
                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_preds.extend(preds.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        val_loss = val_loss / len(val_loader.dataset)
        val_f1 = f1_score(val_targets, val_preds, average='macro')

        # Update scheduler
        scheduler.step(val_loss)

        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_weights = model.state_dict().copy()

        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1)

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')
        print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')

    # Load best model weights
    model.load_state_dict(best_model_weights)

    return model, history

# Evaluation function
def evaluate_model_with_size(model, dataloader, device='cuda'):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, sizes, labels in tqdm(dataloader, desc="Evaluating"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)
            labels = labels.to(device)

            outputs = model(inputs, sizes)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    # Calculate F1 score
    f1 = f1_score(all_targets, all_preds, average='macro')
    f1_per_class = f1_score(all_targets, all_preds, average=None)

    # Create confusion matrix
    cm = confusion_matrix(all_targets, all_preds)

    return f1, f1_per_class, cm

# Prediction function for test data
def predict_test_data_with_size(model, test_loader, device='cuda'):
    model.eval()
    predictions = {}
    all_probs = {}  # For storing probabilities

    with torch.no_grad():
        for inputs, sizes, image_ids in tqdm(test_loader, desc="Predicting test data"):
            inputs = inputs.to(device)
            sizes = sizes.to(device)

            outputs = model(inputs, sizes)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(probs, 1)

            for img_id, pred, prob in zip(image_ids, preds.cpu().numpy(), probs.cpu().numpy()):
                predictions[img_id] = int(pred)
                all_probs[img_id] = prob

    return predictions, all_probs

# Visualization functions
def plot_confusion_matrix(cm, class_mapping):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[class_mapping[i] for i in range(len(class_mapping))],
                yticklabels=[class_mapping[i] for i in range(len(class_mapping))])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_f1'], label='Train F1')
    plt.plot(history['val_f1'], label='Validation F1')
    plt.title('F1 Score over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Utility function to create submission file
def create_submission_file(predictions, output_file='submission.csv'):
    with open(output_file, 'w') as f:
        f.write('idx,gt\n')
        for img_id, pred in predictions.items():
            f.write(f'{img_id},{pred}\n')

    print(f"Submission file created: {output_file}")

# Main function that puts everything together
def main():
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Set paths - UPDATED PATHS
    train_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"
    test_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/datatest/datatest"

    # 1. Process training data
    print("Processing training data...")
    processor = CollemboleDataProcessor(train_dir)
    crops, labels = processor.crop_images(resize_size=(224, 224))  # Keeping original crop size
    processor.visualize_samples(n_samples=5)

    # 2. Balance dataset
    print("Balancing dataset...")
    crops_balanced, labels_balanced = balance_dataset(crops, labels, target_count=200, augment=True)

    # 3. Prepare data transforms - We're keeping the standard sizes
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # 4. Create datasets and split into train/val
    X = crops_balanced
    y = np.array(labels_balanced)

    # Split dataset (80% train, 20% validation)
    from sklearn.model_selection import train_test_split
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    print(f"Training set: {len(X_train)} samples")
    print(f"Validation set: {len(X_val)} samples")

    # Create size-aware datasets with special handling for FOND class
    train_dataset = SizeAwareDataset(X_train, y_train, transform=train_transform)
    val_dataset = SizeAwareDataset(X_val, y_val, transform=val_transform)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)

    # 5. Initialize model
    model = ImprovedSizeAwareClassifier(num_classes=len(CLASS_MAPPING), pretrained=True)

    # 6. Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)

    # 7. Train model
    print("Training model...")
    model, history = train_model_with_size(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=15,
        device=device
    )

    # Save model
    torch.save(model.state_dict(), 'collembole_classifier_b4_size_aware_improved.pth')

    # 8. Evaluate model
    print("Evaluating model...")
    f1, f1_per_class, cm = evaluate_model_with_size(model, val_loader, device)

    print(f"Overall F1 Score: {f1:.4f}")
    print("F1 Score per class:")
    for class_id, score in enumerate(f1_per_class):
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        print(f"  {class_name}: {score:.4f}")

    plot_confusion_matrix(cm, CLASS_MAPPING)
    plot_training_history(history)

    # 9. Predict test data
    print("Predicting test data...")
    test_dataset = SizeAwareTestDataset(test_dir, transform=val_transform)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)

    predictions, all_probs = predict_test_data_with_size(model, test_loader, device)
    create_submission_file(predictions, 'submission_b4_size_aware_improved.csv')

    print("Done!")

if __name__ == "__main__":
    main()

"""## 4.3.1.2"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
from sklearn.metrics import f1_score, confusion_matrix
from collections import Counter
import seaborn as sns
from tqdm import tqdm
import random

# Define constants
CLASS_MAPPING = {
    0: "AUTRE",
    1: "Cer",
    2: "CRY_THE",
    3: "HYP_MAN",
    4: "ISO_MIN",
    5: "LEP",
    6: "MET_AFF",
    7: "PAR_NOT",
    8: "FOND"
}

# 1. Data Preprocessing
class CollemboleDataProcessor:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.image_files = []
        self.labels_files = []
        self.crops = []
        self.crop_labels = []
        self.crop_confidences = []  # Pour stocker la confiance associée à chaque label

    def find_files(self):
        """Find all image and corresponding text files"""
        for file in os.listdir(self.data_dir):
            if file.endswith('.jpg') or file.endswith('.png'):
                img_path = os.path.join(self.data_dir, file)
                txt_path = os.path.join(self.data_dir, os.path.splitext(file)[0] + '.txt')

                if os.path.exists(txt_path):
                    self.image_files.append(img_path)
                    self.labels_files.append(txt_path)

        print(f"Found {len(self.image_files)} images with corresponding label files")

    def parse_label_file_weighted(self, label_file):
        """Parse YOLO+ format label file en attribuant des poids aux votes d'experts"""
        boxes = []
        labels = []
        confidences = []  # Confiance pour chaque boîte

        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()

                # Vérifier qu'il y a au moins 5 parties (labels + 4 coordonnées)
                if len(parts) < 5:
                    continue

                # Récupérer les labels des experts
                expert_labels = parts[0].split('_')

                # Compter les occurrences de chaque label
                label_counter = Counter(expert_labels)

                # Si tous les experts sont d'accord, confiance = 1.0
                if len(label_counter) == 1:
                    majority_label = int(expert_labels[0])
                    confidence = 1.0
                else:
                    # Sinon, prenons le label avec le plus d'occurrences
                    majority_label, count = label_counter.most_common(1)[0]
                    majority_label = int(majority_label)

                    # Calculer la confiance comme 0.25 * nombre d'experts ayant voté pour ce label
                    confidence = count * 0.25

                # Les 4 derniers éléments sont les coordonnées
                try:
                    x_center = float(parts[-4])
                    y_center = float(parts[-3])
                    width = float(parts[-2])
                    height = float(parts[-1])

                    # Vérifier que les valeurs sont dans des plages raisonnables
                    if 0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1:
                        boxes.append([x_center, y_center, width, height])
                        labels.append(majority_label)
                        confidences.append(confidence)
                    else:
                        print(f"Valeurs de coordonnées hors limites: {parts[-4:]} dans {label_file}")
                except ValueError:
                    # Les 4 derniers éléments ne sont pas des nombres
                    print(f"Impossible de convertir les coordonnées en nombres: {parts[-4:]} dans {label_file}")

        return boxes, labels, confidences

    def generate_background_samples(self, image_file, boxes, num_samples=3, min_size=64, max_size=224, max_attempts=100):
        """Génère des échantillons de fond (FOND) qui ne chevauchent pas les boîtes existantes"""
        try:
            img = Image.open(image_file)
            img_width, img_height = img.size

            # Convertir les coordonnées relatives en absolues pour toutes les boîtes
            abs_boxes = []
            for box in boxes:
                x_center, y_center, width, height = box
                x1 = int((x_center - width/2) * img_width)
                y1 = int((y_center - height/2) * img_height)
                x2 = int((x_center + width/2) * img_width)
                y2 = int((y_center + height/2) * img_height)
                abs_boxes.append((x1, y1, x2, y2))

            background_crops = []
            attempts = 0
            # Increased max_attempts to ensure we get enough samples
            successful_samples = 0

            while successful_samples < num_samples and attempts < max_attempts:
                attempts += 1

                # Taille aléatoire entre min_size et max_size
                crop_width = np.random.randint(min_size, min(max_size, img_width//2))
                crop_height = np.random.randint(min_size, min(max_size, img_height//2))

                # Position aléatoire
                x1 = np.random.randint(0, img_width - crop_width)
                y1 = np.random.randint(0, img_height - crop_height)
                x2 = x1 + crop_width
                y2 = y1 + crop_height

                # Vérifie si ce crop chevauche une boîte existante
                overlap = False
                for box_x1, box_y1, box_x2, box_y2 in abs_boxes:
                    if not (x2 < box_x1 or x1 > box_x2 or y2 < box_y1 or y1 > box_y2):
                        overlap = True
                        break

                # Si pas de chevauchement, ajouter comme échantillon de fond
                if not overlap:
                    crop = img.crop((x1, y1, x2, y2))
                    crop = crop.resize((224, 224), Image.LANCZOS)
                    background_crops.append(crop)
                    successful_samples += 1

            if successful_samples < num_samples:
                print(f"Warning: Could only generate {successful_samples}/{num_samples} background samples after {max_attempts} attempts")

            return background_crops
        except Exception as e:
            print(f"Erreur lors de la génération d'échantillons de fond: {e}")
            return []

    def crop_images(self, resize_size=(224, 224)):
        """Extract crops from images using bounding boxes with weighted expert votes"""
        self.find_files()

        for img_path, label_path in tqdm(zip(self.image_files, self.labels_files), total=len(self.image_files)):
            try:
                img = Image.open(img_path)
                img_width, img_height = img.size

                boxes, labels, confidences = self.parse_label_file_weighted(label_path)

                for box, label, confidence in zip(boxes, labels, confidences):
                    # Convert relative coordinates to absolute
                    x_center, y_center, width, height = box
                    x_center *= img_width
                    y_center *= img_height
                    width *= img_width
                    height *= img_height

                    # Vérifier que width et height sont positifs
                    if width <= 0 or height <= 0:
                        continue

                    # Calculate box coordinates
                    x1 = int(x_center - width / 2)
                    y1 = int(y_center - height / 2)
                    x2 = int(x_center + width / 2)
                    y2 = int(y_center + height / 2)

                    # Ensure coordinates are within image bounds
                    x1 = max(0, x1)
                    y1 = max(0, y1)
                    x2 = min(img_width, x2)
                    y2 = min(img_height, y2)

                    # S'assurer que x2 > x1 et y2 > y1
                    if x2 <= x1 or y2 <= y1:
                        continue

                    # Crop the image
                    crop = img.crop((x1, y1, x2, y2))

                    # Resize the crop
                    crop = crop.resize(resize_size, Image.LANCZOS)

                    self.crops.append(crop)
                    self.crop_labels.append(label)
                    self.crop_confidences.append(confidence)

                # Ne générer des échantillons de fond que pour certaines images
                # Limiter le nombre total d'échantillons de fond à environ 300
                if len(self.crop_labels) > 0 and self.crop_labels.count(8) < 300:
                    # Générer aléatoirement 0 ou 1 échantillon avec une probabilité de 0.3
                    if np.random.random() < 0.3:
                        bg_samples = self.generate_background_samples(img_path, boxes, num_samples=1)
                        for bg_sample in bg_samples:
                            self.crops.append(bg_sample)
                            self.crop_labels.append(8)  # Classe 8 = FOND
                            self.crop_confidences.append(1.0)  # Haute confiance pour les échantillons de fond

            except Exception as e:
                print(f"Error processing {img_path}: {e}")

        print(f"Generated {len(self.crops)} crops (including background samples)")

        # Afficher la distribution des classes
        unique_labels, counts = np.unique(self.crop_labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = CLASS_MAPPING.get(label, f"Class {label}")
            print(f"  {class_name} (ID: {label}): {count} samples")

        # Afficher la distribution des confiances
        confidence_ranges = [(0.0, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1.0)]
        confidence_counts = {r: 0 for r in confidence_ranges}

        for conf in self.crop_confidences:
            for low, high in confidence_ranges:
                if low <= conf <= high:
                    confidence_counts[(low, high)] += 1
                    break

        print("\nConfidence distribution:")
        for (low, high), count in confidence_counts.items():
            percentage = count / len(self.crop_confidences) * 100
            print(f"  {low:.2f} - {high:.2f}: {count} samples ({percentage:.1f}%)")

        return self.crops, self.crop_labels, self.crop_confidences

    def visualize_samples(self, n_samples=5):
        """Visualize random sample crops with their labels and confidences"""
        if not self.crops:
            print("No crops available. Run crop_images() first.")
            return

        # Sélectionner des échantillons de chaque classe si possible
        samples_by_class = {}
        for i, label in enumerate(self.crop_labels):
            if label not in samples_by_class:
                samples_by_class[label] = []
            if len(samples_by_class[label]) < n_samples:
                samples_by_class[label].append(i)

        # Créer une liste de tous les échantillons à visualiser
        all_samples = []
        for label, samples in samples_by_class.items():
            all_samples.extend(samples[:min(n_samples, len(samples))])

        # Si trop d'échantillons, sélectionner aléatoirement
        if len(all_samples) > n_samples * len(samples_by_class):
            all_samples = np.random.choice(all_samples, n_samples * len(samples_by_class), replace=False)

        # Créer la figure
        n_cols = min(5, len(all_samples))
        n_rows = (len(all_samples) + n_cols - 1) // n_cols
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))
        axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]

        for i, idx in enumerate(all_samples):
            if i < len(axes):
                axes[i].imshow(self.crops[idx])
                label = self.crop_labels[idx]
                confidence = self.crop_confidences[idx] if idx < len(self.crop_confidences) else 1.0
                class_name = CLASS_MAPPING.get(label, f"Class {label}")
                axes[i].set_title(f"{class_name} (ID: {label})\nConf: {confidence:.2f}")
                axes[i].axis('off')

        # Masquer les axes inutilisés
        for i in range(len(all_samples), len(axes)):
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

# Fonction pour générer un dataset équilibré avec considération des confidences
def balance_dataset_with_confidence(crops, labels, confidences, target_count=None, augment=True, min_confidence=0.25):
    """
    Équilibre les données pour avoir le même nombre d'échantillons par classe,
    en tenant compte des niveaux de confiance.

    Args:
        crops: Liste d'images (PIL.Image)
        labels: Liste des étiquettes correspondantes
        confidences: Liste des niveaux de confiance
        target_count: Nombre cible d'échantillons par classe (si None, utilise le maximum disponible)
        augment: Si True, utilise l'augmentation de données pour les classes sous-représentées
        min_confidence: Seuil minimal de confiance pour considérer un échantillon fiable

    Returns:
        balanced_crops: Liste équilibrée d'images
        balanced_labels: Liste équilibrée d'étiquettes
        balanced_confidences: Liste équilibrée des confidences
    """
    import numpy as np
    from PIL import Image, ImageEnhance
    import random

    # Créer une liste d'indices pour les échantillons avec confiance suffisante
    reliable_indices = [i for i, conf in enumerate(confidences) if conf >= min_confidence]
    reliable_labels = [labels[i] for i in reliable_indices]

    # Compter les échantillons fiables par classe
    unique_labels = np.unique(reliable_labels)
    class_counts = {}
    class_indices = {}

    # Organiser les indices par classe
    for label in unique_labels:
        indices = [i for i, l in enumerate(reliable_labels) if l == label]
        real_indices = [reliable_indices[i] for i in indices]
        class_counts[label] = len(indices)
        class_indices[label] = real_indices

    print("Distribution originale des classes (échantillons fiables):")
    for label, count in class_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    # Déterminer le nombre cible d'échantillons par classe
    if target_count is None:
        # Utiliser la classe majoritaire comme cible
        target_count = max(class_counts.values())

    print(f"Cible: {target_count} échantillons par classe")

    # Fonction d'augmentation améliorée
    def augment_image_improved(img):
        """Applique une augmentation aléatoire à une image"""
        if not isinstance(img, Image.Image):
            print(f"Warning: Expected PIL Image but got {type(img)}. Skipping augmentation.")
            return img

        # Appliquer plusieurs augmentations aléatoires
        num_augs = random.randint(1, 3)

        # Liste des types d'augmentation
        aug_types = ['rotate', 'flip_h', 'flip_v', 'contrast', 'brightness', 'color']

        # Sélectionner des augmentations aléatoires
        selected_augs = random.sample(aug_types, min(num_augs, len(aug_types)))

        result_img = img.copy()

        for aug_type in selected_augs:
            if aug_type == 'rotate':
                # Rotation aléatoire entre -30 et 30 degrés
                angle = random.uniform(-30, 30)
                result_img = result_img.rotate(angle, resample=Image.BICUBIC, expand=False)

            elif aug_type == 'flip_h':
                # Retournement horizontal
                result_img = result_img.transpose(Image.FLIP_LEFT_RIGHT)

            elif aug_type == 'flip_v':
                # Retournement vertical
                result_img = result_img.transpose(Image.FLIP_TOP_BOTTOM)

            elif aug_type == 'contrast':
                # Modification du contraste
                factor = random.uniform(0.7, 1.5)
                enhancer = ImageEnhance.Contrast(result_img)
                result_img = enhancer.enhance(factor)

            elif aug_type == 'brightness':
                # Modification de la luminosité
                factor = random.uniform(0.7, 1.3)
                enhancer = ImageEnhance.Brightness(result_img)
                result_img = enhancer.enhance(factor)

            elif aug_type == 'color':
                # Modification de la saturation
                factor = random.uniform(0.7, 1.3)
                enhancer = ImageEnhance.Color(result_img)
                result_img = enhancer.enhance(factor)

        return result_img

    # Créer le jeu de données équilibré
    balanced_crops = []
    balanced_labels = []
    balanced_confidences = []

    for label in unique_labels:
        # Récupérer les indices des échantillons de cette classe
        class_idx = class_indices[label]
        class_size = len(class_idx)

        # Si la classe a suffisamment d'échantillons, sous-échantillonner
        if class_size >= target_count:
            # Choisir aléatoirement target_count échantillons
            selected_indices = np.random.choice(class_idx, target_count, replace=False)
            for idx in selected_indices:
                balanced_crops.append(crops[idx])
                balanced_labels.append(labels[idx])
                balanced_confidences.append(confidences[idx])

        # Sinon, augmenter le nombre d'échantillons
        else:
            # Ajouter tous les échantillons existants
            for idx in class_idx:
                balanced_crops.append(crops[idx])
                balanced_labels.append(labels[idx])
                balanced_confidences.append(confidences[idx])

            # Compléter avec des échantillons augmentés ou dupliqués
            samples_to_add = target_count - class_size

            if augment:
                # Augmentation de données
                for _ in range(samples_to_add):
                    # Choisir un échantillon aléatoire à augmenter, en favorisant ceux avec haute confiance
                    weights = [confidences[idx] for idx in class_idx]
                    sum_weights = np.sum(weights)
                    if sum_weights > 0:  # Prevent division by zero
                        weighted_indices = np.random.choice(class_idx, 1, p=np.array(weights)/sum_weights)[0]
                    else:
                        weighted_indices = np.random.choice(class_idx, 1)[0]

                    original_img = crops[weighted_indices]

                    # Appliquer l'augmentation améliorée
                    augmented_img = augment_image_improved(original_img)

                    balanced_crops.append(augmented_img)
                    balanced_labels.append(labels[weighted_indices])
                    # Pour les échantillons augmentés, réduire légèrement la confiance
                    balanced_confidences.append(max(0.7 * confidences[weighted_indices], 0.5))
            else:
                # Simple duplication d'échantillons existants
                for i in range(samples_to_add):
                    # Sélection circulaire des échantillons
                    idx = class_idx[i % class_size]
                    balanced_crops.append(crops[idx])
                    balanced_labels.append(labels[idx])
                    balanced_confidences.append(confidences[idx])

    # Mélanger le jeu de données
    combined = list(zip(balanced_crops, balanced_labels, balanced_confidences))
    random.shuffle(combined)
    balanced_crops, balanced_labels, balanced_confidences = zip(*combined)

    # Convertir en listes
    balanced_crops = list(balanced_crops)
    balanced_labels = list(balanced_labels)
    balanced_confidences = list(balanced_confidences)

    # Vérifier la distribution finale
    final_counts = {}
    for label in balanced_labels:
        if label not in final_counts:
            final_counts[label] = 0
        final_counts[label] += 1

    print("Distribution équilibrée des classes:")
    for label, count in final_counts.items():
        class_name = CLASS_MAPPING.get(label, f"Class {label}")
        print(f"  {class_name} (ID: {label}): {count} échantillons")

    return balanced_crops, balanced_labels, balanced_confidences

# Dataset with confidence weights
class ConfidenceAwareDataset(Dataset):
    def __init__(self, images, labels, confidences, transform=None):
        self.images = images
        self.labels = labels
        self.confidences = confidences
        self.transform = transform
        # Store original image dimensions
        self.original_sizes = []
        for img in images:
            width, height = img.size
            self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]
        confidence = self.confidences[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        # Apply transformations to the image
        if self.transform:
            image = self.transform(image)

        # Normalize dimensions for network input (between 0 and 1)
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        # Include confidence
        confidence_tensor = torch.tensor([confidence], dtype=torch.float32)

        # Combine size and confidence
        features = torch.cat((size_tensor, confidence_tensor), dim=0)

        return image, features, label

class ConfidenceAwareTestDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                           if f.endswith('.jpg') or f.endswith('.png')]
        self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in self.image_files]

        # Store original image dimensions
        self.original_sizes = []
        for image_path in self.image_files:
            with Image.open(image_path) as img:
                width, height = img.size
                self.original_sizes.append((width, height))

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        image_id = self.image_ids[idx]

        # Get original dimensions
        original_size = self.original_sizes[idx]

        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        # Normalize dimensions
        size_tensor = torch.tensor([
            original_size[0] / 1000.0,
            original_size[1] / 1000.0
        ], dtype=torch.float32)

        # For test data, use a neutral confidence
        confidence_tensor = torch.tensor([0.75], dtype=torch.float32)

        # Combine size and confidence
        features = torch.cat((size_tensor, confidence_tensor), dim=0)

        return image, features, image_id

# Enhanced model that accounts for confidence
class ConfidenceAwareClassifier(nn.Module):
    def __init__(self, num_classes=9, pretrained=True):
        super(ConfidenceAwareClassifier, self).__init__()
        # Use EfficientNet-B4 for performance
        self.base_model = models.efficientnet_b4(pretrained=pretrained)

        # Remove the final classification layer
        self.features = nn.Sequential(*list(self.base_model.children())[:-1])

        # Get the number of output features
        in_features = self.base_model.classifier[1].in_features

        # Create a layer for features (size + confidence = 3 values)
        self.features_encoder = nn.Sequential(
            nn.Linear(3, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # Combine visual features and metadata
        self.classifier = nn.Sequential(
            nn.Linear(in_features + 128, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, num_classes)
        )

    def forward(self, x, features):
        # Extract visual features
        visual_features = self.features(x)
        visual_features = torch.flatten(visual_features, 1)

        # Encode metadata (size + confidence)
        metadata_features = self.features_encoder(features)

        # Concatenate features
        combined = torch.cat((visual_features, metadata_features), dim=1)

        # Final classification
        output = self.classifier(combined)

        return output

# Training with weighted loss based on sample confidence
def train_model_with_confidence(model, train_loader, val_loader, criterion, optimizer, scheduler,
                               num_epochs=15, device='cuda', patience=5):
    model = model.to(device)
    best_val_f1 = 0.0
    best_model_weights = None
    patience_counter = 0

    history = {
        'train_loss': [],
        'val_loss': [],
        'train_f1': [],
        'val_f1': [],
        'learning_rates': []
    }

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_preds = []
        train_targets = []
        total_weights = 0.0

        for inputs, features, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            inputs = inputs.to(device)
            features = features.to(device)
            labels = labels.to(device)

            # Extract confidence weights
            confidences = features[:, 2]  # The confidence is the third feature

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs, features)

            # Use sample weights based on confidence
            loss = 0.0
            for i in range(len(labels)):
                sample_loss = criterion(outputs[i:i+1], labels[i:i+1])
                weighted_loss = sample_loss * confidences[i]
                loss += weighted_loss

            # Normalize loss by the sum of weights
            batch_weights = confidences.sum().item()
            # Add a small epsilon to avoid division by zero
            epsilon = 1e-7
            loss = loss / (batch_weights + epsilon)
            total_weights += batch_weights

            # Backward pass and optimize
            loss.backward()

            # Gradient clipping to stabilize training
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            # Statistics
            train_loss += loss.item() * batch_weights
            _, preds = torch.max(outputs, 1)
            train_preds.extend(preds.cpu().numpy())
            train_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        # Add epsilon to avoid division by zero
        train_loss = train_loss / (total_weights + epsilon)
        train_f1 = f1_score(train_targets, train_preds, average='macro')

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_targets = []

        with torch.no_grad():
            for inputs, features, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation"):
                inputs = inputs.to(device)
                features = features.to(device)
                labels = labels.to(device)

                # Forward pass
                outputs = model(inputs, features)
                loss = criterion(outputs, labels)  # This returns per-sample losses

                # Calculate mean loss for the batch
                batch_loss = loss.mean().item()

                # Statistics
                val_loss += batch_loss * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_preds.extend(preds.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())

        # Calculate epoch statistics
        val_loss = val_loss / len(val_loader.dataset)
        val_f1 = f1_score(val_targets, val_preds, average='macro')

        # Get current learning rate
        current_lr = optimizer.param_groups[0]['lr']

        # Save learning rate in history
        history['learning_rates'].append(current_lr)

        # Update scheduler
        scheduler.step(val_loss)

        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_weights = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping
        if patience_counter >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break

        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1)

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')
        print(f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')
        print(f'Learning Rate: {current_lr:.6f}')

    # Load best model weights
    model.load_state_dict(best_model_weights)

    return model, history

# Evaluation function
def evaluate_model_with_confidence(model, dataloader, device='cuda'):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, features, labels in tqdm(dataloader, desc="Evaluating"):
            inputs = inputs.to(device)
            features = features.to(device)
            labels = labels.to(device)

            outputs = model(inputs, features)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    # Calculate F1 score
    f1 = f1_score(all_targets, all_preds, average='macro')
    f1_per_class = f1_score(all_targets, all_preds, average=None)

    # Create confusion matrix
    cm = confusion_matrix(all_targets, all_preds)

    return f1, f1_per_class, cm

# Prediction function for test data
def predict_test_data_with_confidence(model, test_loader, device='cuda'):
    model.eval()
    predictions = {}
    all_probs = {}  # For storing probabilities

    with torch.no_grad():
        for inputs, features, image_ids in tqdm(test_loader, desc="Predicting test data"):
            inputs = inputs.to(device)
            features = features.to(device)

            outputs = model(inputs, features)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(probs, 1)

            for img_id, pred, prob in zip(image_ids, preds.cpu().numpy(), probs.cpu().numpy()):
                predictions[img_id] = int(pred)
                all_probs[img_id] = prob

    return predictions, all_probs

# Visualization functions
def plot_confusion_matrix(cm, class_mapping):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[class_mapping[i] for i in range(len(class_mapping))],
                yticklabels=[class_mapping[i] for i in range(len(class_mapping))])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_f1'], label='Train F1')
    plt.plot(history['val_f1'], label='Validation F1')
    plt.title('F1 Score over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Plot learning rate if available
    if 'learning_rates' in history and history['learning_rates']:
        plt.figure(figsize=(8, 4))
        plt.plot(history['learning_rates'], marker='o')
        plt.title('Learning Rate over epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.yscale('log')
        plt.grid(True)
        plt.tight_layout()
        plt.show()

# Utility function to create submission file
def create_submission_file(predictions, output_file='submission.csv'):
    with open(output_file, 'w') as f:
        f.write('idx,gt\n')
        for img_id, pred in predictions.items():
            f.write(f'{img_id},{pred}\n')

    print(f"Submission file created: {output_file}")

# Main function that puts everything together
def main():
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Set paths - UPDATED PATHS
    train_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/data/data"
    test_dir = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/datatest/datatest"

    # 1. Process training data
    print("Processing training data...")
    processor = CollemboleDataProcessor(train_dir)
    crops, labels, confidences = processor.crop_images(resize_size=(224, 224))
    processor.visualize_samples(n_samples=5)

    # 2. Balance dataset with confidences
    print("Balancing dataset...")
    crops_balanced, labels_balanced, confidences_balanced = balance_dataset_with_confidence(
        crops, labels, confidences, target_count=200, augment=True)

    # 3. Prepare data transforms
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # 4. Create datasets and split into train/val
    X = crops_balanced
    y = np.array(labels_balanced)
    confidences_array = np.array(confidences_balanced)

    # Split dataset (80% train, 20% validation)
    from sklearn.model_selection import train_test_split

    # Create indices for splitting that preserves stratification
    indices = np.arange(len(X))
    indices_train, indices_val = train_test_split(
        indices, test_size=0.2, random_state=42, stratify=y)

    # Split data using indices
    X_train = [X[i] for i in indices_train]
    y_train = y[indices_train]
    confidences_train = confidences_array[indices_train]

    X_val = [X[i] for i in indices_val]
    y_val = y[indices_val]
    confidences_val = confidences_array[indices_val]

    print(f"Training set: {len(X_train)} samples")
    print(f"Validation set: {len(X_val)} samples")

    # Create confidence-aware datasets
    train_dataset = ConfidenceAwareDataset(X_train, y_train, confidences_train, transform=train_transform)
    val_dataset = ConfidenceAwareDataset(X_val, y_val, confidences_val, transform=val_transform)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)

    # 5. Initialize model
    model = ConfidenceAwareClassifier(num_classes=len(CLASS_MAPPING), pretrained=True)

    # 6. Training setup
    criterion = nn.CrossEntropyLoss(reduction='none')  # Need to use 'none' for per-sample weighting
    optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-2)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)

    # 7. Train model
    print("Training model...")
    model, history = train_model_with_confidence(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=15,
        device=device
    )

    # Save model
    torch.save(model.state_dict(), 'collembole_classifier_confidence_aware.pth')

    # 8. Evaluate model
    print("Evaluating model...")
    f1, f1_per_class, cm = evaluate_model_with_confidence(model, val_loader, device)

    print(f"Overall F1 Score: {f1:.4f}")
    print("F1 Score per class:")
    for class_id, score in enumerate(f1_per_class):
        class_name = CLASS_MAPPING.get(class_id, f"Class {class_id}")
        print(f"  {class_name}: {score:.4f}")

    plot_confusion_matrix(cm, CLASS_MAPPING)
    plot_training_history(history)

    # 9. Predict test data
    print("Predicting test data...")
    test_dataset = ConfidenceAwareTestDataset(test_dir, transform=val_transform)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)

    predictions, all_probs = predict_test_data_with_confidence(model, test_loader, device)
    create_submission_file(predictions, 'submission_confidence_aware.csv')

    print("Done!")

if __name__ == "__main__":
    main()

import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image

# Charger le fichier CSV
csv_file = '/content/submission_confidence_aware.csv'
df = pd.read_csv(csv_file)

# Filtrer les lignes dont le label est 8
df_label8 = df[df['gt'] == 8]

# Afficher le nombre d'images trouvées
print(f"Nombre d'images avec le label 8 : {len(df_label8)}")

# Parcourir les lignes filtrées et afficher les images
for idx, row in df_label8.iterrows():
    image_path = "/content/drive/MyDrive/PROJET_COLLEMBOLE/Données challenge deep/datatest/datatest/"+row['idx']+".jpg"  # Adaptez ce nom de colonne selon votre CSV
    try:
        image = Image.open(image_path)
        plt.figure(figsize=(6, 6))
        plt.imshow(image)
        plt.title(f"Label 8 - {image_path}")
        plt.axis('off')
        plt.show()
    except Exception as e:
        print(f"Erreur lors de l'ouverture de l'image {image_path} : {e}")